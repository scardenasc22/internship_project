{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68384438",
   "metadata": {},
   "source": [
    "### candidate tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round finished. 9 candidates promoted\n",
      "Round finished. 4 candidates promoted\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from functions import text_extraction\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "root = \"/\".join(os.getcwd().split(\"/\")[:-1])\n",
    "resumes_path = os.path.join(root, 'data', \"raw\", \"cv\")\n",
    "resumes_list = os.listdir(resumes_path)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-2024-08-06\"\n",
    ")\n",
    "\n",
    "candidates_dict = {\n",
    "    file.split(\"-\")[0] : text_extraction(os.path.join(resumes_path, file)) for file in resumes_list\n",
    "}\n",
    "\n",
    "job_description = text_extraction(\n",
    "    file_path = \"/Users/santiagocardenas/Documents/MDSI/202502/internship/internship_project/data/raw/job/data_role_des.txt\"\n",
    ")\n",
    "\n",
    "class GroupWinners(BaseModel):\n",
    "    winner_ids: List[str] = Field(..., description=\"A list of 1 or 2 IDs of the best candidates in the group.\")\n",
    "\n",
    "selection_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        (\n",
    "            \"You are an expert recruiter conducting a tournament selection round. \"\n",
    "            \"Analyze the provided resumes and select the ONE to TWO best candidates \"\n",
    "            \"who are most qualified for the role described below. Your output must ONLY be the IDs of the winners.\"\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        (\n",
    "            \"Based on the **Job Description** provided, select the best 1 or 2 winners \"\n",
    "            \"from the candidates with the following IDs: {all_candidate_ids}.\"\n",
    "            \"\\n\\n**Job Description:**\\n{job_description}\" \n",
    "            \"\\n\\n**Candidate Resumes:**\\n{group_resumes}\"\n",
    "        ),\n",
    "    ),\n",
    "])\n",
    "\n",
    "def tournament_evaluation(\n",
    "    candidates_dict : dict,\n",
    "    job_description : str,\n",
    "    final_target : int = 4,\n",
    "    group_size : int = 5,\n",
    "    \n",
    "):\n",
    "    llm_selector = llm.with_structured_output(schema = GroupWinners)\n",
    "    selection_chain = selection_prompt | llm_selector\n",
    "    candidates_to_evaluate = candidates_dict\n",
    "    \n",
    "    while len(candidates_to_evaluate) > final_target:\n",
    "        candidate_ids = list(candidates_to_evaluate.keys())\n",
    "        random.shuffle(candidate_ids)\n",
    "        winners_of_round = {}\n",
    "        groups = [\n",
    "            candidate_ids[i : i + group_size] for i in range(0, len(candidate_ids), group_size)\n",
    "        ]\n",
    "        for group_index, group_ids in enumerate(groups):\n",
    "            group_resumes_string = '\\n---\\n'.join([\n",
    "                f\"Candidate ID: {cid}\\nResume: {candidates_to_evaluate[cid]}\" for cid in group_ids\n",
    "            ])\n",
    "            try:\n",
    "                selection_obj = selection_chain.invoke({\n",
    "                    \"group_resumes\" : group_resumes_string,\n",
    "                    \"all_candidate_ids\" : ', '.join(group_ids),\n",
    "                    \"job_description\" : job_description\n",
    "                })\n",
    "                for winner_id in selection_obj.winner_ids:\n",
    "                    if winner_id in group_ids:\n",
    "                        winners_of_round[winner_id] = candidates_to_evaluate[winner_id]\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating group {group_index + 1}: {e}\")\n",
    "                if group_ids:\n",
    "                    winners_of_round[group_ids[0]] = candidates_to_evaluate[group_ids[0]]\n",
    "        candidates_to_evaluate = winners_of_round\n",
    "        print(f\"Round finished. {len(candidates_to_evaluate)} candidates promoted\")\n",
    "    \n",
    "    return candidates_to_evaluate\n",
    "\n",
    "final_selection = tournament_evaluation(\n",
    "    candidates_dict = candidates_dict,\n",
    "    job_description = job_description\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80a3f2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'90e485656ce9236d': 'Innovative and growth-driven Data Analyst with over 2 years of experience designing and delivering end-to-end data analytics and data science solutions across cloud platforms. Adept at building and monitoring secure, scalable data pipelines and transforming raw data into actionable insights that improve business performance. Proficient in SQL, Python, R, and data visualization tools like Power BI and Tableau, with hands-on experience using dbt, Databricks, BigQuery, and Snowflake. Strong foundational understanding of machine learning models and their integration within business intelligence platforms. Demonstrated ability to collaborate across cross-functional teams—including engineers, data scientists, and stakeholders—to support data-driven decision-making and ensure data integrity and governance. Excellent communicator, known for turning complex data into clear narratives that support strategic innovation and growth.\\nProjects\\nRetail Sales Analysis | SQL & Power BI                                                                         \\nAnalysed retail sales data using SQL, leveraging joins, subqueries, and aggregations to create a structured dataset.\\nDeveloped a Power BI dashboard featuring key KPIs such as revenue growth, regional sales performance, and inventory turnover.\\nUtilized DAX calculations and data modelling to enhance reporting accuracy and decision-making.\\nAirbnb Listing Analysis | Tableau                                                                    \\nDesigned an interactive Tableau dashboard to analyse Airbnb listings, highlighting trends in pricing, occupancy, and location impact.\\nImplemented geospatial mapping and drill-down features for enhanced user experience and insights extraction.\\nBank Customer Churn Analysis | Excel                                                                   \\nUtilized Excel (Pivot Tables, VLOOKUP, conditional formatting) to analyse customer data and identify churn trends.\\nCreated a dynamic dashboard to visualize churn factors and recommended data-driven retention strategies.\\n\\nProfessional Experience\\nNext Wave MSP, Australia\\nData Analyst | Jan 2024 – Present\\nDesigned and monitored secure, end-to-end data pipelines, transforming raw data into analytics-ready formats using tools like dbt, Databricks, and SQL-based transformations.\\nBuilt interactive dashboards in Power BI and Tableau to visualize performance metrics, sentiment analysis, and call outcomes—enabling real-time monitoring of customer experience KPIs.\\nCollaborated with data engineers and scientists to tune and integrate machine learning models into voice analytics workflows, enhancing system adaptability and business relevance.\\nConducted deep-dive analyses using SQL, Python, R, and cloud platforms (BigQuery, Snowflake, Azure) to extract meaningful trends and drive strategic, data-informed decisions.\\nApplied data governance and documentation practices to improve data quality, pipeline transparency, and team-wide accessibility to structured and unstructured datasets.\\nWorked with cross-functional stakeholders to understand business challenges and deliver tailored data science and analytics solutions that support innovation and long-term growth.\\nPartnered with technical and non-technical teams to ensure secure data migration and integration, identifying potential risks and issues and supporting pre-deployment testing phases.\\nContributed to NLP model optimization for transcription and sentiment tagging, aligning data science models with contact center monitoring goals.\\nSupported continuous improvement by evaluating emerging tools, optimizing workflows, and applying best practices across analytics and data science functions.\\nKnown for excellent communication skills and ability to collaborate in agile, cross-functional teams to meet tight deadlines and evolving client requirements.\\nUnited Fuel Pump, VIC, Australia \\nCustomer Service Representative | Aug 2023 – Jan 2024\\nHelped customers with fuel transactions, always making sure to provide friendly and fast service.\\nManaged the cash register, handling payments with cash, card, or loyalty points.\\nRestocked shelves and kept track of inventory to make sure products were available when customers needed them.\\nTook the time to listen to customer questions and resolve any concerns in a professional and helpful way.\\nFollowed safety guidelines for handling fuel and made sure the work environment stayed safe and secure.\\nAssisted with everyday station tasks, like opening, closing, and completing end-of-day reports.\\n\\nPantech, India \\nIntern | Jan 2023 – Jun 2023\\nContributed to gathering, cleaning, and preparing large datasets for analysis, ensuring the data was both accurate and reliable.\\nPerformed exploratory data analysis (EDA) to uncover trends and patterns.\\nDeveloped interactive dashboards and visualizations using Tableau and Power BI to present key insights to stakeholders.\\nWrote SQL queries to retrieve data from relational databases for reporting and analysis purposes.\\nAssisted in the development of predictive models by selecting features and testing various algorithms.\\nMonitored the integrity of data throughout the ETL process to ensure smooth and accurate data integration.\\n\\nTechnical Skills\\n\\nTechnical Skills\\nData Analysis & Analytics: Data mining, statistical analysis, predictive modeling, data cleaning, data modeling, KPI analysis, advanced analytics, business intelligence\\nProgramming & Scripting Languages: SQL (including advanced troubleshooting, ETL, nested queries), Python (pandas, NumPy, Seaborn, Matplotlib), R, C#, C++, VBA\\nData Visualization: Power BI, Tableau, Seaborn, Matplotlib\\nData Warehousing & Databases: Oracle e-Business Suite (Core HR and Payroll data structures), Databricks, Snowflake, MySQL, PostgreSQL, MS SQL Server, Azure Synapse\\nModeling & Orchestration Tools: DBT (Data Build Tool), Apache Airflow\\nCloud Platforms: Microsoft Azure, AWS\\nTools & Technologies: Advanced Excel (pivot tables, formulas), Power Query, DAX, Jupyter Notebooks, Visual Studio Code, Git, Jira, CI/CD pipelines for analytics workflows\\nSoft Skills: Strong analytical and problem-solving ability, entrepreneurial mindset, excellent communication and stakeholder collaboration, detail-oriented, collaborative, proactive learner\\n\\nEducation\\nMaster of Information Systems | Holmes Institute, Australia (June 2025)\\nData Management & Databases | Statistical Analysis & Predictive Modelling | Business Intelligence & Data Visualization | Data Governance & Privacy| \\n\\nBachelor of Electrical Engineering | Osmania University, India (November 2022)\\nAnalytical & problem-solving skills | Circuit design & electrical systems | Computer Science Fundamentals\\n\\nProfessional Attributes\\nAnalytical Thinking | Attention to Detail | Problem-Solving | Communication Skills | Critical Thinking | Adaptability | Time Management | Collaboration | Curiosity | Data-Driven Mindset | Active Listening | Accountability | communicate \\n\\nReferences\\nAvailable upon request.',\n",
       " 'c69f2087775fc760': 'Yanlin (Nikki) Chen\\nTel: 0490850973 Email: chen_0616@outlook.com LinkedIn: www.linkedin.com/in/nikki-chen-a589ba20b\\nEDUCATION\\nMaster of Business Information Systems 02/2023 – 12/2024\\nMonash University, Melbourne\\n\\uf09f Published first-author research in spatial data analysis (Nov 2024).\\n\\uf09f Awarded International Study Grants (2023); achieved the highest grade in Advanced Database Technology.\\n\\uf09f Leadership & Activities: PASS Leader, Project Translator (English-Mandarin), Monash Open Day\\nAmbassador.\\nPROFESSIONAL EXPERIENCE\\nMonash University — Melbourne, Australia\\nOnline Unit Coordinator 02/2025 – Present\\n• Responsible for the end-to-end coordination of ITO4132, reporting directly to the Chief Examiner and\\nMonash Online management team.\\n• Oversee delivery for 100+ students per semester, managing learning schedules, assessments, grading\\nschemes, and LMS infrastructure.\\n• Lead weekly live sessions to deepen student understanding of relational database concepts (SQL, ER\\nmodeling, Transaction Management) and real-world data handling.\\n• Resolve 100% of escalated academic and administrative student issues, maintaining a smooth and high-\\nsatisfaction learning experience.\\n• Ensure curriculum and assessment alignment with Monash Online’s academic standards and industry best\\npractices, contributing to consistently strong student performance.\\nTeaching Associate 01/2024 – Present\\n• Lead tutorials for FIT9132, FIT5137, FIT3003, and PASS units at Monash University, covering relational\\ndatabases, advanced SQL, dimensional modeling, and business intelligence tools.\\n• Assess technical assignments and final projects, providing constructive feedback to students that improve\\nanalytical thinking and practical database skills.\\n• Achieve a 91% student satisfaction rating in the 2025 SETU, reflecting high-quality instruction and\\ntechnical mentorship.\\nTECHNICAL SKILLS\\n\\uf09f Programming & Query Languages: SQL (PostgreSQL, MySQL, Oracle SQL), Python (Pandas, Matplotlib,\\nNumPy)\\n\\uf09f Data Engineering: DBT (Data Build Tool), ETL Pipelines, Data Warehousing, Star/Snowflake Schema,\\nOLAP\\n\\uf09f Visualization & BI Tools: Tableau, Power BI, QGIS\\n\\uf09f Database Management: PostgreSQL, Oracle SQL Developer, MySQL, MongoDB\\n\\uf09f Big Data & Cloud: Docker, GitLab\\n\\uf09f Others: MS Office suites, Google suites, Miro\\nRESEARCH PROJECT\\nFirst Author 03/2024 – 11/2024\\nSpatial analysis of train catchments in outer Melbourne\\n\\uf09f Designed and engineered a data processing framework, integrating more than seven public datasets with\\nPostgreSQL/PostGIS for large-scale spatial analysis.\\n\\uf09f Developed spatial SQL queries to analyze transit accessibility, revealing that 20.81% of the outer\\nMelbourne population lacks access to transit catchments and informing infrastructure planning.\\n\\uf09f Created interactive maps and dashboards in QGIS and Tableau, transforming raw data into actionable\\ninsights for urban planners.\\n\\uf09f Led research and manuscript writing, securing publication in Computers (MDPI) Journal:\\nhttps://doi.org/10.3390/computers13110299\\nCERTIFICATIONS\\n\\uf09f Professional Scrum Master II, Scrum org.\\n\\uf09f Oracle Certified Foundations Associate Database, Oracle\\n\\uf09f Data Engineer Associate, Datacamp\\n\\uf09f Standard Mental Health First Aider, Mental Health First Aid Australia',\n",
       " '6700f96c37c87933': \"HON MENG LEE\\nBUSINESS AND SYSTEMS DATA ANALYST\\nDETAILS PROFILE\\nADDRESS A Business and Systems Data Analyst with 3 years working experience in Data\\nUnit 331/299 Spring Street Analysis, ETL solution development, Database and IT system maintenance. In\\nMelbourne, 3000\\naddition to a masters of Data Science and a bachelors of Computer Science; strong\\nAustralia\\nexperience with customer communication, stakeholder engagement and project\\nPHONE management were also gained in prior study and employment roles. A keen learner\\n+61481 386 471 who is passionate about technology, highly motivated, reliable, detail oriented,\\nanalytical, and obsessed with deriving meaningful insights through the use of clean\\nEMAIL\\nand reliable data.\\nleehonmeng@yahoo.com\\nEMPLOYMENT HISTORY\\nLINKS\\nLinkedIn Business and Systems Data Analyst, PrimaPMI Noble Park North\\nMar 2022 — Feb 2025\\n• Designed, developed, documented, and managed automated ETL solutions\\nSKILLS\\nfor production, sales, and invoicing monitoring\\n• Communicating business needs between vendors and stakeholders for\\nDatabase (SQL, PSQL,\\nsoftware improvement and fixes\\nTSQL, MySQL)\\n• Data analysis, manipulation, and visualisation tasks for consumption and\\nData Cleaning, Modelling, waste investigation through peak season\\nAnalytics (R, Python) • Solving software/technology issues acting as SME of company’s Production\\nOrder Management Software (POMS)\\nMicrosoft Office 365 Suite\\n• Assisted with company merger by aligning and setting up the product\\nExtract, Transform, Load catalogues of both companies\\n(ETL)\\nNPI & Pre-Sales Coordinator, PrimaPMI Noble Park North\\nData Visualisation\\n(PowerBI, QlikView, Feb 2021 — Mar 2022\\nTableau)\\n• Provided technical support to customer on-boarding process\\n• Communication with customers, clients, and vendors\\nAgile Methodology (Scrum,\\nWaterfall, Kanban) • Performed data scraping, cleaning, and analysis to present company\\nperformance insight to stakeholders\\nMachine Learning • Assisted with migration to new company POMS\\n(Clustering, Classification,\\n• Worked with internal non-technical team and external IT for improvement and\\nNeural Networks)\\nfixes to new POMS Software\\nLANGUAGES\\nIT Technician, PrimaPMI Noble Park North\\nNov 2020 — Feb 2021\\nEnglish\\n• Maintenance, repair, and support of company's IT infrastructure\\nBahasa Malaysia • Documentation and inventory of company's IT infrastructure\\n• IT on-boarding and setup for newly hired employees of the company Team Leader & Lead Developer, Grey-Box Québec (Remote)\\nJun 2021 — Oct 2021\\n• Team Leader and main point of stakeholder contact\\n• Arranging and moderating all meetings\\n• Delegation of roles and tasks for each team member\\n• Validation of the Recurrent Neural Network (RNN) model's performance\\n• Optimizing and reporting on the performance of the RNN model\\n• Data mining and cleaning datasets for training and testing of the RNN model\\nEDUCATION\\nMaster of Data Science, RMIT University Melbourne\\nMar 2020 — Dec 2021\\nA Master degree in Data Science that provided knowledge and skills in all facets\\nof the Data Science process. Including analytical skills with multiple programming\\nlanguages - R, Python, and Scala to name a few; alongside tools such as Weka,\\nHadoop, and more. This course also provided knowledge and skills in data mining,\\nanalysis, preprocessing, modelling, visualization and communication. Capstone\\nproject was an internship with Grey-Box. Graduated with Distinction.\\nBachelor of Computer Science, Swinburne University Hawthorn\\nof Technology\\nMar 2017 — Nov 2019\\nA Bachelor of Computer Science with a Major in Software Development which\\nprovided the knowledge and skills in the field of Computer Science and\\nProgramming. This includes programming skills with languages such as Python,\\nJava, C#, Swift, SQL, PHP, and more; alongside Networking knowledge and practical\\nexperience with routers and switches. Additional Project Management workflow\\nknowledge and methodologies such as Agile Waterfall and Agile Scrum were\\nimparted and taught in a practical manner. Capstone project was on applying\\nNatural Language Processing onto Victorian court sentencing documents in order\\nto derive insights into factors that determine court sentencing outcomes.\\nSKILLS AND PROJECTS\\nAutomated Data Warehouse Dashboards SQL, PSQL, Excel, QLIK\\nCreated multiple automated ETL pipelines (and the Data Warehouse) for different\\nbusiness intelligence dashboards (company performance dashboard that includes\\ndaily and historical performance, invoicing and revenue dashboard) by extracting\\ndaily performance data from PostgreSQL Server over to a Microsoft SQL Server for\\ndata cleaning and pre-processing then automatically loaded onto QLIK daily.\\nThis allowed management access to monitor the company's daily and historical\\nperformance, providing insights to key stakeholders on what needs to be improved.\\nData Analysis and Visualisation for Waste and SQL, Excel, R\\nConsumption\\nCollected, analysed, and presented company's paper and ink wastage and\\nconsumption for peak season performance every year. This involved collecting\\ndata from various sources, cleaning, separating and combining them by different\\ncategories and performing analysis and visualisations mainly via Excel with a some\\nusage of R programming.\\nThis allowed the leadership team to identify points of waste and need of\\noptimization.\",\n",
       " 'fae54469e0d024e8': 'Jaewon Yun\\njaewonyun.contact@gmail.com | +61 424 420 725 | Kensington, Melbourne, VIC, 3031 | LinkedIn\\nEDUCATION\\nMonash University Clayton, VIC\\nBachelor of Computer Science Graduated in December 2024\\nMajor in Data Science | Minor in Economics\\n● Extracurricular Activities: Peer Mentor for undergraduate students in the IT Faculty, Committee Member of\\nMonash Data Science Society (MDSS - Monash Data and AI), and Member of Monash Association of Coding.\\n● Relevant Coursework: Predictive Modeling (Python, R), Database Systems (SQL), Data Visualisation (Tableau).\\nEXPERIENCE\\nMonash University - Innovation Guarantee Program\\nResearch Analyst - Academic Initiative with the United Nations (UNEP) January - March 2023\\n● Conducted exploratory data analysis (EDA) using R (tidyverse, corrplot, skimr) on government recycling facility\\nperformance metrics to identify bottlenecks in End-of-Life Vehicle (ELV) recycling processes across 14 Pacific\\nIsland states, resulting in three key policy recommendations on compliance, efficiency, and emissions reduction.\\n● Pre-processed datasets by merging heterogeneous datasets, handling missing values, and creating new features\\nusing Python (pandas), ensuring data quality and enabling statistical analysis to uncover recycling inefficiencies.\\n● Designed interactive Power BI dashboards to visualise key insights, supporting data-driven policymaking.\\n● Collaborated with an interdisciplinary team and UNEP representatives to develop an ELV recycling strategy,\\ndemonstrating strong verbal and written communication skills to present complex findings clearly.\\nTutor Doctor\\nAcademic Tutor - Mathematics and English (VCE) April 2024 - Present\\n● Provided tutoring to VCE students, effectively communicating complex ideas to enhance understanding.\\n● Demonstrated strong professionalism, time management, critical thinking, and organisational skills to maximise\\nstudent outcomes, improving student academic performance by an average of 16% through targeted strategies.\\n● Developed data-driven learning plans, tracking student progress through Excel dashboards.\\nPROJECTS\\nPricing Optimisation and Analysis for Retail Products (Capstone Project - Monash University)\\n● Led end-to-end data analysis on retail pricing records, using Python (Pandas, NumPy, statsmodels) and SQL\\n(PostgreSQL) for data pre-processing, transformation, exploratory analysis, and feature engineering.\\n● Developed an ARIMA-based time series forecasting model to predict sales trends and conducted price elasticity\\nanalysis using log-log regression, identifying pricing strategies projecting a 5-10% revenue increase.\\n● Queried large datasets with PostgreSQL, utilising inner and left joins, window functions, and indexing to optimise\\nquery performance by 30% and uncover insights in peak purchasing times and trends across seasons.\\n● Built interactive Tableau dashboards featuring trend lines and segment filters to visualise pricing impact across\\ncustomer segments, used in quarterly pricing reviews to guide data-driven adjustments.\\nGlobal Renewable Energy Consumption Data Visualisation\\n● Developed an interactive web-based platform using HTML, CSS, R, and Tableau, enhancing user engagement\\nand data visualisation, and integrated JavaScript to enable Tableau dashboard interactivity and data exploration.\\n● Analysed International Energy Agency (IEA) data using R, identifying key insights such as the rapid adoption of\\nsolar energy in Asia, shifting regional consumption trends over time, and emerging market opportunities.\\n● Built dynamic Tableau dashboards featuring geospatial mapping tools, time-series analysis, and interactive\\nfiltering, enabling in-depth exploration of renewable energy consumption by region and energy type.\\nTECHNICAL SKILLS & QUALIFICATIONS\\nTechnical Skills: Python, SQL, Excel, R, Tableau, Power BI, & Microsoft Suite (Word, PowerPoint, Outlook)\\nQualifications: Working with Children Check (WWCC) and Professional Google Data Analytics Certificate (via Coursera)'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52a342",
   "metadata": {},
   "source": [
    "### adding an explanation to the selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92328de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Overall summary: Priya Singh and Sairaj Suryavanshi are the best candidates due to their strong backgrounds in ETL processes, SQL, and data visualization, which are critical for providing innovative data-driven solutions as outlined in the job description.\n",
      "--------------------\n",
      "Candidate e5f8984dfa4cae3b promoted. Reason: Priya Singh stands out with her strong experience in ETL processes, SQL proficiency, and business intelligence reporting through Celonis, directly aligning with the job's data analytics and solution delivery focus.\n",
      "Candidate a8ca1108dd06b907 promoted. Reason: Sairaj Suryavanshi is well-suited for the role with extensive experience in SQL, ETL processes, and data visualization using Power BI, combined with a proactive consulting background, meeting the key requirements of the job description.\n",
      "--------------------\n",
      "Overall summary: Abhishek Sarda and Sahil Dwivedi stand out due to their strong technical backgrounds in data science and experience with tools pertinent to the role, such as SQL and Tableau, coupled with their ability to develop ETL pipelines, aligning them closely with both the technical and business aspects of the Graduate Consultant position.\n",
      "--------------------\n",
      "Candidate b5142b9b9676a4f9 promoted. Reason: Abhishek Sarda has a master's in Data Science and extensive experience with SQL, Python, and ETL pipelines, making him well-suited for data modeling and transformation tasks. His demonstrated capability in creating predictive models and interactive dashboards aligns strongly with the job requirements.\n",
      "Candidate 0176a86af53206ae promoted. Reason: Sahil Dwivedi possesses three years of hands-on experience in data analysis, predictive modeling, and developing ETL pipelines, alongside proficiency in SQL and Tableau. His experience in cloud platforms and ability to communicate insights make him an excellent fit for both technical and client engagement aspects of the role.\n",
      "--------------------\n",
      "Overall summary: The selected winners have demonstrated advanced technical proficiency in the required tools and platforms, a strong academic foundation, and direct practical experience in handling data-centric projects, which aligns closely with the job's requirements for collaboration and innovation in data analytics.\n",
      "--------------------\n",
      "Candidate f2ca85739fda42e4 promoted. Reason: Rafsan Al Mamun stands out with a Master of Data Science and strong SQL, Python, and Tableau skills, alongside practical experience which includes developing a tool for managing large DNA sequences and significant data-driven project contributions.\n",
      "Candidate 90e485656ce9236d promoted. Reason: This candidate has demonstrated experience in building and monitoring data pipelines, proficiency in dbt, Databricks, and SQL, and practical experience using both Power BI and Tableau, making them well-suited for the data modeling and transformation tasks outlined in the job description.\n",
      "--------------------\n",
      "Overall summary: Jaewon Yun and Yanlin Chen are selected for their strong technical skills in data and analytics tools, combined with academic and extracurricular involvement that demonstrate their readiness and enthusiasm to tackle the data challenges outlined in the job description.\n",
      "--------------------\n",
      "Candidate fae54469e0d024e8 promoted. Reason: Jaewon Yun displays a strong academic background in computer science with a focus on data science, complemented by hands-on experience in SQL, Tableau, and Power BI, which are critical for the role. Their involvement with the Monash Data Science Society and proven ability to design impactful data visualizations directly aligns with key job responsibilities.\n",
      "Candidate c69f2087775fc760 promoted. Reason: Yanlin (Nikki) Chen holds a Master of Business Information Systems with a publication in spatial data analysis and significant teaching experience in SQL and data transformation, providing a solid foundation in both technical and communicative aspects necessary for the role. Her experience with DBT and advanced data visualizations aligns well with the job requirements.\n",
      "--------------------\n",
      "Overall summary: Eric Zhang and Kevin Wu stand out due to their substantial experience and technical skills in SQL, Power BI, and cloud platforms, directly aligning with the job's requirements for data modeling, ETL processes, and analytics solutions.\n",
      "--------------------\n",
      "Candidate 1f04921d492de215 promoted. Reason: Eric Zhang is a strong candidate due to his extensive experience in SQL, Power BI, and Azure DevOps, aligning well with the job requirements for data modeling, ETL development, and cloud platform exposure.\n",
      "Candidate 4a9374ec3b2ca204 promoted. Reason: Kevin Wu brings nearly three years of experience in data analysis and machine learning, with proficiency in Python, SQL, and Power BI, which directly supports the role's focus on analytics solutions and data-driven insights.\n",
      "--------------------\n",
      "Overall summary: The selected candidate possesses relevant technical skills, experience in data analytics tools, and an appropriate educational background, aligning closely with the job requirements and setting him apart from potential alternatives.\n",
      "--------------------\n",
      "Candidate 6d743d56d9692fd0 promoted. Reason: Venkatesh Kadiyala demonstrates strong expertise in SQL, proven experience in Power BI development, and has a relevant Master’s degree, making him well-equipped for the Graduate Consultant - Data & Analytics role at Synogize, which requires proficiency in ETL processes, data visualizations, and client engagement.\n",
      "Round finished. 11 candidates promoted\n",
      "--------------------\n",
      "Overall summary: Both Jaewon Yun and Kevin Wu exhibit strong technical competencies in data analytics, including proficiency with key tools like SQL and Power BI, and have demonstrated practical experience in data-driven projects, making them more aligned with the job requirements compared to other candidates.\n",
      "--------------------\n",
      "Candidate fae54469e0d024e8 promoted. Reason: Jaewon Yun stands out with his strong academic background in computer science focused on data science from Monash University and hands-on experience in data visualization, particularly using Tableau and Power BI, two critical tools mentioned in the job description. His role as Research Analyst with key data analysis responsibilities aligns well with the requirements for collaborative data-driven problem-solving.\n",
      "Candidate 4a9374ec3b2ca204 promoted. Reason: Kevin Wu is highly qualified with his mastery in data analytics, proficiency in relevant tools such as Python, SQL, and Power BI, and experience in delivering insights and automated reporting, which aligns directly with the job description's focus on analytics solutions and infrastructure development for data transformation.\n",
      "--------------------\n",
      "Overall summary: Sairaj Suryavanshi and Sahil Dwivedi are selected due to their extensive hands-on experience with essential data engineering and visualization tools, as well as their demonstrated ability to solve complex data problems, which closely aligns with the responsibilities and skills required for the Graduate Consultant position.\n",
      "--------------------\n",
      "Candidate a8ca1108dd06b907 promoted. Reason: Sairaj Suryavanshi’s blend of technical expertise and demonstrated experience in data analysis, ETL processes, and data visualization using tools like SQL and Power BI, combined with exposure to machine learning and cloud environments like AWS and Azure, makes him an ideal fit for the Graduate Consultant role.\n",
      "Candidate 0176a86af53206ae promoted. Reason: Sahil Dwivedi stands out due to his comprehensive experience with data analysis, ETL processes, and a range of data visualization and transformation tools such as Power BI and Tableau, coupled with a strong academic background in data science, directly aligning with the role’s requirements.\n",
      "--------------------\n",
      "Overall summary: The selected candidate stands out due to their extensive hands-on experience and technical skills in data analytics and transformation, demonstrated through their work with advanced tools and proficiency in relevant software platforms, which are crucial for the Data & Analytics role at Synogize.\n",
      "--------------------\n",
      "Candidate 90e485656ce9236d promoted. Reason: This candidate possesses a strong proficiency in data analytics tools and platforms such as SQL, dbt, Databricks, and BigQuery, directly aligning with the job's requirement for ETL processes and data transformation. Their professional experience includes designing and monitoring secure data pipelines and building interactive dashboards with Power BI and Tableau, making them well-suited for developing analytics solutions that Synogize seeks.\n",
      "Round finished. 5 candidates promoted\n",
      "--------------------\n",
      "Overall summary: The selected candidates were chosen for their strong practical experience in data analytics, particularly with SQL, Power BI, and Tableau, coupled with exposure to cloud computing platforms and their capabilities in data modeling and ETL processes, which are critical for the Graduate Consultant role at Synogize.\n",
      "--------------------\n",
      "Candidate 0176a86af53206ae promoted. Reason: Sahil Dwivedi stands out with his extensive hands-on experience in data analysis, predictive modeling, and business intelligence using tools like SQL, Power BI, and Tableau, combined with his understanding of cloud platforms like Snowflake and BigQuery, aligning closely with Synogize's requirements.\n",
      "Candidate 90e485656ce9236d promoted. Reason: This candidate demonstrates solid experience in building and monitoring secure data pipelines across cloud platforms, proficiency in SQL and data visualization tools, and knowledge of dbt and Snowflake, making them well-suited for the role's data transformation and analytics solution needs.\n",
      "Round finished. 2 candidates promoted\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from functions import text_extraction\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "root = \"/\".join(os.getcwd().split(\"/\")[:-1])\n",
    "resumes_path = os.path.join(root, 'data', \"raw\", \"cv\")\n",
    "resumes_list = os.listdir(resumes_path)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-2024-08-06\"\n",
    ")\n",
    "\n",
    "candidates_dict = {\n",
    "    file.split(\"-\")[0] : text_extraction(os.path.join(resumes_path, file)) for file in resumes_list\n",
    "}\n",
    "\n",
    "job_description = text_extraction(\n",
    "    file_path = \"/Users/santiagocardenas/Documents/MDSI/202502/internship/internship_project/data/raw/job/data_role_des.txt\"\n",
    ")\n",
    "\n",
    "\n",
    "class WinnerRationale(BaseModel):\n",
    "    \"\"\"Details for a single selected candidate.\"\"\"\n",
    "    candidate_id: str = Field(..., description=\"The unique ID of the selected winner.\")\n",
    "    justification: str = Field(..., description=\"A 1-2 sentence explanation citing specific evidence from the resume that qualifies this candidate over the others.\")\n",
    "\n",
    "class GroupWinners(BaseModel):\n",
    "    \"\"\"The structured output containing all selected winners and their rationale.\"\"\"\n",
    "    selected_winners: List[WinnerRationale] = Field(..., description=\"A list of 1 or 2 candidates selected as the best in the group, each with a justification.\")\n",
    "    # You can also add a field for an overall comparison summary if helpful\n",
    "    overall_summary: str = Field(..., description=\"A brief summary of the key difference between the winners and the rest of the group.\")\n",
    "\n",
    "selection_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        (\n",
    "            \"You are an expert recruiter conducting a tournament selection round. \"\n",
    "            \"Analyze the provided resumes and select the ONE to TWO best candidates who are most qualified for the role. \"\n",
    "            \"For each winner, you **MUST** provide a concise justification (1-2 sentences) that highlights the most relevant skills/experience \"\n",
    "            \"from their resume compared to the other candidates in the group. Your output must strictly follow the required JSON schema.\"\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        (\n",
    "            \"Based on the **Job Description** provided, select the best 1 or 2 winners \"\n",
    "            \"from the candidates with the following IDs: {all_candidate_ids}.\"\n",
    "            \"\\n\\n**Job Description:**\\n{job_description}\" \n",
    "            \"\\n\\n**Candidate Resumes:**\\n{group_resumes}\"\n",
    "        ),\n",
    "    ),\n",
    "])\n",
    "\n",
    "def tournament_evaluation(\n",
    "    candidates_dict : dict,\n",
    "    job_description : str,\n",
    "    final_target : int = 2,\n",
    "    group_size : int = 5,\n",
    "    \n",
    "):\n",
    "    llm_selector = llm.with_structured_output(schema = GroupWinners)\n",
    "    selection_chain = selection_prompt | llm_selector\n",
    "    candidates_to_evaluate = candidates_dict\n",
    "    \n",
    "    while len(candidates_to_evaluate) > final_target:\n",
    "        candidate_ids = list(candidates_to_evaluate.keys())\n",
    "        random.shuffle(candidate_ids)\n",
    "        winners_of_round = {}\n",
    "        groups = [\n",
    "            candidate_ids[i : i + group_size] for i in range(0, len(candidate_ids), group_size)\n",
    "        ]\n",
    "        for group_index, group_ids in enumerate(groups):\n",
    "            group_resumes_string = '\\n---\\n'.join([\n",
    "                f\"Candidate ID: {cid}\\nResume: {candidates_to_evaluate[cid]}\" for cid in group_ids\n",
    "            ])\n",
    "            try:\n",
    "                # selection_obj is now an instance of the new GroupWinners class\n",
    "                selection_obj = selection_chain.invoke({\n",
    "                    \"group_resumes\": group_resumes_string,\n",
    "                    \"all_candidate_ids\": \", \".join(group_ids),\n",
    "                    \"job_description\": job_description\n",
    "                })\n",
    "                print(\"-\"*20)\n",
    "                print(f\"Overall summary: {selection_obj.overall_summary}\")\n",
    "                print(\"-\"*20)\n",
    "                # Iterate through the structured list of winners and rationales\n",
    "                for winner_data in selection_obj.selected_winners:\n",
    "                    winner_id = winner_data.candidate_id\n",
    "                    \n",
    "                    if winner_id in group_ids:\n",
    "                        # You'll need a way to store the rationale. Let's assume a new state field:\n",
    "                        # state.round_rationales[round_number][winner_id] = winner_data.justification\n",
    "                        \n",
    "                        # For simplicity in this example, we just promote the winner\n",
    "                        winners_of_round[winner_id] = candidates_to_evaluate[winner_id] \n",
    "                        \n",
    "                        print(f\"Candidate {winner_id} promoted. Reason: {winner_data.justification}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating group {group_index + 1}: {e}\")\n",
    "                if group_ids:\n",
    "                    winners_of_round[group_ids[0]] = candidates_to_evaluate[group_ids[0]]\n",
    "        candidates_to_evaluate = winners_of_round\n",
    "        print(f\"Round finished. {len(candidates_to_evaluate)} candidates promoted\")\n",
    "    \n",
    "    return candidates_to_evaluate\n",
    "\n",
    "final_selection = tournament_evaluation(\n",
    "    candidates_dict = candidates_dict,\n",
    "    job_description = job_description\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de153e21",
   "metadata": {},
   "source": [
    "### providing the evaluation criteria instead of the full job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4396b17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Overall summary: Sairaj Suryavanshi and Abhishek Sarda were selected as the best candidates due to their direct experience and proficiency in ETL/ELT processes, data modeling, SQL, and data analytics, as well as their proven ability to leverage advanced data transformation and visualization tools.\n",
      "--------------------\n",
      "Candidate a8ca1108dd06b907 promoted. Reason: Sairaj Suryavanshi is highly qualified with extensive experience in ETL, data modeling, and visualization using PowerBI, combined with a Master's in Data Science focusing on statistical analysis and business-driven solutions, directly aligning with the required skills.\n",
      "Candidate b5142b9b9676a4f9 promoted. Reason: Abhishek Sarda boasts strong expertise in ETL, data modeling, SQL, and cloud computing with Azure, and demonstrates a comprehensive application of advanced analytics through projects that enhance business decision-making, perfectly fitting the criteria.\n",
      "--------------------\n",
      "Overall summary: Eric Zhang and Kevin Wu were selected due to their proven experience and skills in SQL, data visualization, and comprehensive data analytics, which are crucial for meeting business transformation and IT requirements. They both demonstrate strong technical aptitudes in key areas such as BI stacks and automated reporting, setting them apart from others.\n",
      "--------------------\n",
      "Candidate 1f04921d492de215 promoted. Reason: Eric Zhang stands out with over 6 years of IT experience, especially in data visualization and SQL, combined with business intelligence skills in Power BI and exposure to entire BI stacks, making him well-rounded for data governance and analytics roles.\n",
      "Candidate 4a9374ec3b2ca204 promoted. Reason: Kevin Wu's extensive skill set in SQL, data tools, Python, and Power BI, combined with experience in automation and predictive analytics, aligns well with the required data transformation and visualization skills, giving him a competitive edge.\n",
      "--------------------\n",
      "Overall summary: Sahil Dwivedi and Priya Singh stand out due to their comprehensive experience in data analytics, proficiency with SQL and ETL processes, and notable skills in data visualization and cloud computing, which are critical components of the required evaluation criteria.\n",
      "--------------------\n",
      "Candidate 0176a86af53206ae promoted. Reason: This candidate demonstrates strong expertise in data analytics, cloud computing, SQL, ETL processes, data visualization tools, and data modeling, as evidenced by their extensive experience with SQL, Python, Power BI, Tableau, and Google BigQuery, making them well-rounded for all required skills.\n",
      "Candidate e5f8984dfa4cae3b promoted. Reason: Priya Singh is highly skilled in SQL, ETL processes, and cloud computing with Azure fundamentals certification. Her ability to provide actionable insights through tools like Celonis dashboard and strong SQL experience makes her highly suitable for roles involving data governance and business transformation.\n",
      "--------------------\n",
      "Overall summary: The selected candidates distinguish themselves by demonstrating substantial proficiency across all required skills, specifically data analytics, SQL, cloud computing, and ETL processes, greatly surpassing others who lack depth in these areas or do not cover all specified skills.\n",
      "--------------------\n",
      "Candidate f2ca85739fda42e4 promoted. Reason: This candidate has demonstrated comprehensive skills in data analytics, leveraging Python and SQL effectively, and has detailed experience in ETL/ELT processes, data visualization (Tableau), and data modeling through advanced projects and master's-level study.\n",
      "Candidate 8821085d3ae9e0ce promoted. Reason: The candidate provides robust experience in cloud computing (AWS/GCP), SQL, ETL system development, and registered significant project experience in data transformation and visualization tools, which comprehensively matches all required criteria for data analytics and related fields.\n",
      "--------------------\n",
      "Overall summary: The selected candidates possess a strong mix of technical skills across the required criteria, including cloud computing, data modeling, and expertise in both SQL and multiple data visualization tools. They are well-versed in practical applications of data transformation and governance, setting them apart from others who may have more general or less applicable experience.\n",
      "--------------------\n",
      "Candidate 90e485656ce9236d promoted. Reason: This candidate has over two years of direct experience in delivering data analytics and data science solutions across cloud platforms, demonstrating proficiency in SQL, Python, R, and data visualization tools like Power BI and Tableau, along with experience in data modeling, cloud computing, and data governance, making them highly versatile compared to others.\n",
      "Candidate 8ed9c5e84fb32c65 promoted. Reason: Peter Wotherspoon excels with hands-on experience in ETL processes, SQL, and Power BI, along with cloud technologies such as Azure and AWS, providing a strong background in both advanced data processing and visualization essential for transforming data into actionable insights.\n",
      "--------------------\n",
      "Overall summary: Jaewon Yun is the standout candidate due to his comprehensive experience and skills in data analytics, visualization, modeling, and transformation tools, demonstrating a strong ability to apply these in practical projects and academic settings, aligning closely with all the required evaluation criteria.\n",
      "--------------------\n",
      "Candidate fae54469e0d024e8 promoted. Reason: Jaewon Yun has demonstrated proficiency in all required evaluation criteria, particularly showcasing experience in cloud computing, data governance, SQL, ETL/ELT processes, and data visualization through coursework and projects such as working with UNEP on data analysis using R and creating interactive dashboards with Tableau and Power BI. His capstone project and technical skills in data modeling and transformation further establish his capabilities in business transformation and information technology.\n",
      "Round finished. 11 candidates promoted\n",
      "--------------------\n",
      "Overall summary: Abhishek Sarda and Peter Wotherspoon both exhibit strong alignments with the required criteria through practical experiences in data transformation, ETL/ELT processes, and cloud computing, which are less evident in the other candidates' resumes.\n",
      "--------------------\n",
      "Candidate b5142b9b9676a4f9 promoted. Reason: Abhishek Sarda has demonstrated strong expertise in ETL processes, data modeling, and cloud computing, especially with Azure, which aligns closely with the required skills. His involvement in designing scalable databases and implementing ETL pipelines also showcases significant experience in data transformation and governance.\n",
      "Candidate 8ed9c5e84fb32c65 promoted. Reason: Peter Wotherspoon stands out for his extensive work with ETL processes, SQL, and data visualization tools like Power BI, alongside his experience with cloud computing on Azure. His ability to manage large data migration projects and develop automated solutions further supports his qualification for the required criteria.\n",
      "--------------------\n",
      "Overall summary: Sahil Dwivedi and the candidate with the ID 90e485656ce9236d stood out for their strong alignment with all the required criteria, including SQL, ETL processes, data visualization, cloud technologies, and data governance, better matching the job requirements than other candidates who lacked in one or more areas.\n",
      "--------------------\n",
      "Candidate 0176a86af53206ae promoted. Reason: Sahil Dwivedi has a comprehensive background in data analytics, with strong skills in SQL, ETL processes, Power BI, and cloud computing technologies like Snowflake and Azure, which align perfectly with the required skills.\n",
      "Candidate 90e485656ce9236d promoted. Reason: This candidate demonstrates expertise in SQL, data visualization with Power BI and Tableau, and possesses practical experience with cloud platforms and data governance, making them highly suitable for the business transformation and data governance aspects of the role.\n",
      "--------------------\n",
      "Overall summary: The selected candidate showcases significant expertise in key areas such as data analytics, cloud computing, and SQL, combined with demonstrated experience in using ETL processes and data visualization tools, which clearly align with the required skills more effectively than would be seen in an average candidate.\n",
      "--------------------\n",
      "Candidate f2ca85739fda42e4 promoted. Reason: Rafsan Al Mamun stands out due to his comprehensive expertise in Data Analytics and proficiency with Data Visualization and Transformation Tools, demonstrated by his high-impact projects like Tableau dashboards and ETL workflow automation, and his extensive application of SQL and cloud computing in large-scale data projects.\n",
      "Round finished. 5 candidates promoted\n",
      "--------------------\n",
      "Overall summary: The winners were selected for their thorough expertise in both SQL and ETL/ELT processes, and their demonstrated hands-on experience with data visualization tools and cloud technologies, which are essential for business transformation and data governance.\n",
      "--------------------\n",
      "Candidate 0176a86af53206ae promoted. Reason: This candidate has demonstrated proficiency in ETL processes, SQL, Tableau, and cloud technologies such as Snowflake and Azure, which cover critical requirements of data transformation, visualization, and cloud computing specified.\n",
      "Candidate b5142b9b9676a4f9 promoted. Reason: Abhishek Sarda has extensive experience in designing ETL pipelines, data modeling, and SQL, as well as data visualization using Power BI, aligning closely with the critical skills of data governance, cloud computing, and data transformation tools required.\n",
      "Round finished. 2 candidates promoted\n"
     ]
    }
   ],
   "source": [
    "selection_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        (\n",
    "            \"You are an expert recruiter conducting a tournament selection round. \"\n",
    "            \"Analyze the provided resumes and select the ONE to TWO best candidates who are most qualified based **ONLY** on the required skills listed below. \"\n",
    "            \"For each winner, you MUST provide a concise justification (1-2 sentences) that highlights the comparative advantages of choosing that candidate instead of another \"\n",
    "            \"from their resume compared to the other candidates in the group. Your output must strictly follow the required JSON schema.\"\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        (\n",
    "            \"Based on the **REQUIRED EVALUATION CRITERIA** provided, select the best 1 or 2 winners \"\n",
    "            \"from the candidates with the following IDs: {all_candidate_ids}.\"\n",
    "            \"\\n\\n**REQUIRED EVALUATION CRITERIA:**\\n{combined_criteria_list}\"\n",
    "            \"\\n\\n**Candidate Resumes:**\\n{group_resumes}\"\n",
    "        ),\n",
    "    ),\n",
    "])\n",
    "\n",
    "def tournament_evaluation(\n",
    "    candidates_dict : dict,\n",
    "    job_criteria_list : list,\n",
    "    final_target : int = 2,\n",
    "    group_size : int = 5,\n",
    "    \n",
    "):\n",
    "    candidates_to_evaluate = candidates_dict\n",
    "    llm_selector = llm.with_structured_output(schema = GroupWinners)\n",
    "    selection_chain = selection_prompt | llm_selector\n",
    "    all_criteria = \"\\n- \".join(job_criteria_list)\n",
    "    # Store initial group assignments\n",
    "    initial_groups = {}\n",
    "    round_number = 0\n",
    "    \n",
    "    while len(candidates_to_evaluate) > final_target:\n",
    "        candidate_ids = list(candidates_to_evaluate.keys())\n",
    "        random.shuffle(candidate_ids)\n",
    "        winners_of_round = {}\n",
    "        groups = [\n",
    "            candidate_ids[i : i + group_size] for i in range(0, len(candidate_ids), group_size)\n",
    "        ]\n",
    "        for group_index, group_ids in enumerate(groups):\n",
    "            # Store initial group assignments\n",
    "            if round_number == 0:\n",
    "                initial_groups[group_index] = group_ids\n",
    "            \n",
    "            group_resumes_string = '\\n---\\n'.join([\n",
    "                f\"Candidate ID: {cid}\\nResume: {candidates_to_evaluate[cid]}\" for cid in group_ids\n",
    "            ])\n",
    "            try:\n",
    "                # selection_obj is now an instance of the new GroupWinners class\n",
    "                selection_obj = selection_chain.invoke({\n",
    "                    \"group_resumes\": group_resumes_string,\n",
    "                    \"all_candidate_ids\": \", \".join(group_ids),\n",
    "                    \"combined_criteria_list\": all_criteria # <--- PASS THE CRITERIA\n",
    "                })\n",
    "                print(\"-\"*20)\n",
    "                print(f\"Overall summary: {selection_obj.overall_summary}\")\n",
    "                print(\"-\"*20)\n",
    "                # Iterate through the structured list of winners and rationales\n",
    "                for winner_data in selection_obj.selected_winners:\n",
    "                    winner_id = winner_data.candidate_id\n",
    "                    \n",
    "                    if winner_id in group_ids:\n",
    "                        # You'll need a way to store the rationale. Let's assume a new state field:\n",
    "                        # state.round_rationales[round_number][winner_id] = winner_data.justification\n",
    "                        \n",
    "                        # For simplicity in this example, we just promote the winner\n",
    "                        winners_of_round[winner_id] = candidates_to_evaluate[winner_id] \n",
    "                        \n",
    "                        print(f\"Candidate {winner_id} promoted. Reason: {winner_data.justification}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating group {group_index + 1}: {e}\")\n",
    "                if group_ids:\n",
    "                    winners_of_round[group_ids[0]] = candidates_to_evaluate[group_ids[0]]\n",
    "\n",
    "        round_number += 1\n",
    "        candidates_to_evaluate = winners_of_round\n",
    "        print(f\"Round finished. {len(candidates_to_evaluate)} candidates promoted\")\n",
    "    \n",
    "    return candidates_to_evaluate, initial_groups\n",
    "\n",
    "final_selection, initial_groups = tournament_evaluation(\n",
    "    candidates_dict = candidates_dict,\n",
    "    job_criteria_list = ['Data Analytics', \"Business Transformation\", \"Information Technology\", \"Cloud Computing\", \"Data Governance\",\"SQL\",\"ETL/ELT Processes\",\"Data Visualization Tools\",\"Data Transformation Tools\", \"Data Modeling\"]\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36de3253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0176a86af53206ae': 'Sahil Dwivedi\\nData Analyst\\nMelbourne, Vic | +61- 040 220 9556 | sahild091999@gmail.com | LinkedIn\\nSUMMARY\\nExperienced Data Analyst with over 3 years of hands-on experience in data analysis, predictive modeling, and business\\nintelligence. Skilled in using tools such as Python, SQL, Power BI, Tableau, and Excel to collect, clean, transform, and visualize\\nlarge datasets, enabling data-driven decision-making across business functions. Proficient in developing ETL pipelines,\\nforecasting models, and interactive dashboards to optimize inventory, identify trends, and improve operational efficiency. Adept\\nat statistical analysis, machine learning, and data storytelling to communicate insights to both technical and non-technical\\nstakeholders. Strong team collaborator with a proven ability to work cross-functionally in agile environments, solve complex\\nproblems, and deliver impactful analytical solutions. Passionate about leveraging data to drive strategic decisions and continuous\\nbusiness improvement.\\nSKILLS\\nData Analysis & Manipulation: Python, Pandas, NumPy, SQL, Excel\\nData Visualization: Tableau, Power BI, Matplotlib, Seaborn\\nMachine Learning & Predictive Analytics: Scikit-learn, TensorFlow, Keras, XGBoost, KNN, Classification, Regression, Clustering,\\nAnomaly Detection, Time Series Forecasting\\nETL Processes: ETL Pipelines, Data Transformation, AWS S3, Redshift, Data Cleansing\\nStatistical Analysis: SciPy, Statsmodels, R, SPSS, Mathematical Modeling\\nData Modeling & Forecasting: Data Aggregation, Predictive Models, Demand Forecasting, Data Modeling\\nCloud Technologies & Big Data: Snowflake, Google BigQuery, Dataflow, Azure Data Factory, Hadoop, EC2\\nVersion Control & Collaboration: Git, GitHub, Jira, Confluence, Slack\\nEXPERIENCE\\nJohnson & Johnson (J&J), Australia\\nData Analyst Jan 2025 – Present\\n\\uf0b7 Analyzed historical inventory and sales data using SQL to identify trends in device usage and demand across various regions,\\nensuring optimal stock levels and reducing the risk of stockouts.\\n\\uf0b7 Cleaned and merged large datasets with Python (Pandas, NumPy) to prepare data for analysis, handling missing values,\\nduplicates, and ensuring data consistency for accurate reporting and forecasting.\\n\\uf0b7 Developed predictive models using historical usage data to forecast future demand for medical devices, leveraging Python\\nto implement demand forecasting techniques and optimize inventory levels.\\n\\uf0b7 Visualized real-time inventory and sales data using Tableau, creating dynamic dashboards that displayed device usage\\ntrends, stock levels, and regional demand, helping stakeholders make data-driven decisions on inventory replenishment.\\n\\uf0b7 Produced comprehensive reports in Excel, including pivot tables and charts to analyze inventory turnover rates and\\nhighlight slow-moving devices, providing insights for better resource allocation and waste reduction.\\nAustralian Red Cross Lifeblood, Australia\\nData Scientist Intern May 2024 – Dec 2024\\n\\uf0b7 Processed and cleaned large datasets using Python (Pandas, NumPy) to ensure consistency in donor information, such as\\nblood type, donation frequency, and geographic location, optimizing the accuracy of the analysis.\\n\\uf0b7 Developed predictive models in Python, including K-Nearest Neighbors (KNN), to forecast blood type demand in different\\nregions, providing actionable insights for Lifeblood’s supply chain and donation planning.\\n\\uf0b7 Utilized SQL to query and extract donor data, generating key metrics such as monthly donations, donor engagement levels,\\nand donation type distributions to support decision-making.\\n\\uf0b7 Created interactive dashboards in Tableau to visualize donation trends, regional variations, and donor retention rates,\\nhelping stakeholders make informed decisions for targeted donation drives.\\n\\uf0b7 Generated reports in Excel, presenting analysis on donor retention and engagement, identifying trends, and supporting\\nmarketing strategies to boost donor participation in underperforming regions.\\nMouse & Cheese Design Studio, India\\nData Scientist Jan 2023 – Apr 2024\\n\\uf0b7 Constructed predictive models using Python, R, and XGBoost to identify demographic groups with low survey engagement,\\nenabling targeted digital outreach strategies across underrepresented urban populations.\\n\\uf0b7 Executed multivariate statistical analyses to evaluate behavioral and demographic trends affecting citizen participation,\\noffering actionable insights for campaign strategy refinement.\\n\\uf0b7 Monitored user engagement metrics via Google Analytics, interpreting session durations, bounce rates, and interaction\\nflows to enhance platform accessibility and user experience.\\n\\uf0b7 Integrated real-time data pipelines to track and segment website traffic, which supported the marketing and communication\\nteams in planning cost-efficient outreach and digital engagement campaigns.\\n\\uf0b7 Collaborated with UI/UX teams and government stakeholders to translate data insights into user-centric platform\\nimprovements, aligning technical findings with policy-level impact goals.\\nSixD Engineering Solutions\\nData Analyst Jul 2021 – Dec 2022\\n\\uf0b7 Designed automated data ingestion pipelines using Python to collect and preprocess sensor data from manufacturing\\nequipment, reducing dependency on manual intervention and ensuring timely data availability. \\uf0b7 Implemented complex SQL queries to extract actionable insights from structured data related to machine uptime,\\nproduction cycles, and operational throughput, supporting continuous monitoring and reporting.\\n\\uf0b7 Built interactive dashboards in Power BI to visualize plant KPIs including machine utilization, shift performance, and energy\\nusage, enhancing transparency across departments.\\n\\uf0b7 Performed root cause analysis on production delays and inefficiencies by analyzing historical equipment data, aiding\\noperations and finance teams in cost control and resource planning.\\n\\uf0b7 Coordinated with production engineers, quality teams, and financial analysts to ensure that data models and reports aligned\\nwith operational needs and business goals.\\nEDUCATION\\nMaster of Science in Data Science Oct 2024\\nRMIT University, Melbourne, Australia\\nBachelor of Technology in computer science June 2021\\nMaharaja Surajmal Institute of Technology',\n",
       " 'b5142b9b9676a4f9': 'Abhishek Sarda\\n(cid:131) +61455252644 # abhisheksarda0113@gmail.com (cid:239) linkedin.com/in/abhishek-sarda-18a157171\\nProfessional Summary\\nDataprofessionalwithaMaster’sinDataScienceandstrongexpertiseindataanalysis,visualization,and\\ndatabasedevelopment. Skilledindesigning,developing,andoptimizingscalabledatabasesanddatamodelsto\\nsupportlarge-scaledatastorage,analysis,andreporting. ProficientinSQL,Python,AzureServicesandPowerBI,\\nleveragingadvancedanalyticstodriveactionableinsightsandoptimizebusinessstrategies. Experiencedin\\nenhancingcustomerretention,refiningmarketingstrategies,andenablingbusinessgrowththroughpredictive\\nanalytics,automation,andefficientdatamanagement. Passionateaboutcontinuouslearningandapplyingdata\\nsciencesolutionstosolvereal-worldbusinesschallenges.\\nExperience\\nB2CFurniture Nov2022-Aug2024(Part-time),Nov2024-Present(Casual)\\nDataAnalyst Melbourne,Australia\\nAnalyzedcustomerbehaviordatatoidentifytrendsandinsights,resultingina15%increaseincustomerretention\\n•\\nbyoptimizingmarketingstrategies.\\nImplementedmachinelearningmodels(classification&clustering)topredictcustomerpurchasingbehavior,refining\\n•\\nsegmentationandimprovingadconversionratesby10%.\\nDevelopedpredictivemodelsformarkettrendanalysis,providingactionableinsightstothemarketingteamand\\n•\\ncontributingtoimprovedcampaignROIthroughdata-drivendecision-making.\\nBuiltinteractivePowerBIdashboardstovisualizesalesperformance,customersegmentation,andmarketing\\n•\\ncampaignresults,empoweringstakeholderstomakedata-drivendecisions.\\nLeveragedPythontoclean,merge,andtransformdata,automatingreportingprocessestodelivertimelyand\\n•\\naccurateinsights,savingsignificantmanualeffort.\\nPerformedA/Btestingandhypothesistestingtoevaluatemarketingstrategies,providingevidence-based\\n•\\nrecommendationsthatimproveddigitalmarketingeffortsandadperformance.\\nIntegrateddatafrommultipleplatforms(CRM,GoogleAnalytics,marketingtools)intocohesiveSQLdatamodels,\\n•\\nensuringconsistentandaccuratereportingacrossdepartments.\\nCreateddashboardsandreportstocommunicatecomplexdatainsightstostakeholders,enablingbetter\\n•\\ndecision-makingandoptimizationofmarketing,sales,andproductstrategies.\\nConduentBusinessServicesIndiaLLP April2021–Oct2022\\nDataSpecialist Noida,India\\nLedmonthlypaymentprocessingforElectronicChildCare(ECC),analyzingattendancedataandusingadvancedSQL\\n•\\ntoidentifydiscrepancies,ensuringaccuratereconciliationandtimelyreporting.\\nUtilisedSQLtoextract,transform,andintegratedatafromvariousclientdatabases,operationalsystems,and\\n•\\nattendancelogs,ensuringhigh-quality,reliablepaymentreportingforfinanceteams.\\nDevelopedcomplexSQLstoredprocedurestoautomaterecurringfinancialreports,reducingmanualeffortby40%\\n•\\nandimprovingreportingefficiencyandaccuracy,directlysupportingtimelydecision-making.\\nDesignedandimplementedETLpipelinesusingAzureDataFactorytoautomatedataingestion,transformation,and\\n•\\nintegrationfromdiversesourcesintoacentralizeddatawarehouse,enhancingdataaccessibilityandconsistency.\\nLeveragedAzureDatabricksandPySparkforlarge-scaledatatransformations,includingcleansing,deduplication,\\n•\\nandfeatureengineering,ensuringhighdataqualityandconsistentreportingforfinanceandoperationsteams.\\nOptimizedETLworkflowsbyimplementingpartitioning,caching,andjobschedulingtechniques,cuttingdowndata\\n•\\nprocessingtimeby30%andimprovingoverallreportingefficiency.\\nCollaboratedwithfinance,operations,andengineeringteamstointegratediversedatasetsusingPythonandSQL,\\n•\\nstreamliningdatapipelinesandimprovingdecision-makingprocessesacrossdepartments. Projects\\nMetadata-DrivenETLFrameworkforScalableDataProcessing—ADF,Databricks,PySpark,SQL,DataWarehousing\\n* Developedametadata-drivenETLframeworkusingAzureDataFactory,automatingdataingestionand\\ntransformationtoacentralizeddatawarehouse,improvingdataavailabilityandreporting.\\n* CreatedametadatarepositoryinAzureSQLServertostorepipelineconfigurationsanderror-handlinglogic,\\nenablingdynamicETLexecutionwithminimalmanualintervention.\\n* BuiltscalabledataworkflowsinAzureDatabrickswithPySpark,enhancingdataqualitythroughlarge-scale\\ntransformations,cleansing,andenrichment.\\n* ImplementedDeltaLakeonAzureDataLakeStorageformanaginglarge-scaledatawithimprovedquerying\\nefficiency.\\n* OptimizedDatabricksclustersandparallelizedjobs,reducingETLexecutiontime,speedingupdataprocessing.\\n* Designedadatawarehousearchitecturewithdimensionalmodelingtosupportanalyticsandreporting.\\n* DevelopedPowerBIdashboardstovisualizeETLperformanceandkeymetrics,aidinginfasterdecision-making.\\nParkandSwitch|Python,SQL,PySpark,Docker,GitHub,MachineLearning,Azure\\n* DevelopedawebapplicationaspartofmyMaster’scapstoneproject,integratingusageofpublictransportwith\\nprivatevehiclestopromoteeco-friendlytransportationoptionsinMelbourne.\\n* AnalysedhistoricalparkingsensordatausingPySparkonaDockerimagetopredictparkingspotavailability,\\ncreatinginteractivemapstoenhanceuserengagementanddecision-making.\\n* Deployedmachinelearningmodelstorecommendoptimalpublictransportroutes,helpinguserstransition\\nseamlesslyfromprivatevehiclestopublictransit.\\n* UtilisedAzurecloudinfrastructureandApacheKafkaforreal-timedataprocessingandAzureSQLforscalable\\nstorage,enablingefficientdeploymentandmodelscalability.\\n* ImplementedDockerforcontainerizationandusedGitHubforversioncontrol,ensuringsmoothdeploymentand\\nefficientteamcollaboration.\\n* Ledprojectplanning,taskdelegation,andteamcoordination,ensuringtimelydeliverywhilefosteringa\\ncollaborativeteamenvironment.\\n* Deliveredaninnovativesolutionaddressingurbantransportationchallenges,integratingparkingdata,machine\\nlearninginsights,andpublictransportrecommendationsforsustainablecommutingchoices.\\nEducation\\nMonashUniversity June2024\\nMastersofDataScience(WAM:72) Melbourne,Australia\\n* RelevantCoursework:DataWrangling,DataVisualisation,StatisticalDatamodeling(Python),DataWarehouse\\ndesigning,AzureServices,DataEngineering\\nAmityUniversity April2021\\nBachelorsofTechnology:Electronics&Communications(WAM:7.59) Noida,India\\n* RelevantCoursework:Machinelearning,Python,Databases,SQL\\nTechnical Skills\\nLanguages:Python(anditslibraries),R,RShiny,pandas,numpy,scipy,ApachePySpark\\nCloud:AzureDataFactory,ETL,AzureDatabricks\\nDataAnalysis&Visualization:Python,RSHinyTableau,PowerBI,Excel,PredictiveModeling,MachineLearning\\nVersionControl&APITesting:Git,Postman\\nProjectManagement:Agile,Trello,Kanban,LeanKit\\nSoftSkills:Communication,Teamwork,Leadership,CriticalThinking'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd0e123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['a8ca1108dd06b907',\n",
       "  '934d1fdb8a6d1063',\n",
       "  '493d0d19e0150412',\n",
       "  'b5142b9b9676a4f9',\n",
       "  'c5f45dfe145427e1'],\n",
       " 1: ['adbef5e743bf5c48',\n",
       "  'f476ce0793a219b1',\n",
       "  '6d743d56d9692fd0',\n",
       "  '4a9374ec3b2ca204',\n",
       "  '1f04921d492de215'],\n",
       " 2: ['ccfed7e7060c9916',\n",
       "  '0176a86af53206ae',\n",
       "  'e5f8984dfa4cae3b',\n",
       "  'c9cddf5fb828eab6',\n",
       "  'c69f2087775fc760'],\n",
       " 3: ['f2ca85739fda42e4',\n",
       "  '8821085d3ae9e0ce',\n",
       "  'dd417e3f63181d20',\n",
       "  '77aacb560b1dd217',\n",
       "  'de4e24b044e42f42'],\n",
       " 4: ['af62b86c3a326086',\n",
       "  '2b4c705e8b2a0770',\n",
       "  '8ed9c5e84fb32c65',\n",
       "  '6700f96c37c87933',\n",
       "  '90e485656ce9236d'],\n",
       " 5: ['fae54469e0d024e8']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d13261",
   "metadata": {},
   "source": [
    "### incorporating that tournament into a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "814ac096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(2000)\n",
    "from typing import Dict, List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from functions import text_extraction\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from IPython.display import Image, display\n",
    "from collections import defaultdict\n",
    "\n",
    "# environment variables\n",
    "load_dotenv()\n",
    "# folder paths\n",
    "root = \"/\".join(os.getcwd().split(\"/\")[:-1])\n",
    "resumes_path = os.path.join(root, 'data', \"raw\", \"cv\")\n",
    "resumes_list = os.listdir(resumes_path)\n",
    "# llm\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-2024-08-06\"\n",
    ")\n",
    "# candidates dictionary with id : resume pairs\n",
    "candidates_dict = {\n",
    "    file.split(\"-\")[0] : text_extraction(os.path.join(resumes_path, file)) for file in resumes_list\n",
    "}\n",
    "# string with the job description\n",
    "job_description = text_extraction(\n",
    "    file_path = \"/Users/santiagocardenas/Documents/MDSI/202502/internship/internship_project/data/raw/job/data_role_des.txt\"\n",
    ")\n",
    "# prompt for selection\n",
    "selection_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        (\n",
    "            \"You are an expert recruiter conducting a tournament selection round. \"\n",
    "            \"Analyze the provided resumes and select the ONE to TWO best candidates who are most qualified based **ONLY** on the required skills listed below. \"\n",
    "            \"For each winner, you MUST provide a concise justification (1-2 sentences) that highlights the comparative advantages of choosing that candidate instead of another \"\n",
    "            \"from their resume compared to the other candidates in the group. Your output must strictly follow the required JSON schema.\"\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        (\n",
    "            \"Based on the **REQUIRED EVALUATION CRITERIA** provided, select the best 1 or 2 winners \"\n",
    "            \"from the candidates with the following IDs: {all_candidate_ids}.\"\n",
    "            \"\\n\\n**REQUIRED EVALUATION CRITERIA:**\\n{combined_criteria_list}\"\n",
    "            \"\\n\\n**Candidate Resumes:**\\n{group_resumes}\"\n",
    "        ),\n",
    "    ),\n",
    "])\n",
    "# schema for structure output\n",
    "class WinnerRationale(BaseModel):\n",
    "    \"\"\"Details for a single selected candidate.\"\"\"\n",
    "    candidate_id: str = Field(..., description=\"The unique ID of the selected winner.\")\n",
    "    justification: str = Field(..., description=\"A 1-2 sentence explanation citing specific evidence from the resume that qualifies this candidate over the others.\")\n",
    "\n",
    "class GroupWinners(BaseModel):\n",
    "    \"\"\"The structured output containing all selected winners and their rationale.\"\"\"\n",
    "    selected_winners: List[WinnerRationale] = Field(..., description=\"A list of 1 or 2 candidates selected as the best in the group, each with a justification.\")\n",
    "    # You can also add a field for an overall comparison summary if helpful\n",
    "    overall_summary: str = Field(..., description=\"A brief summary of the key difference between the winners and the rest of the group.\")\n",
    "\n",
    "class SampleState(BaseModel):\n",
    "    candidates_dict : Optional[Dict[str, str]]\n",
    "    job_description : Optional[str]\n",
    "    job_criteria : Optional[List[str]]\n",
    "    batch_size : Optional[int] = 5\n",
    "    top_candidates_num : Optional[int] = 3\n",
    "    # store the top candidates\n",
    "    top_candidates_dict : Optional[Dict] = {}\n",
    "    # initial groups of candidates\n",
    "    initial_tournament_groups : Optional[Dict] = {}\n",
    "    # overall summary per round\n",
    "    round_summaries : Optional[Dict] = {}\n",
    "    # rationale for the selected candidates\n",
    "    round_rationales : Optional[Dict] = {}\n",
    "    \n",
    "    \n",
    "# unique node\n",
    "def candidates_tourname(state : SampleState) ->SampleState:\n",
    "    candidates_to_evaluate = state.candidates_dict\n",
    "    all_criteria = ', '.join(state.job_criteria)\n",
    "    llm_selector = llm.with_structured_output(schema = GroupWinners)\n",
    "    selection_chain = selection_prompt | llm_selector\n",
    "    # Store initial group assignments\n",
    "    initial_groups = defaultdict(dict)\n",
    "    round_number = 0\n",
    "    round_summaries = defaultdict(dict)\n",
    "    round_rationales = defaultdict(dict)\n",
    "    while len(candidates_to_evaluate) > state.top_candidates_num:\n",
    "        candidate_ids = list(candidates_to_evaluate.keys())\n",
    "        random.shuffle(candidate_ids)\n",
    "        winners_of_round = {}\n",
    "        groups = [\n",
    "            candidate_ids[i : i + state.batch_size] for i in range(0, len(candidate_ids), state.batch_size)\n",
    "        ]\n",
    "        for group_index, group_ids in enumerate(groups):\n",
    "            # Store initial group assignments\n",
    "            if round_number == 0:\n",
    "                initial_groups[group_index] = group_ids\n",
    "            \n",
    "            group_resumes_string = '\\n---\\n'.join([\n",
    "                f\"Candidate ID: {cid}\\nResume: {candidates_to_evaluate[cid]}\" for cid in group_ids\n",
    "            ])\n",
    "            try:\n",
    "                # selection_obj is now an instance of the new GroupWinners class\n",
    "                selection_obj = selection_chain.invoke({\n",
    "                    \"group_resumes\": group_resumes_string,\n",
    "                    \"all_candidate_ids\": \", \".join(group_ids),\n",
    "                    \"combined_criteria_list\": all_criteria # <--- PASS THE CRITERIA\n",
    "                })\n",
    "                print(\"-\"*20)\n",
    "                print(f\"Overall summary: {selection_obj.overall_summary}\")\n",
    "                print(\"-\"*20)\n",
    "                round_summaries[round_number][group_index] = selection_obj.overall_summary\n",
    "                # state.round_summaries[round_number][group_index] = selection_obj.overall_summary\n",
    "                # Iterate through the structured list of winners and rationales\n",
    "                for winner_data in selection_obj.selected_winners:\n",
    "                    winner_id = winner_data.candidate_id\n",
    "                    \n",
    "                    if winner_id in group_ids:\n",
    "                        # You'll need a way to store the rationale. Let's assume a new state field:\n",
    "                        # state.round_rationales[round_number][winner_id] = winner_data.justification\n",
    "                        \n",
    "                        # For simplicity in this example, we just promote the winner\n",
    "                        winners_of_round[winner_id] = candidates_to_evaluate[winner_id] \n",
    "                        \n",
    "                        print(f\"Candidate {winner_id} promoted. Reason: {winner_data.justification}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating group {group_index + 1}: {e}\")\n",
    "                if group_ids:\n",
    "                    winners_of_round[group_ids[0]] = candidates_to_evaluate[group_ids[0]]\n",
    "\n",
    "        round_number += 1\n",
    "        candidates_to_evaluate = winners_of_round\n",
    "        print(f\"Round finished. {len(candidates_to_evaluate)} candidates promoted\")\n",
    "    state.top_candidates_dict = candidates_to_evaluate\n",
    "    state.initial_tournament_groups = initial_groups\n",
    "    state.round_summaries = round_summaries\n",
    "    return state\n",
    "\n",
    "# simple graph\n",
    "simple_graph = StateGraph(SampleState)\n",
    "simple_graph.add_node(\"candidates_tourname\", candidates_tourname)\n",
    "simple_graph.add_edge(START, \"candidates_tourname\")\n",
    "simple_graph.add_edge(\"candidates_tourname\", END)\n",
    "compiled_graph = simple_graph.compile()\n",
    "\n",
    "display(Image(compiled_graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b65e95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Overall summary: Both Sahil Dwivedi and Abhishek Sarda provide a strong match to all required skills, showcasing significant practical experience and expertise in data analytics, cloud platforms, ETL processes, and data visualization tools, unlike other candidates whose experience did not cover the required broad technical criteria as comprehensively.\n",
      "--------------------\n",
      "Candidate 0176a86af53206ae promoted. Reason: Sahil Dwivedi has extensive experience in data analysis, ETL processes, cloud technologies, and data visualization, evident from working with tools and platforms such as AWS, Azure, and Tableau, making him highly proficient across all required criteria.\n",
      "Candidate b5142b9b9676a4f9 promoted. Reason: Abhishek Sarda demonstrates comprehensive skills in data analytics, cloud computing, ETL/ELT processes, and data modeling, with practical experience in SQL, Power BI, Azure, and implementing complex ETL workflows, meeting all the specified evaluation criteria.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m sample_input \u001b[38;5;241m=\u001b[39m SampleState(\n\u001b[1;32m      2\u001b[0m     candidates_dict \u001b[38;5;241m=\u001b[39m candidates_dict,\n\u001b[1;32m      3\u001b[0m     job_description \u001b[38;5;241m=\u001b[39m job_description,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     top_candidates_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_input\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/langgraph/pregel/main.py:3026\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3023\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3024\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   3027\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3028\u001b[0m     config,\n\u001b[1;32m   3029\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   3030\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[1;32m   3033\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[1;32m   3034\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   3035\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   3036\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   3037\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3039\u001b[0m ):\n\u001b[1;32m   3040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3041\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/langgraph/pregel/_runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[24], line 109\u001b[0m, in \u001b[0;36mcandidates_tourname\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    104\u001b[0m group_resumes_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCandidate ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResume: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcandidates_to_evaluate[cid]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cid \u001b[38;5;129;01min\u001b[39;00m group_ids\n\u001b[1;32m    106\u001b[0m ])\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# selection_obj is now an instance of the new GroupWinners class\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     selection_obj \u001b[38;5;241m=\u001b[39m \u001b[43mselection_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroup_resumes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_resumes_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall_candidate_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcombined_criteria_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_criteria\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# <--- PASS THE CRITERIA\u001b[39;49;00m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall summary: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselection_obj\u001b[38;5;241m.\u001b[39moverall_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:3245\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3243\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3244\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3245\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:5710\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5703\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5705\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5708\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5709\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5711\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5712\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5713\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5714\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:395\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 395\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    405\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1023\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1021\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1022\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:840\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    839\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 840\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m         )\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1089\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1093\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1152\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1149\u001b[0m payload\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1152\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m     )\n\u001b[1;32m   1156\u001b[0m     response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:183\u001b[0m, in \u001b[0;36mCompletions.parse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparser\u001b[39m(raw_completion: ChatCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedChatCompletion[ResponseFormatT]:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[1;32m    178\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    179\u001b[0m         chat_completion\u001b[38;5;241m=\u001b[39mraw_completion,\n\u001b[1;32m    180\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mchat_completion_tools,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/openai/_base_client.py:982\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 982\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    988\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Documents/MDSI/202502/internship/internship_project/.venv/lib/python3.10/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/ssl.py:1258\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1256\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1257\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/ssl.py:1131\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_input = SampleState(\n",
    "    candidates_dict = candidates_dict,\n",
    "    job_description = job_description,\n",
    "    job_criteria = ['Data Analytics', \"Business Transformation\", \"Information Technology\", \"Cloud Computing\", \"Data Governance\",\"SQL\",\"ETL/ELT Processes\",\"Data Visualization Tools\",\"Data Transformation Tools\", \"Data Modeling\"],\n",
    "    batch_size = 5,\n",
    "    top_candidates_num = 3\n",
    ")\n",
    "results = compiled_graph.invoke(\n",
    "    input = sample_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4a0436a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'candidates_dict': {'af62b86c3a326086': 'ASHWAT KUMAR CHAMAN\\n+971 558078154 | ashwatkumar.ak1@gmail.com | linkedin.com/in/ashwat-kumar-chaman-144650173 | Dubai\\n------------------------------------------------------------------------------------------------------------------------------\\nEDUCATION\\nChandigarh University, Mohali\\nMaster of Business Administration, Business Analytics (07/2022 - 05/2024)\\nPanjab University, Chandigarh\\nBachelor of Arts (07/2018 - 07/2021)\\nEXPERIENCE\\nInnovative Incentives and Rewards Pvt. Ltd. Mohali\\nProject Coordinator (07/2024-04/2025)\\n• Led data-driven operations for a premium loyalty program targeting retailers and distributors, enhancing program\\nperformance and engagement through real-time data tracking.\\n• Designed automated reporting systems using Power BI, improving the analysis of key metrics like participation and\\nreward redemption.\\n• Utilized SQL and Excel to streamline data pipelines, ensuring accurate tracking of member activities and interactions.\\n• Collaborated with cross-functional teams to create dashboards and reports, enabling data-driven\\ndecision-making and optimizing program incentives.\\n• Managed a team of 20 customer care executives, ensuring seamless member support, efficient issue resolution, and\\nconsistent program satisfaction.\\nThe Future University, Mohali\\nData Analyst (01/2023 - 06/2024)\\n• Implemented Power BI for data visualization, boosting decision-making and increasing revenue by 30%, while\\nreducing reporting time by 50%.\\n• Led Mix panel analytics integration to optimize funnel efficiency, working closely with marketing teams and\\nimproving conversion rates by 30%.\\n• Developed a \"super customer\" database using SQL queries to assign lead scores, resulting in a 10% improvement in\\nconversion rates and a 200% increase in average retail price units.\\n• Led and mentored a team of 10 Business Development Associates, improving team efficiency and driving a monthly\\nrevenue increase of INR 15 lacs.\\n• Designed and deployed dashboards for leadership and cross-functional teams, enhancing communication and data\\ninsights.\\nThe Times of India, Chandigarh\\nMarketing Intern (06/2023 - 07/2023)\\n• Accurately input large volumes of data into company databases using Salesforce and SAP. Provided general\\nadministrative support to the market research team as needed\\nPROJECTS\\nResearch Publication: A study on Self-efficacy and job satisfaction in Early Childhood Educators (Published: December\\n22,2020)\\nProject: Sustainable Factors for Innovation in Service Based Industries.\\nSKILLS\\n● Data Analysis: SQL, Excel, Power BI, Google Sheets\\n● Data Visualization: Power BI, Tableau\\n● Automation & Scripting: Advanced Excel Formulas, App Script\\n● Business Intelligence: CRM Analytics, Mixpanel, Alteryx\\n● A/B Testing & Optimization: Campaign Analysis, User Behavior Insights\\n● Reporting & Dashboarding: Power BI, Google Sheets, Excel\\n● Team Leadership & Collaboration: Strategic Planning, KPI Tracking',\n",
       "  '2b4c705e8b2a0770': \"MOHAMMAD TOUSIF SHAHRIAR\\nPassionate about constructing machine learning models to drive positive change through insightful\\nanalytics, I am dedicated to harnessing the power of data to tackle complex challenges and create\\nmeaningful solutions. With a commitment to innovation and continuous learning, I aspire to\\nleverage advanced techniques in machine learning to address pressing societal issues.\\nEducation\\nBachelor of Computer Science (Major: Data Science)\\nSwinburne University of Technology\\nContact 02/2022 - Present\\nCurrent CGPA: 3.44\\nCourses\\nPhone Introduction to Programming - D\\n+61475773442 Data Science Principles - HD\\nIntroduction to Artificial Intellingence - D\\nFoundations of Statistics - D\\nEmail\\ntoyonmohammad10@gmail.com Data Management and Analytics - HD\\nDatabase Systems - HD\\nLinkedIn Professional Experience\\nGitHub\\nWeb developer\\n99aupairs\\nSkills\\n08/2023 - 10/2023 Melbourne\\nImproved website content using WordPress by rectifying inconsistencies and\\nTechnical adding missing information.\\nImplemented a Career page utilizing ZohoCRM to streamline recruitment\\nPython\\nprocesses.\\nC#\\nSQL\\nJavascript Additional Experience\\nHTML\\nCSS\\nVue.js Store Team Member\\nKNIME\\nColes\\nPowerBI\\n02/2023 - Present Melbourne\\nOracle PL/SQL\\nMaintained availability to promptly respond to customer inquiries and offer\\nInterpersonal personalized guidance.\\nActively listened to customer concerns, providing empathetic support tailored\\nCommunication\\nto their individual needs.\\nProblem-Solving\\nTeamwork\\nPremium Hospitality Experience Ambassador\\nTime Management\\nTennis Australia\\n01/2023 - 02/2023 Melbourne\\nActively listened to customer inquiries to understand their needs and\\nconcerns thoroughly.\\nProvided tailored solutions to ensure customer satisfaction and address\\nindividual requirements.\\nMaintained effective communication with other departments to support\\ncomprehensive customer service.\\nStudent Volunteer\\nSwinburne University of Technology Melbourne\\n02/2022 - Present\\nProvided warm welcomes to new international students upon their arrival.\\nEnsured seamless guidance to their designated locations, ensuring a smooth\\ntransition into their new environment. Projects\\nHate Speech Detection\\n12/2023 - 01/2024\\nConducted thorough data preparation, including preprocessing and exploratory analysis, to ensure dataset\\nreadiness for machine learning.\\nDeveloped three models using scikit-learn, assessing their performance using precision metrics.\\nOptimized the top-performing model through hyperparameter tuning using RandomizedSearchCV for enhanced\\nperformance.\\nBike Store Analysis\\n12/2023 - 01/2024\\nConducted comprehensive data analysis on a Kaggle dataset obtained from a bike store, focusing on business\\noperations.\\nMeticulously created and populated tables based on the Entity-Relationship Diagram (ERD) to ensure data\\nintegrity and coherence.\\nCrafted precise queries to extract valuable insights, facilitating informed decision-making and strategic\\nplanning for the business.\\nEmphasized efficiency and accuracy in utilizing relevant data entities for analysis, optimizing resource\\nutilization.\\nBlockchain Transaction Information Tracing Platform\\n08/2023 - 10/2023\\nMeticulously set up a Neo4j database with interconnected nodes to represent intricate relationships and data\\nstructures.\\nDeveloped targeted queries tailored to extract specific insights, optimizing the utilization of the graph\\ndatabase's rich information.\\nIntegrated the database seamlessly into the API creation process, facilitating efficient access and utilization of\\nvaluable data for application development.\\nClub Database for Archery Store Recording\\n03/2023 - 05/2023\\nDesigned a database architecture with a focus on effectiveness and scalability, ensuring adaptability to future\\ngrowth and evolving project needs.\\nUtilized a meticulous approach to craft a structured database capable of accommodating expanding data\\nrequirements.\\nDeveloped queries incorporating diverse aggregate functions to enhance database functionality, enabling the\\ngeneration of valuable insights aligned with project objectives.\\nReferees\\nAvailable upon request\",\n",
       "  'b5142b9b9676a4f9': 'Abhishek Sarda\\n(cid:131) +61455252644 # abhisheksarda0113@gmail.com (cid:239) linkedin.com/in/abhishek-sarda-18a157171\\nProfessional Summary\\nDataprofessionalwithaMaster’sinDataScienceandstrongexpertiseindataanalysis,visualization,and\\ndatabasedevelopment. Skilledindesigning,developing,andoptimizingscalabledatabasesanddatamodelsto\\nsupportlarge-scaledatastorage,analysis,andreporting. ProficientinSQL,Python,AzureServicesandPowerBI,\\nleveragingadvancedanalyticstodriveactionableinsightsandoptimizebusinessstrategies. Experiencedin\\nenhancingcustomerretention,refiningmarketingstrategies,andenablingbusinessgrowththroughpredictive\\nanalytics,automation,andefficientdatamanagement. Passionateaboutcontinuouslearningandapplyingdata\\nsciencesolutionstosolvereal-worldbusinesschallenges.\\nExperience\\nB2CFurniture Nov2022-Aug2024(Part-time),Nov2024-Present(Casual)\\nDataAnalyst Melbourne,Australia\\nAnalyzedcustomerbehaviordatatoidentifytrendsandinsights,resultingina15%increaseincustomerretention\\n•\\nbyoptimizingmarketingstrategies.\\nImplementedmachinelearningmodels(classification&clustering)topredictcustomerpurchasingbehavior,refining\\n•\\nsegmentationandimprovingadconversionratesby10%.\\nDevelopedpredictivemodelsformarkettrendanalysis,providingactionableinsightstothemarketingteamand\\n•\\ncontributingtoimprovedcampaignROIthroughdata-drivendecision-making.\\nBuiltinteractivePowerBIdashboardstovisualizesalesperformance,customersegmentation,andmarketing\\n•\\ncampaignresults,empoweringstakeholderstomakedata-drivendecisions.\\nLeveragedPythontoclean,merge,andtransformdata,automatingreportingprocessestodelivertimelyand\\n•\\naccurateinsights,savingsignificantmanualeffort.\\nPerformedA/Btestingandhypothesistestingtoevaluatemarketingstrategies,providingevidence-based\\n•\\nrecommendationsthatimproveddigitalmarketingeffortsandadperformance.\\nIntegrateddatafrommultipleplatforms(CRM,GoogleAnalytics,marketingtools)intocohesiveSQLdatamodels,\\n•\\nensuringconsistentandaccuratereportingacrossdepartments.\\nCreateddashboardsandreportstocommunicatecomplexdatainsightstostakeholders,enablingbetter\\n•\\ndecision-makingandoptimizationofmarketing,sales,andproductstrategies.\\nConduentBusinessServicesIndiaLLP April2021–Oct2022\\nDataSpecialist Noida,India\\nLedmonthlypaymentprocessingforElectronicChildCare(ECC),analyzingattendancedataandusingadvancedSQL\\n•\\ntoidentifydiscrepancies,ensuringaccuratereconciliationandtimelyreporting.\\nUtilisedSQLtoextract,transform,andintegratedatafromvariousclientdatabases,operationalsystems,and\\n•\\nattendancelogs,ensuringhigh-quality,reliablepaymentreportingforfinanceteams.\\nDevelopedcomplexSQLstoredprocedurestoautomaterecurringfinancialreports,reducingmanualeffortby40%\\n•\\nandimprovingreportingefficiencyandaccuracy,directlysupportingtimelydecision-making.\\nDesignedandimplementedETLpipelinesusingAzureDataFactorytoautomatedataingestion,transformation,and\\n•\\nintegrationfromdiversesourcesintoacentralizeddatawarehouse,enhancingdataaccessibilityandconsistency.\\nLeveragedAzureDatabricksandPySparkforlarge-scaledatatransformations,includingcleansing,deduplication,\\n•\\nandfeatureengineering,ensuringhighdataqualityandconsistentreportingforfinanceandoperationsteams.\\nOptimizedETLworkflowsbyimplementingpartitioning,caching,andjobschedulingtechniques,cuttingdowndata\\n•\\nprocessingtimeby30%andimprovingoverallreportingefficiency.\\nCollaboratedwithfinance,operations,andengineeringteamstointegratediversedatasetsusingPythonandSQL,\\n•\\nstreamliningdatapipelinesandimprovingdecision-makingprocessesacrossdepartments. Projects\\nMetadata-DrivenETLFrameworkforScalableDataProcessing—ADF,Databricks,PySpark,SQL,DataWarehousing\\n* Developedametadata-drivenETLframeworkusingAzureDataFactory,automatingdataingestionand\\ntransformationtoacentralizeddatawarehouse,improvingdataavailabilityandreporting.\\n* CreatedametadatarepositoryinAzureSQLServertostorepipelineconfigurationsanderror-handlinglogic,\\nenablingdynamicETLexecutionwithminimalmanualintervention.\\n* BuiltscalabledataworkflowsinAzureDatabrickswithPySpark,enhancingdataqualitythroughlarge-scale\\ntransformations,cleansing,andenrichment.\\n* ImplementedDeltaLakeonAzureDataLakeStorageformanaginglarge-scaledatawithimprovedquerying\\nefficiency.\\n* OptimizedDatabricksclustersandparallelizedjobs,reducingETLexecutiontime,speedingupdataprocessing.\\n* Designedadatawarehousearchitecturewithdimensionalmodelingtosupportanalyticsandreporting.\\n* DevelopedPowerBIdashboardstovisualizeETLperformanceandkeymetrics,aidinginfasterdecision-making.\\nParkandSwitch|Python,SQL,PySpark,Docker,GitHub,MachineLearning,Azure\\n* DevelopedawebapplicationaspartofmyMaster’scapstoneproject,integratingusageofpublictransportwith\\nprivatevehiclestopromoteeco-friendlytransportationoptionsinMelbourne.\\n* AnalysedhistoricalparkingsensordatausingPySparkonaDockerimagetopredictparkingspotavailability,\\ncreatinginteractivemapstoenhanceuserengagementanddecision-making.\\n* Deployedmachinelearningmodelstorecommendoptimalpublictransportroutes,helpinguserstransition\\nseamlesslyfromprivatevehiclestopublictransit.\\n* UtilisedAzurecloudinfrastructureandApacheKafkaforreal-timedataprocessingandAzureSQLforscalable\\nstorage,enablingefficientdeploymentandmodelscalability.\\n* ImplementedDockerforcontainerizationandusedGitHubforversioncontrol,ensuringsmoothdeploymentand\\nefficientteamcollaboration.\\n* Ledprojectplanning,taskdelegation,andteamcoordination,ensuringtimelydeliverywhilefosteringa\\ncollaborativeteamenvironment.\\n* Deliveredaninnovativesolutionaddressingurbantransportationchallenges,integratingparkingdata,machine\\nlearninginsights,andpublictransportrecommendationsforsustainablecommutingchoices.\\nEducation\\nMonashUniversity June2024\\nMastersofDataScience(WAM:72) Melbourne,Australia\\n* RelevantCoursework:DataWrangling,DataVisualisation,StatisticalDatamodeling(Python),DataWarehouse\\ndesigning,AzureServices,DataEngineering\\nAmityUniversity April2021\\nBachelorsofTechnology:Electronics&Communications(WAM:7.59) Noida,India\\n* RelevantCoursework:Machinelearning,Python,Databases,SQL\\nTechnical Skills\\nLanguages:Python(anditslibraries),R,RShiny,pandas,numpy,scipy,ApachePySpark\\nCloud:AzureDataFactory,ETL,AzureDatabricks\\nDataAnalysis&Visualization:Python,RSHinyTableau,PowerBI,Excel,PredictiveModeling,MachineLearning\\nVersionControl&APITesting:Git,Postman\\nProjectManagement:Agile,Trello,Kanban,LeanKit\\nSoftSkills:Communication,Teamwork,Leadership,CriticalThinking',\n",
       "  'de4e24b044e42f42': 'Melisa TOP GÖKIRMAK\\nAddress: Docklands Melbourne 3008 | Mobile: +61 413 037 993\\nE-Mail: topoglumelisa@gmail.com\\nLinkedIn: linkedin.com/in/melisatop | Driving License: B Class\\nProfessional Summary\\nDynamic and results-oriented International Sales Supervisor with 3.5 years of experience in B2B sales,\\nteam leadership, and client relationship management. Proven expertise in executing successful sales\\nstrategies, streamlining processes, and achieving KPIs. Currently pursuing a Master’s degree in Business\\nAnalytics (AI in Business) to deepen analytical and technical skills. Multilingual and adept at working in\\ninternational environments.\\nProfessional Experience\\nEarthwatch Australia | Data Input & Process Development Intern\\nCarlton | March 2025 -Present\\n\\uf0b7 Digitalisation and optimisation of organisational processes\\n\\uf0b7 Collaborated with IT and management teams to customise the CRM based on the\\norganisation’s specific needs and workflows\\n\\uf0b7 Document and data management via Microsoft SharePoint\\n\\uf0b7 Task tracking and project coordination through the ClickUp platform\\n\\uf0b7 Active participation in team communication and meetings in the office\\n\\uf0b7 Transferred company data into the new CRM system\\nBD Electronics Ltd | International Sales Supervisor\\nRemote | December 2020 – March 2024\\n\\uf0b7 Led a team of 3 sales representatives, conducting daily and weekly team meetings to align on\\ngoals and strategies.\\n\\uf0b7 Established and maintained relationships with global clients through cold and warm calls, emails,\\nand virtual meetings.\\n\\uf0b7 Prepared price quotations, managed purchase orders, and ensured timely deliveries by\\ncollaborating with logistics teams.\\n\\uf0b7 Developed and implemented targeted sales strategies, contributing to significant revenue growth.\\n\\uf0b7 Conducted recruitment processes, monitored KPIs, and mentored team members for performance\\nimprovement.\\nInternships\\n\\uf0b7 Kale Pratt & Whitney | Financial Affairs Intern\\nManisa, Turkey | March 2020 – April 2020\\no Registered invoices using CANIAS ERP and supported the accounting team.\\n\\uf0b7 Aegean Exporters Association | Finance Intern\\nIzmir, Turkey | July 2019 – August 2019\\no Edited invoices and offset vouchers.\\n\\uf0b7 Gates Powertrain | Human Resources Intern\\nIzmir, Turkey | August 2019 – September 2019 Awards & Projects\\n\\uf0b7 AFS Exchange Student Full Scholarship (Denmark) | Turkish Cultural Foundation (2011-2012)\\n\\uf0b7 MCBU Social Responsibility Project | Volunteer (2018-2019)\\n\\uf0b7 \"Hakkari Gel Gari\" AFS Intercultural Exchange Project (2016)\\nEducation\\nDeakin University | Master of Business Analytics (Artificial Intelligence in Business)\\nMelbourne, Australia | October 2024 – Present. 25% Scholarship GPA 73/100 (1st trimester)\\nManisa Celal Bayar University | Bachelor of Business Administration\\nManisa, Turkey | September 2016 – July 2020. GPA: 3.09/4.00 | Graduated with Honors\\nPoznan University of Life Sciences | Erasmus Program\\nPoznan, Poland | September 2019 – February 2020. GPA: 3.77/4.00\\nUdby Skole | AFS Exchange Program\\nDenmark,Tuse Næs | July 2011 – August 2012. 100% Scholarship\\nSkills\\nLanguages:\\n\\uf0b7 Turkish (Native), English (B2+ to C1), Danish (A2)\\nTechnical Skills:\\n\\uf0b7 Microsoft Office, Zoho CRM, Bitrix24, PALMA ERP, Basic SAP\\n\\uf0b7 Currently learning: Phyton, R Programming Language, SQL,Altair AI, Tableau,Excel-Data,\\nMicrosoft Dynamics\\nSoft Skills:\\n\\uf0b7 Team Leadership, Communication, Strategic Planning, Problem Solving,Team Work\\nHobbies & Interests\\nTraveling, Camping, Kickboxing, Horse Riding, Underwater Rugby, Beach Volleyball, Flute.\\nReferences\\nBuğra Sal | Owner of Dunyasal Atlas Education Consultancy\\n+61416120096\\nNesli Kiraz\\n+61413596268 | Project Manager',\n",
       "  '8821085d3ae9e0ce': 'Mashruk Jahangir\\nhttps://www.linkedin.com/in/mashruk-jahangir-2559421ba\\n116 Main Drive, Macleod, VIC, 3085\\n+61468897969 jmashruk@outlook.com\\nPROFESSIONAL SUMMARY\\nVersatile Software Engineer with expertise in AI/ML, computer vision, NLP, and cloud infrastructure (AWS/GCP), complemented\\nby recent experience in cybersecurity solutions. Specializes in developing optimized machine learning models, containerized\\napplications, and scalable systems. Seeking challenging roles that leverage my strong technical foundation in Python\\ndevelopment, AI engineering, and cloud architecture to drive innovation across diverse technology domains.\\nWORK EXPERIENCE\\nFIRCY Adelaide, South Australia (Remote)\\nSoftware Engineer (Part-time) (December, 2023 – Present)\\n• Refactored legacy codebase and implemented architectural improvements, reducing system response time by 40%, decreasing\\nmaintenance overhead by 80%, and significantly increasing code readability and documentation.\\n• Containerized applications and implemented automated deployment across Docker Swarm, reducing deployment time to\\nwithin minutes and enabling seamless scaling during peak usage periods.\\n• Led migration of processes from AWS ecosystem to on-premises infrastructure, resulting in 35% reduction in monthly\\noperational costs while maintaining system performance.\\n• Designed and implemented comprehensive AWS decoy systems including S3 buckets, IAM/Identity, Route53, Lambda\\nfunctions, and Windows servers that identified over 50K+ threat actors, enhancing security infrastructure and threat detection\\ncapabilities.\\n• Developed and deployed similar decoy and honeypot systems across GCP, utilizing services such as GCP CDN origin, GCP bucket,\\nand GCP cloud DNS, with full end-to-end processes from service deployment to log collection, parsing, storing, and analysis.\\n• Built specialized honeypots using BIND DNS on Ubuntu and Windows infrastructure, analyzing firewall logs, RDP access logs,\\nand IIS logs to identify attack patterns and malicious activities.\\n• Developed comprehensive REST APIs using FastAPI to enable customers to deploy and manage custom decoys, providing\\nprogrammatic access to system functionality.\\n• Created a customer decoy management portal leveraging custom APIs, allowing customers to deploy and manage decoys\\ndirectly from the UI.\\n• Successfully integrated and deployed cloud systems within Cloudflare environment, expanding service capabilities.\\n• Created custom scripts to access and operate AWS services (boto3 functionalities).\\nTru Recognition Ltd. Melbourne, Victoria\\nSoftware Engineer (Part-time) (September, 2023 – December, 2023\\n• Performed robust unit testing on Nvidia DeepStream components on a GStreamer computer vision pipeline.\\n• Written test automation scripts using Pytest testing framework to increase the code coverage from 0 to 80%+.\\n• Fixed bugs, analyzed and performed test driven development (TDD) on current computer vision pipelines.\\nNagad Ltd. Dhaka, Bangladesh\\nData Engineer Intern (February, 2023 – July, 2023)\\n• Performed geographic data analysis on real time transactions.\\n• Developed a real time map data visualization tool using Python and JavaScript.\\n• Automated KYC data verification using Python.\\n• Developed an ETL system to categorize KYC data using SQL, OracleDB and Python.\\n• Developed a full-fledged ETL software along with GUI to perform multimodal data transformation and transfer at an optimized\\nspeed.\\nEra Infotech Ltd. Dhaka, Bangladesh\\nSoftware Engineering Intern (AI Team) (December, 2022 – February, 2023)\\n• Improving the existing eKYC system model using contemporary technologies like Computer Vision and Image Processing.\\n• Developing Deep Learning models to detect facial presentation attack during customer onboarding and ID verification.\\n• Developing systems to automate authentication of bank cheques and signatures using Computer Vision.\\n• Created Restful API’s using Flask framework to receive requests and deliver analytics.\\nAonic (formerly Poladrone) Cyberjaya, Malaysia\\nAI Engineering Intern (July, 2021 – November, 2021)\\n• Developed multiple image classification, object detection/tracking and image segmentation models for agricultural use cases\\nto gain insights.\\n• Deployed trained models on a Jetson Nano and evaluated inference performances.\\n• Used NVIDIA dockers to develop real-time object tracking applications.\\n• Assisted UAV engineering team with serial communication between Jetson Nano and Pixhawk flight controller to manoeuvre\\nthe drone using the Jetson Nano. • Used NVIDIA TensorRT SDK to optimize model performance.\\n• Got first-hand experience in CUDA programming, model quantization, DeepStream SDK, OpenVINO Toolkit and ONNX format.\\nUniversiti Teknologi Malaysia Johor Bahru, Malaysia\\nUndergraduate Research Assistant (June, 2020 – Feb, 2022)\\n• Explored legacy machine vision techniques to detect concrete cracks.\\n• Trained, tuned and evaluated various models to classify concrete cracks.\\n• Experimented various methods to localize concrete cracks which includes image processing, object detection and sliding-\\nwindow technique paired with image classification.\\n• Trained, tuned and evaluated segmentation models using various networks to segment concrete cracks and quantify them.\\n• Performed TensorRT optimization on trained models to make compatible for edge computing and increase scalability.\\n• Deployed all models on a RaspberryPi that was installed on a wall climbing robot.\\n• Experimented SIFT, SURF, ORB and multiple other feature extraction algorithms to stitch images collected from the wall\\nclimbing robot to output a panorama.\\n• Performed camera calibration to measure crack dimensions.\\nEDUCATION\\nLa Trobe University Melbourne, Victoria\\nMaster of Artificial Intelligence 2023-2025\\nUniversiti Teknologi Malaysia Johor Bahru, Malaysia\\nBachelor of Engineering (Electrical-Mechatronics) with Honours 2018-2022\\nCGPA: 3.25\\nCambridge International Examinations – A Level Dhaka, Bangladesh\\nGrades: 3A’s 2017\\nCambridge International Examinations – O Level Dhaka, Bangladesh\\nGrades: 5A*’s, 3A’s 2015\\nCERTIFICATIONS & PROFESSIONAL DEVELOPMENT\\n• AWS Certified Solutions Architect – Associate\\n• IBM Certified Machine Learning Specialist – Associate\\n• Engineers Australia Skill Assessment – Mechatronics Engineer\\n• Python for Data Science – IBM\\n• NVIDIA Getting Started with AI on Jetson Nano – NVIDIA Academy\\n• Optimize Tensorflow Models for Deployment with TensorRT – Coursera\\nSKILLS\\nProgramming Languages: Python, C, C++, Bash, SQL, R\\nCloud and DevOps: AWS, GCP, OCI, Cloudflare, Docker, Docker Swarm, Terraform, Ansible, Linux, Git, Datadog, RSyslogs\\nAI/ML Frameworks & Tools: PyTorch, TensorFlow, OpenCV, NVIDIA DeepStream/TensorRT, Jetson Nano, GStreamer, Image\\nProcessing. Object Classification/Detection/Segmentation/Localization/Tracking. Video Classification,\\nHuggingFace Transformers, LangChain, LlamaIndex, BERT, Prompt Engineering\\nData Engineering: PostgreSQL, OracleDB, Redis, Kafka, MQTT, Hive, Airflow, MLFlow, Celery, GeoPandas, Folium\\nWeb & UI: Django, FastAPI, Flask, Auth0, Restful APIs, PyQt, Tkinter\\nDevelopment Practices: OOP, Functional & Asynchronous Programming, Parallel Processing, Multi-Threading, TDD, PyTest, ETL\\nDevelopment\\nREFEREES\\nAvailable upon request',\n",
       "  'c9cddf5fb828eab6': 'Brisbane, Australia, happy to relocate\\nSAHAR NAJ\\n(0) 481850371\\nnajafikhah1370@gmail.com\\nData Analyst\\nlinkedin.com/in/Sahar-Najafikhah/\\nProfessional Summary\\nExperienced, passionate and dedicated data analyst with a strong drive for continuous\\nlearning. Exceled in data analysis, data visualisation and problem-solving, with a keen\\ninterest in business analytics. I thrive in collaborative environments and enjoy working\\nwith diverse teams or autonomously.\\nEducation\\n• Master by Research, Accounting Information Systems, Queensland University\\nof Technology\\n• Awarded a prestigious scholarship to pursue a Higher Degree by Research\\n(HDR) at QUT. This scholarship recognised academic excellence and research\\npotential, providing support for conducting research in the field of AI in\\naccounting.\\n• Master of Science, Information Technology Management, University of Tehran\\n• Bachelor of Science, Business Management, University of Tehran\\nSkills\\n• Data Analysis: Statistical analysis, data mining, predictive analytics, trend\\nanalysis, SPSS Statistics & SPSS Modeler\\n• Technical skills: SQL, Tableau, Power BI, Dax, Excel, Python, QlikView and\\nPowerQuery, Salesforce development (CRM)\\n• Data Visualisation: Creating dashboards, reports, and data visualisations\\n• Business Intelligence: KPI tracking, performance analysis, strategic insights,\\nMicrosoft Project (MSP)\\n• Communication: Presenting findings, writing reports, stakeholder engagement\\n• Soft skills: Teamwork, adaptability, continuous learning\\nCertificates\\n• Getting Started with Microsoft Azure (LinkedIn learning, 2025)\\n• Power BI Data Methods (LinkedIn learning, 2025)\\n• Data Visualisation in Power BI (2024, DataCamp)\\n• Data Manipulation in SQL, Exploratory Data Analysis in SQL, Functions for\\nManipulating Data in PostgreSQL, Joining Data in SQL (DataCamp, 2024)\\n• Database Design (DataCamp, 2024)\\n• Intermediate Python (DataCamp, 2024) Work Experience\\nAssistant Tutor, QUT University, Feb 2025 - Present\\n• Responsible for tutoring one to two classes per week alongside a senior tutor,\\ndelivering content on statistical analysis, data analysis, and data visualisation\\nusing Python programming.\\nTutor, QUT University, Feb 2023 - Present\\n• Taught “Enterprise Systems”, providing students with guidance in Salesforce\\ndevelopment (CRM system), including Business Object Modeling, process\\nautomation, and software development.\\n• Instructed students in \"Business Intelligence,\" teaching Tableau for business\\ndata analysis.\\n• Instructed students in \"Management Information Systems,\" teaching Tableau\\nfor business data analysis and dashboard design.\\n• Taught \"System Analysis and Design,\" guiding students in requirements\\nelicitation and conceptual system design.\\nBusiness Process Analyst, National Research Institute of Science Policy (NRISP), May\\nJan 2015 – Dec 2021\\n• Engaged stakeholders via structured workshops and interviews.\\n• Designed and documented standardised workflows for ERP-like systems\\n(Exir).\\n• Developed over 25 BPMN-aligned process diagrams and requirement\\ndocuments.\\n• Delivered training to 20+ end-users on updated procedures and system\\nenhancements.\\n• Translated user stories into detailed functional specifications for IT delivery\\nteams.\\n• Coordinated UAT planning and test case execution, ensuring solution\\nalignment with business needs.\\n• Used SQL to extract and analyse data across departments.\\n• Collaborated with data analysts and IT teams to ensure data integrity\\nKey Achievements:\\n• Integrated BI tools into supply chain processes, reducing staff ordering time by\\n10%.\\n• Supported technology optimisation decisions, increasing efficiency by 20%. Portfolio Projects\\nSQL Projects\\n• Customer Personality Analysis – Conducted segmentation analysis using\\ncustomer demographics, purchases, and engagement data.\\nGitHub Repository\\n• Airbnb Listings Analysis – Built SQL queries for exploring and analysing Airbnb-\\nlisting data\\nGitHub Repository\\nTableau Dashboards\\n• COVID-19 Dashboard – Visualised global COVID-19 trends using filters, KPIs,\\nand interactive charts.\\nView Dashboard\\n• Tourism in Indonesia – Created a dashboard to showcase tourism visitor\\ntrends across Indonesian regions.\\nView Dashboard\\nProfessional Referees\\nAvailable upon request',\n",
       "  '90e485656ce9236d': 'Innovative and growth-driven Data Analyst with over 2 years of experience designing and delivering end-to-end data analytics and data science solutions across cloud platforms. Adept at building and monitoring secure, scalable data pipelines and transforming raw data into actionable insights that improve business performance. Proficient in SQL, Python, R, and data visualization tools like Power BI and Tableau, with hands-on experience using dbt, Databricks, BigQuery, and Snowflake. Strong foundational understanding of machine learning models and their integration within business intelligence platforms. Demonstrated ability to collaborate across cross-functional teams—including engineers, data scientists, and stakeholders—to support data-driven decision-making and ensure data integrity and governance. Excellent communicator, known for turning complex data into clear narratives that support strategic innovation and growth.\\nProjects\\nRetail Sales Analysis | SQL & Power BI                                                                         \\nAnalysed retail sales data using SQL, leveraging joins, subqueries, and aggregations to create a structured dataset.\\nDeveloped a Power BI dashboard featuring key KPIs such as revenue growth, regional sales performance, and inventory turnover.\\nUtilized DAX calculations and data modelling to enhance reporting accuracy and decision-making.\\nAirbnb Listing Analysis | Tableau                                                                    \\nDesigned an interactive Tableau dashboard to analyse Airbnb listings, highlighting trends in pricing, occupancy, and location impact.\\nImplemented geospatial mapping and drill-down features for enhanced user experience and insights extraction.\\nBank Customer Churn Analysis | Excel                                                                   \\nUtilized Excel (Pivot Tables, VLOOKUP, conditional formatting) to analyse customer data and identify churn trends.\\nCreated a dynamic dashboard to visualize churn factors and recommended data-driven retention strategies.\\n\\nProfessional Experience\\nNext Wave MSP, Australia\\nData Analyst | Jan 2024 – Present\\nDesigned and monitored secure, end-to-end data pipelines, transforming raw data into analytics-ready formats using tools like dbt, Databricks, and SQL-based transformations.\\nBuilt interactive dashboards in Power BI and Tableau to visualize performance metrics, sentiment analysis, and call outcomes—enabling real-time monitoring of customer experience KPIs.\\nCollaborated with data engineers and scientists to tune and integrate machine learning models into voice analytics workflows, enhancing system adaptability and business relevance.\\nConducted deep-dive analyses using SQL, Python, R, and cloud platforms (BigQuery, Snowflake, Azure) to extract meaningful trends and drive strategic, data-informed decisions.\\nApplied data governance and documentation practices to improve data quality, pipeline transparency, and team-wide accessibility to structured and unstructured datasets.\\nWorked with cross-functional stakeholders to understand business challenges and deliver tailored data science and analytics solutions that support innovation and long-term growth.\\nPartnered with technical and non-technical teams to ensure secure data migration and integration, identifying potential risks and issues and supporting pre-deployment testing phases.\\nContributed to NLP model optimization for transcription and sentiment tagging, aligning data science models with contact center monitoring goals.\\nSupported continuous improvement by evaluating emerging tools, optimizing workflows, and applying best practices across analytics and data science functions.\\nKnown for excellent communication skills and ability to collaborate in agile, cross-functional teams to meet tight deadlines and evolving client requirements.\\nUnited Fuel Pump, VIC, Australia \\nCustomer Service Representative | Aug 2023 – Jan 2024\\nHelped customers with fuel transactions, always making sure to provide friendly and fast service.\\nManaged the cash register, handling payments with cash, card, or loyalty points.\\nRestocked shelves and kept track of inventory to make sure products were available when customers needed them.\\nTook the time to listen to customer questions and resolve any concerns in a professional and helpful way.\\nFollowed safety guidelines for handling fuel and made sure the work environment stayed safe and secure.\\nAssisted with everyday station tasks, like opening, closing, and completing end-of-day reports.\\n\\nPantech, India \\nIntern | Jan 2023 – Jun 2023\\nContributed to gathering, cleaning, and preparing large datasets for analysis, ensuring the data was both accurate and reliable.\\nPerformed exploratory data analysis (EDA) to uncover trends and patterns.\\nDeveloped interactive dashboards and visualizations using Tableau and Power BI to present key insights to stakeholders.\\nWrote SQL queries to retrieve data from relational databases for reporting and analysis purposes.\\nAssisted in the development of predictive models by selecting features and testing various algorithms.\\nMonitored the integrity of data throughout the ETL process to ensure smooth and accurate data integration.\\n\\nTechnical Skills\\n\\nTechnical Skills\\nData Analysis & Analytics: Data mining, statistical analysis, predictive modeling, data cleaning, data modeling, KPI analysis, advanced analytics, business intelligence\\nProgramming & Scripting Languages: SQL (including advanced troubleshooting, ETL, nested queries), Python (pandas, NumPy, Seaborn, Matplotlib), R, C#, C++, VBA\\nData Visualization: Power BI, Tableau, Seaborn, Matplotlib\\nData Warehousing & Databases: Oracle e-Business Suite (Core HR and Payroll data structures), Databricks, Snowflake, MySQL, PostgreSQL, MS SQL Server, Azure Synapse\\nModeling & Orchestration Tools: DBT (Data Build Tool), Apache Airflow\\nCloud Platforms: Microsoft Azure, AWS\\nTools & Technologies: Advanced Excel (pivot tables, formulas), Power Query, DAX, Jupyter Notebooks, Visual Studio Code, Git, Jira, CI/CD pipelines for analytics workflows\\nSoft Skills: Strong analytical and problem-solving ability, entrepreneurial mindset, excellent communication and stakeholder collaboration, detail-oriented, collaborative, proactive learner\\n\\nEducation\\nMaster of Information Systems | Holmes Institute, Australia (June 2025)\\nData Management & Databases | Statistical Analysis & Predictive Modelling | Business Intelligence & Data Visualization | Data Governance & Privacy| \\n\\nBachelor of Electrical Engineering | Osmania University, India (November 2022)\\nAnalytical & problem-solving skills | Circuit design & electrical systems | Computer Science Fundamentals\\n\\nProfessional Attributes\\nAnalytical Thinking | Attention to Detail | Problem-Solving | Communication Skills | Critical Thinking | Adaptability | Time Management | Collaboration | Curiosity | Data-Driven Mindset | Active Listening | Accountability | communicate \\n\\nReferences\\nAvailable upon request.',\n",
       "  '77aacb560b1dd217': 'Data Scientist/Analyst (+61) 416 970 677 | ahsannawab477@gmail.com\\nhttps://www.linkedin.com/in/ahsan-nawab-khan/\\nAHSAN NAWAB KHAN\\nhttps://github.com/ank21\\nHawthorn East, Victoria, 3123\\nSummary\\nCurious, analytical, and impact-driven - I’m a Data Analyst skilled in SQL, Python, Power BI, and Excel, delivering insights through\\ndashboards, predictive models, and real-world datasets. My projects include analyzing global layoff trends, building ML classifiers for\\ncategorization, and creating a full-stack solution for healthcare analytics. I apply strong data cleaning, wrangling, and visualisation\\nskills to solve business problems and I’m actively advancing my machine learning capabilities through practical, project-driven\\ntraining. Excited and ready to bring data to life in a fast-paced analytics team.\\nTechnical Skills\\nProgramming & Data Analysis\\nPython (Pandas, NumPy, Matplotlib), R (dplyr, ggplot2), SQL (MySQL, SQL Server), Excel (Advanced formulas, Pivot Tables, VBA)\\nBusiness Intelligence & Visualization\\nPower BI (DAX, Power Query), Tableau, Excel Dashboards, Matplotlib, Seaborn\\nDatabases & Big Data\\nMicrosoft Azure (Data Lake, Synapse Analytics), Apache Hadoop, Hive, MongoDB\\nMachine Learning & Modeling\\nLogistic Regression, Linear Regression, K-Means Clustering, TF-IDF, Scikit-learn, KNIME\\nTools & Technologies\\nGit, REST APIs, Beautiful Soup, PowerShell, Microsoft Teams, SharePoint\\nProjects\\nExploratory Data Analysis of IT Asset Inventory – Personal Project | GitHub: ank21/it-asset-eda | 2025\\n- Simulated and analyzed a dataset of 2,000 IT assets to uncover insights on cost efficiency, department-wise spending, and e-waste\\ntrends.\\n- Performed comprehensive data cleaning and preprocessing (e.g., data validation, outlier handling, duplicate removals, column\\nderivation) and visualized key metrics using Matplotlib and Seaborn, including time-based asset utilization by department and staff.\\n- Delivered actionable insights and procurement recommendations that could cut annual hardware waste by 10–15%.\\nGlobal Layoffs Exploratory Data Analysis – Personal Project | GitHub: ank21/global-layoff-eda | 2025\\n- Analyzed 2,000+ global layoff records using SQL and Python to identify post-COVID trends by region, company, and industry.\\n- Cleaned raw data, handled missing values, and visualized trends using Matplotlib. Presented findings in a professional report with\\nactionable insights on workforce risk factors.\\nMENTORd Type II Diabetes Self-Management Solution – University Project (Confidential Repository) | 2023\\n- Developed a full-stack clinician portal web application utilizing the Vue.js JavaScript framework for my final year project.\\n- Integrated 1,000+ patient records from SQL into a structured database, enabling real-time CRUD operations and risk flagging.\\n- Successfully piloted the portal at a hospital for clinical trials, and received recognition for the Best Project Award\\nin the Computing Technologies category at the Swinburne Capstone Project Expo 2023.\\nRepository confidential; demo available upon request. Work Experience\\nData Analyst Intern - Builder’s Academy Australia\\nAugust 2022 – December 2022\\n- Conducted data analysis on multiple educational databases using Excel, including cleaning, transformation, and reporting tasks for\\nstudent and course data.\\n- Automated recurring tasks with advanced Excel formulas, improving report generation time by 30% and producing summary reports\\nthat informed curriculum decisions.\\n- Designed and delivered summary dashboards and reports that informed curriculum and resource planning decisions.\\n- Received outstanding feedback from supervisors for accuracy, initiative, and impact on operational efficiency.\\nEnrolment Officer – Swinburne University of Technology\\nJuly 2022 – March 2023\\n- Supported onboarding of ~50 new students per day by processing and validating student information across internal systems before\\nhandoff to admissions departments.\\n- Provided technical support via a ticket-based system, resolving login, access, and course enrollment issues across platforms such as\\nStudent-One and Canvas.\\n- Ensured high data integrity by cross-verifying student records and updating missing or inconsistent entries, reducing onboarding\\nerrors and delays.\\n- Liaised with IT and academic services to escalate unresolved issues and streamline enrollment workflows during peak intake periods.\\n- Recognized for professionalism, accuracy, and efficiency in a high-volume, fast-paced environment.\\nAdministrator - Australian Education Research Organisation\\nSeptember 2023 – December 2023\\n- Managed team inbox and tracked over 150+ educator communications in Outlook, contributing to data-driven reporting on\\nengagement activity.\\n- Maintained and organized a SharePoint resource library, ensuring up-to-date metadata and version control.\\n- Maintained and updated resources in SharePoint, ensuring accurate and secure documentation.\\n- Contributed to smooth workflow operations by keeping internal records organized, accessible, and up to date.\\nFacilities and Program Delivery Assistant – University of Melbourne\\nApril 2024 – Current\\n- Maintain and optimize Excel audit databases tracking 500+ organizational assets across multiple health science sites, ensuring up-to-\\ndate records and asset traceability.\\n- Collaborate with the IT Asset Management team, including a data analyst and asset manager, to improve the current asset lifecycle\\nsystem and reduce issues such as lost/stolen devices, e-waste, and unnecessary replacement expenses.\\n- Support the development of automated processes and reporting systems to monitor laptop fleet turnover, procurement efficiency,\\nand compliance.\\n- Streamline routine data-cleaning workflows, reducing manual entry time and minimizing reporting errors by introducing\\nstandardized input and formula logic.\\n- Provide day-to-day IT and technical support to academic researchers and PhD students, including troubleshooting analytics tools,\\nsystem access, and data sync issues.\\nEducation\\nBachelor of Computer Science (Data Science) – Swinburne University of Technology December 2023\\nCumulative GPA: 3.64/4.0',\n",
       "  'f476ce0793a219b1': \"Budhil Chakma\\nbudhil.chakma@mq.edu.au | 0472756306| Github Profile | L inkedin Profile\\nEDUCATION\\nMacquarie University Sydney\\nBachelor of Information Technology July 2022 - Dec 2025\\n● Majors: Software Engineering and Web & Mobile Application Development\\n● WAM: 85, Dean’s Merit List (2024)\\nEXPERIENCE\\nSoftware Engineer Intern Feb 2025 – Present\\nRapid Analysis Sydney\\n● Developed a full-stack web application for order tracking and support ticket workflows using React and\\nNode.js .\\n● Implemented authentication and role-based access using AWS Cognito user pools, PostConfirmation\\nLambda triggers , and JWT-based authorization in the backend.\\n● Stored structured order and ticket data in a PostgreSQL database , containerized and deployed via Docker\\non AWS EC2 .\\n● Enabled direct file uploads to Amazon S3 using pre-signed URLs, with secure request handling and role\\nvalidation handled by the Node.js backend.\\n● Set up CI/CD pipelines using GitHub Actions for automated deployments and integrated Grafana\\ndashboards behind NGINX for real-time monitoring.\\nData Analyst Intern Sep 2024 – Dec 2024\\nDHL Sydney\\n● Developed a scalable dashboard application to track the lifecycle of orders, enhancing operational\\ntransparency for over 80,000 orders monthly.\\n● Integrated data from 7 disparate systems using RESTful APIs , creating a dashboard to monitor and manage\\nconnections between system integrations.\\n● Implemented robust data validation processes utilizing SQL, Entity Framework Core ensuring 99.9% data\\naccuracy .\\n● Designed and deployed interactive data visualizations with python & Power BI , enabling over 150\\nstakeholders to make informed, data-driven decisions.\\n● Engineered an error detection module within the order management system using API integrations,\\nreducing error resolution time by 50%.\\nSessional Teaching Academic June 2024 – Present\\nMacquarie University Sydney\\n● Conduct weekly workshops for Fundamentals of Computer Science and Database Management Systems\\n(DBMS) , aimed at enhancing students' understanding and practical skills, tailored to the curriculum's needs.\\n● Communicate regularly with the Faculty of Science and Engineering to align workshop content with the\\nlatest academic and industry trends , ensuring the delivery of the most relevant and up-to-date material.\\nPROJECTS\\nHB-AI Movie Directory ( link ) July 2024 – Nov 2024\\n● Built an AI-driven recommendation engine using Open AI API and IMDB API to deliver personalized movie\\nsuggestions based on user preferences and moods.\\n● Developed a responsive React front end with advanced search features and User Profiles to save\\npreferences, favorites, and watchlists, allowing for a smooth, cross-device experience.\\n● Architected a scalable backend with Express.js, Node.js, and MongoDB , supporting secure user\\nauthentication and data persistence across sessions.\\nParking Spot Booking Service Flutter Application May 2024 – June 2024\\n● Developed a cross-platform mobile app using Flutter and Dart , implementing responsive UI with custom\\nwidgets.\\n● Integrated secure authentication using Firebase Authentication with OAuth 2.0 protocols and JWT tokens .\\n● Implemented local data storage using SQLite and the sqflite plugin for vehicle details and parking schedules.\\nSKILLS\\n● Programming Languages: Java, Python, SQL, JavaScript, Dart, C, DAX.\\n● Frameworks and Technologies: MERN Stack (MongoDB, Express.js, React.js, Node.js), Spring Boot, Flutter.\\n● Tools and Software: MySQL Workbench, Jupyter Notebook, Android Studio, Git, SQLite, Figma, Qualtrics.\\n● Project Management Methodologies: Agile, Scrum and SDLC (Software Development Life Cycle).\",\n",
       "  '6700f96c37c87933': \"HON MENG LEE\\nBUSINESS AND SYSTEMS DATA ANALYST\\nDETAILS PROFILE\\nADDRESS A Business and Systems Data Analyst with 3 years working experience in Data\\nUnit 331/299 Spring Street Analysis, ETL solution development, Database and IT system maintenance. In\\nMelbourne, 3000\\naddition to a masters of Data Science and a bachelors of Computer Science; strong\\nAustralia\\nexperience with customer communication, stakeholder engagement and project\\nPHONE management were also gained in prior study and employment roles. A keen learner\\n+61481 386 471 who is passionate about technology, highly motivated, reliable, detail oriented,\\nanalytical, and obsessed with deriving meaningful insights through the use of clean\\nEMAIL\\nand reliable data.\\nleehonmeng@yahoo.com\\nEMPLOYMENT HISTORY\\nLINKS\\nLinkedIn Business and Systems Data Analyst, PrimaPMI Noble Park North\\nMar 2022 — Feb 2025\\n• Designed, developed, documented, and managed automated ETL solutions\\nSKILLS\\nfor production, sales, and invoicing monitoring\\n• Communicating business needs between vendors and stakeholders for\\nDatabase (SQL, PSQL,\\nsoftware improvement and fixes\\nTSQL, MySQL)\\n• Data analysis, manipulation, and visualisation tasks for consumption and\\nData Cleaning, Modelling, waste investigation through peak season\\nAnalytics (R, Python) • Solving software/technology issues acting as SME of company’s Production\\nOrder Management Software (POMS)\\nMicrosoft Office 365 Suite\\n• Assisted with company merger by aligning and setting up the product\\nExtract, Transform, Load catalogues of both companies\\n(ETL)\\nNPI & Pre-Sales Coordinator, PrimaPMI Noble Park North\\nData Visualisation\\n(PowerBI, QlikView, Feb 2021 — Mar 2022\\nTableau)\\n• Provided technical support to customer on-boarding process\\n• Communication with customers, clients, and vendors\\nAgile Methodology (Scrum,\\nWaterfall, Kanban) • Performed data scraping, cleaning, and analysis to present company\\nperformance insight to stakeholders\\nMachine Learning • Assisted with migration to new company POMS\\n(Clustering, Classification,\\n• Worked with internal non-technical team and external IT for improvement and\\nNeural Networks)\\nfixes to new POMS Software\\nLANGUAGES\\nIT Technician, PrimaPMI Noble Park North\\nNov 2020 — Feb 2021\\nEnglish\\n• Maintenance, repair, and support of company's IT infrastructure\\nBahasa Malaysia • Documentation and inventory of company's IT infrastructure\\n• IT on-boarding and setup for newly hired employees of the company Team Leader & Lead Developer, Grey-Box Québec (Remote)\\nJun 2021 — Oct 2021\\n• Team Leader and main point of stakeholder contact\\n• Arranging and moderating all meetings\\n• Delegation of roles and tasks for each team member\\n• Validation of the Recurrent Neural Network (RNN) model's performance\\n• Optimizing and reporting on the performance of the RNN model\\n• Data mining and cleaning datasets for training and testing of the RNN model\\nEDUCATION\\nMaster of Data Science, RMIT University Melbourne\\nMar 2020 — Dec 2021\\nA Master degree in Data Science that provided knowledge and skills in all facets\\nof the Data Science process. Including analytical skills with multiple programming\\nlanguages - R, Python, and Scala to name a few; alongside tools such as Weka,\\nHadoop, and more. This course also provided knowledge and skills in data mining,\\nanalysis, preprocessing, modelling, visualization and communication. Capstone\\nproject was an internship with Grey-Box. Graduated with Distinction.\\nBachelor of Computer Science, Swinburne University Hawthorn\\nof Technology\\nMar 2017 — Nov 2019\\nA Bachelor of Computer Science with a Major in Software Development which\\nprovided the knowledge and skills in the field of Computer Science and\\nProgramming. This includes programming skills with languages such as Python,\\nJava, C#, Swift, SQL, PHP, and more; alongside Networking knowledge and practical\\nexperience with routers and switches. Additional Project Management workflow\\nknowledge and methodologies such as Agile Waterfall and Agile Scrum were\\nimparted and taught in a practical manner. Capstone project was on applying\\nNatural Language Processing onto Victorian court sentencing documents in order\\nto derive insights into factors that determine court sentencing outcomes.\\nSKILLS AND PROJECTS\\nAutomated Data Warehouse Dashboards SQL, PSQL, Excel, QLIK\\nCreated multiple automated ETL pipelines (and the Data Warehouse) for different\\nbusiness intelligence dashboards (company performance dashboard that includes\\ndaily and historical performance, invoicing and revenue dashboard) by extracting\\ndaily performance data from PostgreSQL Server over to a Microsoft SQL Server for\\ndata cleaning and pre-processing then automatically loaded onto QLIK daily.\\nThis allowed management access to monitor the company's daily and historical\\nperformance, providing insights to key stakeholders on what needs to be improved.\\nData Analysis and Visualisation for Waste and SQL, Excel, R\\nConsumption\\nCollected, analysed, and presented company's paper and ink wastage and\\nconsumption for peak season performance every year. This involved collecting\\ndata from various sources, cleaning, separating and combining them by different\\ncategories and performing analysis and visualisations mainly via Excel with a some\\nusage of R programming.\\nThis allowed the leadership team to identify points of waste and need of\\noptimization.\",\n",
       "  '493d0d19e0150412': 'Mazen Sharshar\\n+90 537 828 07 84 | mazensharshar87@gmail.com | https://www.linkedin.com/in/mazen-sharshar-2376b6247/\\nEDUCATION\\nIstanbul Aydin University Istanbul, Turkey\\nBachelor of Engineering in Computer Engineering. Aug. 2020 – May 2024\\nEXPERIENCE\\nDevelopment Engineer June 2023 –Jan. 2024\\nNTG Clarity Istanbul, Turkey\\n• Developed \"Dream Team\" application\\nIt is a web application that helps the stockholders and team managers to run the football club, it shows\\ndashboards about financial state, and players performance during the current season.\\n• Developed \"Maintenance Work Order\" application\\nIt is a web application that helps an organization to get a ticket about non-working devices that they have built\\nand send it to the IT department for repair.\\n• Collaboration in Agile System application\\nIt is a web application that helps the organization dashboard to stay connected with each team leader, and helps\\neach team leader to stay connected and monitor the progress.\\nInformation Technology Support Specialist Mar. 2023 – May 2023 | June 2022 – Sep. 2022\\nNofal Oğlu Istanbul, Turkey\\n• Tested and installed motherboards, processors, and graphics cards on desktops and laptops for corporate staff.\\n• Responded to support requests from end-users and patiently walked individuals through basic troubleshooting tasks\\n• Explained technical information in clear terms to non-technical individuals to promote better understanding\\n• Set up and maintained user accounts and client access\\n• Recovered critical information from data backups to restore functionality\\n• Completed routine and complex software installations, assisting users of various\\nInformation Technology Support Specialist Sep 2022 – Jan 2023\\nIstanbul Aydin University Istanbul, Turkey\\n• Assess and troubleshoot computer problems brought by students, faculty and staff.\\n• Tested software for data processing systems.\\n• Communicate with managers to set up campus computers used on campus.\\n• Monitored and logged server tasks.\\n• Maintain upkeep of computers, classroom equipment, and 200 printers across campus.\\nPROJECTS\\nAndroid Error Detection Model | Python, HTML/CSS, JavaScript Aug. 2023 – May 2024\\n• Developed a full-stack web application with using python as main programming language and backend.\\n• Collected some Android versions data, cleaned it, and organized it to be ready to train it.\\n• Visualized the accuracy of the model with 96% of recall and with 78% of precision.\\nSimple Fitness App | React, Node.js, API, CI/CD Aug 2023 – Jan 2024\\n• Developed a fitness tracking application to help users monitor workouts and progress.\\n• Built a RESTful API with Node.js and integrated it with a responsive React frontend.\\n• Implemented continuous integration/deployment (CI/CD) to automate testing and deployment.\\n• Collaborated with fitness enthusiasts to gather feedback and suggest new features.\\nTECHNICAL SKILLS\\nLanguages: Java, Python, C/C++, SQL (Postgres), JavaScript, HTML/CSS, PHP\\nFrameworks: React, Node.js, JUnit, WordPress, Excel, REST API, FASTAPI\\nDeveloper Tools: GitHub, Git, Hugging face, Google Cloud Platform, VS Code, PyCharm, IntelliJ, Eclipse\\nLibraries: Pandas, NumPy, Matplotlib, Scikit-learn, PyTorch, Seaborn, Plotly, TensorFlow\\nSpoken Languages: Arabic (Mother Tongue), English (Professional), Turkish (Professional)',\n",
       "  'a8ca1108dd06b907': 'SAIRAJ SURYAVANSHI\\nDATA ANALYST\\nMelbourne, Australia | +61 451968238 | sairajsuryavanshi@outlook.com\\nwww.linkedin.com/in/SairajSuryavanshi\\nCAREER SUMMARY:\\nIT experience having excellent knowledge in Data Analysis with demonstrated history in working\\nwith Data Extract-Transform- Load (ETL), Data Cleaning, Feature Engineering, Feature Extraction,\\nData Analytics, Data Visualization, and designing and developing data management systems.\\nExtensive experience working with SQL, Python and PowerBI. A Strong professional with a\\nMaster’s in data science from Macquarie University - focused on Statistical Analysis, data\\nModelling, Machine Learning, and delivering Strategic Business driven Solutions with almost 4\\nyears of experience working with stakeholders and consulting.\\nSKILLS:\\nTECHNICAL SKILLS\\n• Python / R Programming(R Studio) • SPSS\\n• Jupyter Notebook / Google Colab /GIT • NumPy, Pandas, Matplotlib, scikit-learn,\\nKeras, TensorFlow\\n• Power BI • Machine Learning\\n• Data Analysis / Statistical Analysis / • MySQL\\nModelling\\n• • Agile / SDLC Methodologies\\n• ETL • Microsoft Office (Excel, VBA,\\nPowerPoint, Word etc.)\\n• JIRA (Jira Board) • Success Factors\\nINTERPERSONAL SKILLS\\n• Problem Solving Skills • Communication Skills\\n• Teamwork Skills • Process Improvement\\n• Collaborative Approach • Stakeholder Management\\n• Ability to Learn WORK EXPERIENCE:\\nMedibank | Melbourne, Australia\\nData Solution Designer | October 2024 – April 2025\\n• Design& implement: Develop AWS Cloud based solutions(On Access tier and Optimized tier\\n) that align with Organisation’s architectural principles and meet business needs.\\n• Agile collaboration: Work closely with software engineers to maintain development best\\npractices and technical standards.\\n• Data Models: Design robust, scalable, and efficient data models to support the payment\\nintegrity processes. This could involve structuring both transactional and non-transactional\\ndata.\\n• Data Flow Design: Develop and document data flows across various systems to ensure Claims\\ndata is accurately collected, validated, and processed.\\n• Integrating Systems: Design data integration solutions that ensure seamless communication\\nbetween various internal and external systems related to payment processing, claims\\nvalidation.\\n• Data Quality Assessment: Ensure that the data used for Claims processing and analysis is\\nclean, accurate, and reliable. Perform regular data quality checks.\\n• Identifying Payment Integrity Issues: Identify and propose data-driven solutions to address\\nissues such as fraudulent claims, billing errors, or inconsistencies in payment processing.\\n• Business Rule Definition: Work with stakeholders to define business rules for payment\\nvalidation and fraud detection, ensuring that the data solution can effectively support those\\nrules.\\n• Working with Cross-Functional Teams: Collaborate with other teams such as data engineers,\\ndata analysts, business analysts, and product owners to ensure that the data solution aligns\\nwith the overall goals of the payment integrity project.\\n• Stakeholder Communication: Act as a liaison between technical teams and business units,\\nensuring that the data solution meets business requirements and addresses any payment\\nintegrity challenges.\\n• Requirements Gathering: Work closely with stakeholders to gather business and technical\\nrequirements for the data solution and ensure it aligns with the payment integrity objectives.\\n• Data Reporting: Design and implement reporting solutions that provide actionable insights\\ninto payment integrity, including fraud detection, errors, and trends.\\n• Analytics: Collaborate with data analysts to develop advanced analytics and dashboards that\\nsupport decision-making and payment integrity initiatives.\\n• Documenting Solutions: Maintain clear documentation for the data solution architecture,\\ndata models, workflows, and integration points.\\n• Providing Technical Support: Assist with providing technical support and troubleshooting for\\nthe data solutions. METCASH | Melbourne, Australia\\nData Analyst | September 2022 – June 2024\\n• Analysed, designed, developed Power BI reports and dashboards as per the business needs\\nto provide a better insight.\\n• Simplify and clarify complex information by identifying the main issues, drawing accurate\\nconclusions, and presenting logical arguments. Break down complex details and next steps\\ninto clear, understandable terms.\\n• Interacted with multiple stakeholders within multiple teams(Marketing, Retail pricing,\\npromotions, Sales etc) to understand the business requirements to system requirements on\\ntechnical & non-technical levels.\\n• Manage relationships with stakeholders throughout the consultation process to ensure that\\nthe project scope and reporting requirements align with their expectations and ensure all\\nchanges are properly documented.\\n• Performed action on the source data from a variety of platforms (CRM, SQL Server, system\\napplications) to combine, synthesize and analyse to generate insights of current state\\nprocesses and solutions using extensive SQL.\\n• Performed data ingestion, data cleaning, data transformation, exploratory data analysis,\\nfeature engineering, and model building for the project aimed to perform market basket\\nanalysis and customer segmentation on the customer data using unsupervised learning.\\n• Designed and built the Data Model, Transformation rules for the Unstructured Data available\\nin Various department systems, Excel and Text files were extracted, transformed, and\\ncleansed to create meaningful reports using Power BI.\\nITIC Live (Internship) | Sydney, Australia\\nJunior Data Analyst | February 2022 - July 2022\\n• Performed exploratory data analysis, extracted features, and delivered insights using\\nvisualization techniques using Python and Tableau.\\n• For a range of audiences, provide specialised reports, letters, suggestions, presentations, and\\nguidance on complicated subjects.\\n• Designing and testing data collection instruments including questionnaires and forms.\\n• Develop and maintain state of the art advanced statistical and machine learning models (SVM,\\nRandom forest, xGBoost).\\n• NLTK library for data cleaning, removal of stop words and punctuations, tokenization,\\nstemming and lemmatization.\\n• Built TF-IDF features using NLTK and performed sentiment analysis using Neural Networks.\\n• Model Analysis was done using Precision, Recall, Fl score, AUC and ROC. • Worked with Image Data for Image Recognition using machine learning and Deep learning\\ntechniques.\\n• Frequently worked with libraries like Pandas, Matplotlib, seaborn, Scikit-learn, NumPy, Keras\\nand TensorFlow. Specially to present the data in graphs, Charts, Tables etc.\\n• Worked with relational database management system\\n• End-to-end data science execution including scoping, building, testing, implementation,\\nmaintenance, tracking and optimization of predictive models.\\n• Develop and execute frameworks, techniques, and reporting procedures, particularly those\\nrelated to compliance and quality assurance. Execute research and data initiatives.\\nCanstar India Technologies I Navi Mumbai, India\\nData Analyst | February 2018 – January 2020\\n• Gathering sales data from various sources such as CRM systems, POS systems, online sales\\nplatforms, and offline channels. Ensuring data accuracy, completeness, and consistency,\\nwhile also dealing with challenges such as multilingual data and diverse data formats.\\n• Analysing sales data to identify market trends, regional variations, and customer preferences\\nspecific to the Indian market. Visualizing insights in a way that resonates with stakeholders,\\nconsidering cultural nuances and preferences.\\n• Monitoring sales metrics and KPIs specific to the Indian market, such as region-wise sales\\nperformance, product category trends, and seasonal variations influenced by Indian festivals\\nand events.\\n• Utilizing historical sales data and considering factors unique to India, such as monsoon\\nseasons, festive seasons like Diwali and Navratri, and regional holidays, to forecast sales\\naccurately. Understanding the impact of cultural and economic factors on consumer\\nbehaviour.\\n• Segmenting Indian customers based on demographics, cultural factors, and purchasing\\nbehaviour. Understanding the diversity within the Indian market and tailoring segmentation\\nstrategies accordingly.\\n• Analysing the sales process in the Indian context, considering factors such as the diversity of\\nlanguages and cultures, regional distribution channels, and regulatory compliance specific to\\nIndia. Providing recommendations for optimizing sales processes to improve efficiency and\\neffectiveness.\\n• Evaluating the effectiveness of sales and marketing campaigns in the Indian market,\\nconsidering cultural sensitivities, language preferences, and regional variations. Adapting\\ncampaigns to resonate with Indian consumers across different regions and demographics. • Providing data-driven insights to support decision-making by Indian sales managers and\\nexecutives. Understanding the unique challenges and opportunities in the Indian market and\\nproviding actionable recommendations based on data analysis.\\n• Ensuring compliance with Indian data protection laws and regulations, such as the Personal\\nData Protection Bill. Implementing data security measures and protocols to protect sensitive\\nsales data, considering India-specific cybersecurity risks.\\n• Staying updated with the latest trends, technologies, and market dynamics in the context.\\nContinuously improving analytical skills and exploring new tools and techniques to enhance\\nsales data analysis specifically for the Indian market.\\nEDUCATION:\\n2020-2022 | Macquarie University I Sydney, Australia\\nMaster of Data Science\\n2014-2019 | Mumbai University I Mumbai, India\\nB. Tech Computer Science',\n",
       "  'e5f8984dfa4cae3b': \"PRIYA SINGH\\npriyasingh.5159@gmail.com | 0478 123 503 | Melbourne, Victoria 3032\\nSummary\\nA seasoned Data Analyst Consultant at KPMG, possessing skillset in SQL, Python, and business intelligence reporting\\nthrough Celonis with a focus on data-driven decision making and cultivating client relationships and engagement. ETL\\nexpert over the past 3 years, I have gained practical experience in handling diverse datasets for different ERPs,\\nconducting thorough analysis, and presenting actionable insights to drive strategic decision-making while working with\\ncross functional teams. Certified in Azure Fundamentals and proficient in extracting, transforming, and analyzing large\\ndatasets, with a proven ability to deliver accurate and actionable insights within tight deadlines. Maintains high-quality\\ndocumentation and deliverables and excels at communicating technical concepts to non-technical stakeholders\\neffectively.\\nSkills\\n• Tools & Technologies - Azure Cloud, SSMS, Oracle Fusion • Data Analysis & Visualization: Celonis Dashboard, Excel\\nSandbox, Microsoft Office Suite, Advanced Excel, Power (Advanced), SQL, pySpark, Data Analysis & Wrangling,\\nShell Oracle ERPs Oracle Fusion ERP, SAP, Microsoft Dynamics\\n• Database Management: MySQL, MS SQL Server, Oracle\\nDatabase, Oracle & Oracle Fusion Data extraction\\nExperience\\nData Analyst Senior Consultant|KPMG - Melbourne|04/2022 - Current\\n• Data cleaning, wrangling and extraction for different ERPs such as Oracle, Oracle Fusion etc using SQL and\\npySpark.\\n• Providing insights, identifying gaps, analyzing business efficiencies and performances indicators such as KPIs using\\nCelonis dashboard.\\n• Creating, altering & testing stored procedures in SQL to process Oracle transactional data.\\n• Liaising with client for data extraction, tool upgrade and requirement gathering and ultimately providing\\nrecommendations to solve risk-related problems for clients by reducing data retrieval time by 25%.\\n• Conducted ad-hoc analyses on various datasets as requested by clients.\\n• Identifying the high-risk criteria journals as per client's requirement.\\n• Analyzing audit data for fraud and possible risks, testing data for consistency and completeness.\\nAssociate Software Engineer|Accenture Solutions - Mumbai, India|03/2018 - 02/2020\\n• Working on Annuity and Insurance project to create test cases, scenarios and performing sanity testing, smoke\\ntesting, integration testing and regression testing to ensure the quality of the system.\\n• Test Cases Preparation, Planning and Execution.\\n• Using REST and SOAP APIs for Web Service Testing.\\n• Testing and reporting bug using the tracking tool JIRA and Selenium.\\n• Querying with SQL the database to assess, clean and analyze large datasets.\\nIntern Data Analyst|EY|11/2017 - 02/2018\\n• Analyzed transactions and accounts for fraud and possible risks, testing data for consistency and completeness.\\n• Optimized data entry procedures, resulting in a 10% boost in team productivity.\\n• Contributed to a significant data migration initiative, guaranteeing precise data transfer at scale. Education and Training\\nDeakin University | Burwood, VIC | 02/2022\\nMaster of Information Technology: Data Science\\nU.P.T.U | Delhi, India | 07/2017\\nBachelor of Engineering: Information Technology\\nWebsites, Portfolios, Profiles\\n• https://www.linkedin.com/in/priyasingh5159\\nCertifications\\n• Microsoft Certified: Azure Fundamentals 900 certification, 2023\\n• LOMA Learn, LOMA, 2018\\nOther\\nConferences & Workshops: Attended 'Machine Learning Mastery' workshop (2019)\\nCourses: Advanced SQL for Data Scientists (Coursera, 2021), R for Data Science (Udemy, 2022)\\nVolunteering: OzHarvest Meal Preparation for unprivileged\\nReferences\\nReferences available upon request.\",\n",
       "  '0176a86af53206ae': 'Sahil Dwivedi\\nData Analyst\\nMelbourne, Vic | +61- 040 220 9556 | sahild091999@gmail.com | LinkedIn\\nSUMMARY\\nExperienced Data Analyst with over 3 years of hands-on experience in data analysis, predictive modeling, and business\\nintelligence. Skilled in using tools such as Python, SQL, Power BI, Tableau, and Excel to collect, clean, transform, and visualize\\nlarge datasets, enabling data-driven decision-making across business functions. Proficient in developing ETL pipelines,\\nforecasting models, and interactive dashboards to optimize inventory, identify trends, and improve operational efficiency. Adept\\nat statistical analysis, machine learning, and data storytelling to communicate insights to both technical and non-technical\\nstakeholders. Strong team collaborator with a proven ability to work cross-functionally in agile environments, solve complex\\nproblems, and deliver impactful analytical solutions. Passionate about leveraging data to drive strategic decisions and continuous\\nbusiness improvement.\\nSKILLS\\nData Analysis & Manipulation: Python, Pandas, NumPy, SQL, Excel\\nData Visualization: Tableau, Power BI, Matplotlib, Seaborn\\nMachine Learning & Predictive Analytics: Scikit-learn, TensorFlow, Keras, XGBoost, KNN, Classification, Regression, Clustering,\\nAnomaly Detection, Time Series Forecasting\\nETL Processes: ETL Pipelines, Data Transformation, AWS S3, Redshift, Data Cleansing\\nStatistical Analysis: SciPy, Statsmodels, R, SPSS, Mathematical Modeling\\nData Modeling & Forecasting: Data Aggregation, Predictive Models, Demand Forecasting, Data Modeling\\nCloud Technologies & Big Data: Snowflake, Google BigQuery, Dataflow, Azure Data Factory, Hadoop, EC2\\nVersion Control & Collaboration: Git, GitHub, Jira, Confluence, Slack\\nEXPERIENCE\\nJohnson & Johnson (J&J), Australia\\nData Analyst Jan 2025 – Present\\n\\uf0b7 Analyzed historical inventory and sales data using SQL to identify trends in device usage and demand across various regions,\\nensuring optimal stock levels and reducing the risk of stockouts.\\n\\uf0b7 Cleaned and merged large datasets with Python (Pandas, NumPy) to prepare data for analysis, handling missing values,\\nduplicates, and ensuring data consistency for accurate reporting and forecasting.\\n\\uf0b7 Developed predictive models using historical usage data to forecast future demand for medical devices, leveraging Python\\nto implement demand forecasting techniques and optimize inventory levels.\\n\\uf0b7 Visualized real-time inventory and sales data using Tableau, creating dynamic dashboards that displayed device usage\\ntrends, stock levels, and regional demand, helping stakeholders make data-driven decisions on inventory replenishment.\\n\\uf0b7 Produced comprehensive reports in Excel, including pivot tables and charts to analyze inventory turnover rates and\\nhighlight slow-moving devices, providing insights for better resource allocation and waste reduction.\\nAustralian Red Cross Lifeblood, Australia\\nData Scientist Intern May 2024 – Dec 2024\\n\\uf0b7 Processed and cleaned large datasets using Python (Pandas, NumPy) to ensure consistency in donor information, such as\\nblood type, donation frequency, and geographic location, optimizing the accuracy of the analysis.\\n\\uf0b7 Developed predictive models in Python, including K-Nearest Neighbors (KNN), to forecast blood type demand in different\\nregions, providing actionable insights for Lifeblood’s supply chain and donation planning.\\n\\uf0b7 Utilized SQL to query and extract donor data, generating key metrics such as monthly donations, donor engagement levels,\\nand donation type distributions to support decision-making.\\n\\uf0b7 Created interactive dashboards in Tableau to visualize donation trends, regional variations, and donor retention rates,\\nhelping stakeholders make informed decisions for targeted donation drives.\\n\\uf0b7 Generated reports in Excel, presenting analysis on donor retention and engagement, identifying trends, and supporting\\nmarketing strategies to boost donor participation in underperforming regions.\\nMouse & Cheese Design Studio, India\\nData Scientist Jan 2023 – Apr 2024\\n\\uf0b7 Constructed predictive models using Python, R, and XGBoost to identify demographic groups with low survey engagement,\\nenabling targeted digital outreach strategies across underrepresented urban populations.\\n\\uf0b7 Executed multivariate statistical analyses to evaluate behavioral and demographic trends affecting citizen participation,\\noffering actionable insights for campaign strategy refinement.\\n\\uf0b7 Monitored user engagement metrics via Google Analytics, interpreting session durations, bounce rates, and interaction\\nflows to enhance platform accessibility and user experience.\\n\\uf0b7 Integrated real-time data pipelines to track and segment website traffic, which supported the marketing and communication\\nteams in planning cost-efficient outreach and digital engagement campaigns.\\n\\uf0b7 Collaborated with UI/UX teams and government stakeholders to translate data insights into user-centric platform\\nimprovements, aligning technical findings with policy-level impact goals.\\nSixD Engineering Solutions\\nData Analyst Jul 2021 – Dec 2022\\n\\uf0b7 Designed automated data ingestion pipelines using Python to collect and preprocess sensor data from manufacturing\\nequipment, reducing dependency on manual intervention and ensuring timely data availability. \\uf0b7 Implemented complex SQL queries to extract actionable insights from structured data related to machine uptime,\\nproduction cycles, and operational throughput, supporting continuous monitoring and reporting.\\n\\uf0b7 Built interactive dashboards in Power BI to visualize plant KPIs including machine utilization, shift performance, and energy\\nusage, enhancing transparency across departments.\\n\\uf0b7 Performed root cause analysis on production delays and inefficiencies by analyzing historical equipment data, aiding\\noperations and finance teams in cost control and resource planning.\\n\\uf0b7 Coordinated with production engineers, quality teams, and financial analysts to ensure that data models and reports aligned\\nwith operational needs and business goals.\\nEDUCATION\\nMaster of Science in Data Science Oct 2024\\nRMIT University, Melbourne, Australia\\nBachelor of Technology in computer science June 2021\\nMaharaja Surajmal Institute of Technology',\n",
       "  'ccfed7e7060c9916': 'justinlai841@gmail.com | 0499 220 169 | Melbourne, Australia\\nJUSTIN LAI\\nEDUCATION\\nThe University of Melbourne | Bachelor of Commerce (Economics and Finance) Feb. 2017 – Jul. 2020\\nWeighted Average Mark: 75.48 - Second Class Honours Division A\\n•\\nEnrichment and Awards: (2022) Melbourne University Cricket, Tennis and Swimming Member, (2020) CAINZ Digest\\n•\\nEditor, (2019) Melbourne Global Scholars Award, (2019) Global Management Consulting [Seoul], (2019) FBE Mentee\\nMelbourne High School | Victorian Certificate of Education (Baccalaureate) Feb. 2013 – Dec. 2016\\nAustralian Tertiary Admission Rank: 95.85\\n•\\nSport: (2013-2016) Varsity Swimming Team [Gold and Multiple Silver Medallist at the SSV State Swimming\\n•\\nChampionships], Varsity Water Polo Team [Crawford Shield Exchange, North Sydney Boys High School]\\nWORK EXPERIENCE\\nHaileybury College | Tennis Coach for the Year 10B to C’s in the APS Competition Feb. 2025 – Mar. 2025\\nFaciliated training sessions and school competitions with the Year 10B to C’s at Haileybury College\\n•\\nInvolved teaching them a wide range of techniques such as forehands, backhands, slices, serves, dropshots, volleys and\\n•\\nlobs. Also taught them hand – eye coordination drills with the tennis balls and worked on running laps around the courts\\nTaught them the importance of watching tennis consistently and to maintain a healthy diet\\n•\\nHigh level regime (forehand cross, backhand cross, chip and charge and the philosophy of going hard on serves)\\n•\\nNational Australia Bank | Analyst – Client Management and Execution (Corporate and Industrials) Jun. 2021 – Mar. 2022\\nWorked with Senior Associates and Directors across two portfolios (35+ clients) to deliver a wide range of financial,\\n•\\ntransactional, credit, administrative, risk and portfolio management solutions. Additionally, conducting strategic research\\nand analysis on clients’ industries to develop sector expertise and knowledge\\nPerformed in-depth financial spreading, general analysis, risk ratings and wrote credit papers for 6 clients that alerted\\n•\\nDirectors on the clients’ financial profile, credit risk and the intrinsic details of their businesses\\nPriced manual fees (facilities and bank guarantees) for 10+ clients utilising various financial formulas\\n•\\nFostered stakeholder engagement and management to successfully collaborate with and influence senior colleagues\\n•\\nNational Australia Bank | Analyst – Business Execution (Data, Analytics and Strategy Execution) Feb. 2021 – Jun. 2021\\nSecondment: Supported and led components of the delivery of small to medium non-funded initiatives, focused on\\n•\\nprocess and business improvement that contributes towards NAB’s Personal Banking strategy, vision and goals\\nConstructed presentations, scrutinised existing instructions and policies and provided recommendations to embed a\\n•\\nrevolutionary employee development system (PEAK), that focused on learning, feedback and collaboration\\nLiaised with stakeholders to gather data and update frontline bankers’ performance metrics and key behaviours\\n•\\nDeveloped multiple strategies and Excel automation to improve the Banker Accreditation process\\n•\\nIdentified $1,000,000 in non – lending losses from various datasets due to offset account refunds\\n•\\nNational Australia Bank | Team Member – Pre - Credits (Customer Lending Solutions) Mar. 2017 – Jan. 2021\\nCompleted tasks (Property Valuations, Legal Vetting, Searches, Limit Enquiries and eCRS) lodged in by bankers to\\n•\\nensure due diligence requirements are successfully achieved in the client lending process\\nMonitored 20-40 tasks a day to ensure excellent service, engaged stakeholders, trained new staff, have consistently\\n•\\nmaintained over 85% productivity with <1% pushback from bankers and associates and led team meetings\\nFacilitated 48 property valuations simultaneously, ensured completion of a $30,000,000 commercial property valuation\\n•\\nand consistently volunteered and delegated to help on complex tasks which reduced team’s overall workload by 5%\\nEXTRACURRICULAR ACTIVITIES\\nCulture Bridge | Consultant Apr. 2019 – Jan. 2020\\nAnalysed and implemented solutions to cross-cultural integration issues at universities in Australia\\n•\\nAssisted in organising “The Bridge”, an event designed to generate a proposal and create discussion on the next step in\\n•\\nAustralia’s path towards achieving greater cross-cultural integration which included panellists from Boston Consulting\\nGroup (BCG) and MindTribes\\nConducted interviews with tutors and students and liaised with several faculties to establish working relationships\\n•\\nADDITIONAL INFORMATION\\nCase Competitions: (2018) Global Microfinance Case Competition, (2017) UBS Investment Banking Challenge\\n•\\nInterests: Swimming, Tennis, Cricket, Hip-hop Music, Trying Different Food, Fashion, Investing and Social Impact\\n•',\n",
       "  '8ed9c5e84fb32c65': 'Peter Wotherspoon\\npeter.a.wotherspoon@gmail.com ❖ 04 0789 6360\\nWORK EXPERIENCE\\nCalvary Health Care (contract via Brightforge Talent) Jan 2024 – Present\\nData Engineer Melbourne, VIC\\n• Managed and processed operational data pipelines for aged care facilities using Azure services, ensuring timely\\nand accurate data availability\\n• Developed and maintained ETL processes to transform raw data into formats suitable for analysis and\\nreporting, demonstrating strong data handling skills\\n• Analyzed data requirements and built dashboards for various stakeholders, translating technical information for\\ndifferent professional backgrounds\\n• Performed data validation checks and monitored data quality metrics using SQL and other tools, ensuring high\\nattention to detail and data accuracy\\n• Investigated and resolved ETL pipeline issues and data anomalies.\\n• Authored documentation for data processes, models, and quality checks, contributing to knowledge sharing and\\nprocess consistency\\n• Supported ad-hoc data requests from internal teams using SQL queries and assisted with data analysis using\\nExcel/Python\\n• Collaborated within a data team while also working independently to manage assigned pipelines and data quality\\ntasks\\nThe Salvation Army March 2023 – Dec 2023\\nData Analyst Intern Melbourne, VIC\\n• $3 million worth of rental assets contained inaccurate data. I built a Python program that autonomously cleans\\nthis data in ServiceNow, bringing accuracy up from ~60% to >99%, saving at least $20,000 in return cost\\ncomplications\\n• Accelerated a critical 15TB data migration project by ~2 weeks through analysis of 20M Azure server logs in\\nPython, then creating PowerBI dashboards to visualize user access patterns and streamline stakeholder\\ncommunications across several hundred employees\\n• Built multiple interactive PowerBI dashboards using DAX and data modelling, so that 2 IT project managers\\ncould present progress of their projects, to senior stakeholders in an easily understandable format\\n• Developed an automated emailing system in PowerApps for an IT asset recovery project, to automate\\ncommunications involving ~1,500 employees\\n• Performed inventory forecasting and financial projections for the eCommerce team, to help plan and prepare\\nfor IT assets across Australia being returned for liquidation/disposal\\nEDUCATION\\nDeakin University Completed September, 2023\\nBachelor of Computer Science, Minor in Cloud Technologies Melbourne, VIC\\nCapstone Project: An autonomous tool designed to work out the species of an animal, based on the sound that it\\nmakes. The tool was developed using audio collected from sensors placed in Victorias Otway National Park.\\nSKILLS & INTERESTS\\nSkills: Azure, AWS, dbt, Airflow, CI/CD, Snowflake, PostgreSQL, Git, Tableau, Power BI, Python, SQL, R\\nInterests: Writing D&D Campaigns, Reading, Running, Boardgames, Dog-training, Sitcoms',\n",
       "  'dd417e3f63181d20': \"Tharindu Senanayake\\nMelbourne, Victoria, Australia\\n+61-433-786-860 tharindugbs@gmail.com https://linkedin.com/in/tharindu-gihan-senanayake/\\nSUMMARY\\nA skilled Business Analyst with experience in coordinating projects and managing stakeholder relationships to enhance product\\ndelivery and customer satisfaction. Previously led product implementation and change management initiatives at Zilingo, contributing\\nto strategic objectives. Equipped with a Master's in Data Science and certifications in Power BI, ready to leverage analytical skills to\\ndrive business insights and optimize operational efficiency in the target role.\\nEXPERIENCE\\nAgribio Australia Jul 2023 - Dec 2023\\nIntern Data Analyst\\n• Conducted in-depth data analysis using R, transforming about 5,000 raw data into streamlined database structures.\\n• Ran extensive data wrangling techniques to prepare datasets for rigorous analysis, ensuring data accuracy and readiness for deci-\\nsion-making\\n• Managed the data cleaning and uploaded it into the database to support consistency and reliability.\\n• Documented the data cleaning, wrangling, and uploading steps, creating a comprehensive guide that improved team efficiency and\\nknowledge sharing\\n• Developed and executed SQL queries to extract relevant information for analysis.\\nZilingo Nov 2021 - Feb 2022\\nBusiness Analyst\\n• Coordinated projects with teams at backend, sales, and customers for product delivery consistently.\\n• Led product implementation and stakeholder education, facilitating change management to improve adoption rates\\n• Managed requirement gathering and finalization of product frameworks, strengthening customer satisfaction.\\nMAS Holdings Sep 2021 - Nov 2021\\nProductions Team Lead\\n• Guided production team to achieve on-time delivery targets and overcome operational obstacles, resulting in enhanced efficiency from\\n70% to 80%.\\n• Implemented strategies to optimize workflow and resource utilization, improving production output and reducing re-ordering costs by\\n5%\\n• Ensured compliance with quality standards and regulatory requirements, maintaining product integrity and customer satisfaction by\\nkeeping quality failures under 2%\\n• Spearheaded continuous improvement initiatives, identifying areas for process optimization and implementing solutions to streamline\\noperations and reduce waste.\\n• Provided leadership and mentorship to production staff, fostering a culture of accountability, teamwork, and continuous learning within\\nthe production team.\\nMAS Holdings Mar 2020 - Sep 2021\\nOperations Team Lead\\n• Predicted daily supply requirement, and increased fabric laying by 80% from 5,000m to 9,000m.\\n• Coordinated between the Production Control Unit, Raw Material Warehouse and Lab, and Inspection departments to make sure the\\ncutting plan was executed without any flaws that reduced downtime by 15%.\\n• Analyzed the weekly production plan and prepared the weekly cutting fabric laying plan, ensuring alignment with production targets\\n• Liaised between the planning and cutting departments to secure a smooth flow of cutwork to the production department, enhancing\\nworkflow efficiency\\n• Prepared monthly preparation reports and presentations for the cutting department, facilitating informed decision-making and strategic\\nplanning\\n• Led the fabric laying team to consistently achieve daily targets by utilizing data analysis and effective communication strategies,\\nresulting in improved operational efficiency\\nEmjay - Penguin May 2019 - Mar 2020\\nIncharge - Finishing Department\\n• Led the department to achieve on-time delivery improvement from 95% to 98% by implementing effective data collection and analysis\\nstrategies\\nEmjay - Penguin Aug 2018 - May 2019\\nBusiness Process Management Trainee\\n• Implemented and coordinated Lean concepts in the factory using the QCO (Quick Change Over) method, improving production\\nefficiency by mapping the value stream\\n• Handled over 12 BRT (Barrier Removal Team) projects, successfully resolving key operational issues and enhancing factory produc-\\ntivity\\n• Implemented 5S practices in the company and coordinated weekly 5S meetings, leading to improved workplace organization and\\nefficiency • Held weekly Kaizen meetings and coordinated factory Kaizens, fostering continuous improvement and innovation within the group\\n• Facilitated weekly CFT (Cross Functional Team) meetings with plant management, enhancing cross-departmental collaboration and\\ndecision-making\\nPrintcare Group Apr 2017 - Aug 2018\\nManagement Trainee\\n• Provided competitive pricing to customers and maximized profits by evaluating options for reducing waste, leading to improved cost\\nefficiency\\n• Acquired knowledge about the SAP system (production and operations t-codes), enhancing operational efficiency\\n• Supported customer service activities by preparing quotations and pro-forma invoices, ensuring timely delivery by coordinating with\\nPre-press, Planning, and Production. Contributed to new product developments through scenario planning and cost-benefit analysis,\\nmeeting strict deadlines\\n• Presented management information such as Profitability Analysis and forecasting/Trend analysis, aiding in strategic decision-making\\n• Prepared financial reports, including Debtors analysis and monthly SVAT reports, supporting financial transparency and planning\\nEDUCATION\\nLa Trobe University 2022 - 2023\\nMaster's degree, Data Science\\nUniversity of Peradeniya 2013 - 2015\\nBachelor of Science (B.Sc.), Statistics, Mathematics and Physics\\nSKILLS\\n• Data Analysis & Reporting, IT Business Analysis, , Data Preparation, Statistical Analysis, Data Manipulation,\\nSelf-management & Adaptability, Problem Solving & Troubleshooting,\",\n",
       "  'fae54469e0d024e8': 'Jaewon Yun\\njaewonyun.contact@gmail.com | +61 424 420 725 | Kensington, Melbourne, VIC, 3031 | LinkedIn\\nEDUCATION\\nMonash University Clayton, VIC\\nBachelor of Computer Science Graduated in December 2024\\nMajor in Data Science | Minor in Economics\\n● Extracurricular Activities: Peer Mentor for undergraduate students in the IT Faculty, Committee Member of\\nMonash Data Science Society (MDSS - Monash Data and AI), and Member of Monash Association of Coding.\\n● Relevant Coursework: Predictive Modeling (Python, R), Database Systems (SQL), Data Visualisation (Tableau).\\nEXPERIENCE\\nMonash University - Innovation Guarantee Program\\nResearch Analyst - Academic Initiative with the United Nations (UNEP) January - March 2023\\n● Conducted exploratory data analysis (EDA) using R (tidyverse, corrplot, skimr) on government recycling facility\\nperformance metrics to identify bottlenecks in End-of-Life Vehicle (ELV) recycling processes across 14 Pacific\\nIsland states, resulting in three key policy recommendations on compliance, efficiency, and emissions reduction.\\n● Pre-processed datasets by merging heterogeneous datasets, handling missing values, and creating new features\\nusing Python (pandas), ensuring data quality and enabling statistical analysis to uncover recycling inefficiencies.\\n● Designed interactive Power BI dashboards to visualise key insights, supporting data-driven policymaking.\\n● Collaborated with an interdisciplinary team and UNEP representatives to develop an ELV recycling strategy,\\ndemonstrating strong verbal and written communication skills to present complex findings clearly.\\nTutor Doctor\\nAcademic Tutor - Mathematics and English (VCE) April 2024 - Present\\n● Provided tutoring to VCE students, effectively communicating complex ideas to enhance understanding.\\n● Demonstrated strong professionalism, time management, critical thinking, and organisational skills to maximise\\nstudent outcomes, improving student academic performance by an average of 16% through targeted strategies.\\n● Developed data-driven learning plans, tracking student progress through Excel dashboards.\\nPROJECTS\\nPricing Optimisation and Analysis for Retail Products (Capstone Project - Monash University)\\n● Led end-to-end data analysis on retail pricing records, using Python (Pandas, NumPy, statsmodels) and SQL\\n(PostgreSQL) for data pre-processing, transformation, exploratory analysis, and feature engineering.\\n● Developed an ARIMA-based time series forecasting model to predict sales trends and conducted price elasticity\\nanalysis using log-log regression, identifying pricing strategies projecting a 5-10% revenue increase.\\n● Queried large datasets with PostgreSQL, utilising inner and left joins, window functions, and indexing to optimise\\nquery performance by 30% and uncover insights in peak purchasing times and trends across seasons.\\n● Built interactive Tableau dashboards featuring trend lines and segment filters to visualise pricing impact across\\ncustomer segments, used in quarterly pricing reviews to guide data-driven adjustments.\\nGlobal Renewable Energy Consumption Data Visualisation\\n● Developed an interactive web-based platform using HTML, CSS, R, and Tableau, enhancing user engagement\\nand data visualisation, and integrated JavaScript to enable Tableau dashboard interactivity and data exploration.\\n● Analysed International Energy Agency (IEA) data using R, identifying key insights such as the rapid adoption of\\nsolar energy in Asia, shifting regional consumption trends over time, and emerging market opportunities.\\n● Built dynamic Tableau dashboards featuring geospatial mapping tools, time-series analysis, and interactive\\nfiltering, enabling in-depth exploration of renewable energy consumption by region and energy type.\\nTECHNICAL SKILLS & QUALIFICATIONS\\nTechnical Skills: Python, SQL, Excel, R, Tableau, Power BI, & Microsoft Suite (Word, PowerPoint, Outlook)\\nQualifications: Working with Children Check (WWCC) and Professional Google Data Analytics Certificate (via Coursera)',\n",
       "  '6d743d56d9692fd0': 'VENKATESH KADIYALA                                                                             Email: Kadiyala9963@gmail.com\\n Data Analyst                                                                                              Phone: +61-469085033\\n\\nPROFILE SUMMARY\\nA result oriented and skillful professional having excellent knowledge of IT sector possesses 2.5 years’ experience.\\nA logical, analytical thinker with excellent managerial skills possesses 1 years’ experience as a MSSQL Developer.\\nExtensive experience in writing stored procedures, functions, views, and Tables.\\nExtensive experience in writing and tuning complex sub queries, SQL, stored procedures, functions, and triggers.\\nExperience in requirement analysis, coding and testing various modules in a software development life cycle.\\nExpertise preparing report specifications and database designs to support reporting requirements.\\nHaving 1.5 + years of experience in Power BI development.\\nExperience gathering and translating end user requirements into effective and efficient dashboards.\\nExperience including analysis, design, development of reports and dashboards for analytics.\\nWorked on DAX expressions like filters, Aggregate, Mathematical Functions etc.\\nInvolving in data preparation, based on user requirements manipulating the data set in power pivot.\\nImported data from multiple data sources into BI Tools, created relationships between data, created calculated                  columns and measures using DAX Query.\\nDevelop Power BI reports, Custom Visuals, Groups Creation, and effective dashboards after gathering and translating end user requirements.\\nExperience in creating different visualizations like Line, Bar, Histograms, Scatter Plot, Waterfall Chart, Bullet Graph, Heat maps, tree maps etc.                                                                                                                      \\n Experience in publishing reports to Power BI services and setting up the necessary connection details and scheduling.\\nStrong Communication & Management skills and Excellent Team player.\\n\\nTECHNICAL SKILLS\\n\\nDatabase:\\xa0            SQL Server 2008, 2012 MS Access and Basic of SSIS\\nReporting Tools:\\xa0    Power BI, Business Objects, SAP Crystal Reports and SAP WEBI Reports\\nLanguages:\\xa0           T-SQL, C, PostgreSQL, Basics of python\\nOperating Systems: Windows95/98/2000/NT, MS DOS, UNIX\\n\\nPERSONAL QUALITIES\\n\\nStrong logical and managerial skills.\\nAbility to produce the best result in pressure situations.\\nUnmatchable communication skills in writing and verbal both.\\nAbility to work individually as well as in the Group.\\n\\nEDUCATION QUALIFICATION\\n\\nB – TECH In Electronics and Communication Engineering in JNTUK 2015\\nMasters in Master of Business Information Systems at Torrens University Dec 2022\\n\\nKEY RESPONSIBILITIES HANDLED\\n\\nCreated Tables, Store Procedures, Views, Constrains\\nDeveloped complex reports: multi-functioned, leveled and grouped, summary functionality in main reports providing detailed drill down through On Demand Sub-reports, dynamic link labeling, complex functions, crosstabs, charts, shared variables, imported parameters and record sets from stored procedures, conditionally suppressed sections, error handling/messaging, clean and attractive designs, user friendly with instructional ToolTips.\\nWorked with team on customizing the abilities of Crystal Enterprise reports and WEBI Reports such as exporting, previewing, printing, and emailing reports.\\nLoad the data from MS Excel to tables and tables to MS Excel using SSIS package.\\n\\n\\nWORK EXPERIENCE\\n\\nWorking as Database Developer in FRAGOMEN IMMIGRATION SERVICES, KOCHI from Jan 2017 - Jan 2018\\nFinance Assistance & support in Probe Pty Ltd, Feb 2023 – June 2023\\nWorking as Power BI Developer in Infosys, Melbourne from July 2023 – July 2024\\n\\nPROJECTS PROFILE\\n\\nProject #01\\n                       \\n                Client\\t\\t:   Fragomen\\n  \\tRole\\t\\t:   Associate Consultant\\n          Technology\\t:   SQL server, SAP WEBI\\n\\nResponsibilities:\\nInvolved in Data migration project. Created Tables, stored procedures, views, and supported client facing issues.\\nInvolved in analysis and conversion of manage report (old system) to connect report as a part of Data Migration\\nInvolved in conversion and merging of different Crystal reports to a single WEBI reports.\\n\\nProject #02\\n\\n                Client\\t\\t:   NAB Melbourne\\n  \\tRole\\t\\t:   Data Engineer\\n           Technology\\t:   Power BI, SQL server\\n            \\n            Description:\\nMorrison’s is the fourth largest chain of supermarkets in the United Kingdom. The products are sending to different locations, where each location is divided into different zones. The project includes zone wise reports of sales, employee, and product details. The margins and annual growth are to be compared over a period.\\n\\t\\nResponsibilities:\\nImplementing Power BI reports Effective dashboards after gathering and translating end user requirement.\\nConnect to different Data sources from Power BI.\\nExperience in creating calculated measures and columns with DAX in MS POWER BI DESKTOP.\\nExperience in creating dashboards, presentations, and graphs.\\nExperience in Custom visuals and Groups creation. Good Knowledge on advanced calculations using power BI desktop.\\nExperience in creating different visualizations like line, bar, histograms, scatter, water, Bullet, Heat maps, tree maps etc.\\nKnowledge in creating Groups and content packs in the power BI services.\\nCertifications\\n\\nCredential Renderer – Associate Data Engineer\\n1321_3_449684_1686747294_Databricks - Generic.pdf – Fundamentals of Databricks Lakehouse\\n\\nDECLARATION\\n\\n I hereby declare that the information furnished above is true to the best of my Knowledge and belief.',\n",
       "  'f2ca85739fda42e4': 'RAFSAN AL MAMUN\\n+61 435087238 | Melbourne, VIC, Australia | 485 Temporary Graduate Visa Holder\\nrafsan7238@gmail.com | linkedin.com/in/rafsan7238/ | ralmamun.me | github.com/Rafsan7238 |\\npublic.tableau.com/app/profile/rafsan.al.mamun/vizzes\\nA proactive Data Analyst with expertise in data-driven decision-making, business intelligence, and client-focused analytics.\\nProven ability to build high-impact tools and models, publish open-source projects and research, and foster cross-\\ndisciplinary collaboration. Resilient and detail-oriented, with strong leadership skills, committed to solving real-world\\nchallenges through innovation, technical excellence, and continuous growth.\\nPROFESSIONAL EXPERIENCE\\nINDEPENDENT DATA ANALYST Melbourne, Australia\\nSelf-Employed Apr 2021 - Present\\n● Delivered tailored data solutions to diverse clients and through self-initiated projects (hosted on GitHub), translating\\ncomplex analytics into actionable business recommendations\\n● Executed A/B testing on marketing strategies for an e-commerce platform, analysing revenue, CTR, and\\nconversions; delivered data-backed recommendations that improved campaign ROI\\n● Performed sales and customer segmentation analysis for a digital music store; identified high-value genres and user\\ngroups, enabling targeted business strategies and operational improvements to enhance revenue by 12%\\nRESEARCH DATA INTERN Melbourne, Australia\\nMelbourne Data Analytics Platform (MDAP) Jul 2024 - Oct 2024\\n● Developed and published SeqBank, a command-line tool for managing large-scale DNA sequences, improving data\\nretrieval and workflow efficiency by 35%; published on PyPI and achieved 100% test coverage\\n● Optimised genomic models through hyperparameter tuning, pushing accuracy over 90%\\n● Authored comprehensive technical documentation, facilitating cross-disciplinary collaboration and tool adoption\\n● Maintained high standards of data integrity and confidentiality, especially when handling sensitive health and\\ngenomic datasets\\nSTUDENT TUTOR (ST) Dhaka, Bangladesh\\nBRAC University Jun 2021 - May 2022\\n● Taught foundational programming and computer science to 100+ students, improving academic performance by 10%\\n● Organised coding contests to foster learning, boosting student participation on competitive platforms by 25%\\nPROJECT LEAD AND MANAGER Dhaka, Bangladesh\\nCare2U May 2020 - Jan 2021\\n● Led a multidisciplinary team to design and develop Care2U, a PPE donation platform for healthcare workers during\\nCOVID-19, and managed overall operations\\n● Self-taught basic web development skills and applied Agile practices, reducing project development time by 40%\\n● Facilitated over 500 donations within six months, collaborating with the local government to expand impact\\nEDUCATION\\nMASTER OF DATA SCIENCE Melbourne, Australia\\nThe University of Melbourne Feb 2023 - Dec 2024\\nFirst Class Honours (H1), Collaboration with Bureau of Meteorology (BoM), WAM: 86.94/100\\nBACHELOR OF SCIENCE IN COMPUTER SCIENCE AND ENGINEERING Dhaka, Bangladesh\\nBRAC University May 2018 - Jun 2022\\nHighest Distinction, VC’s List Award, Merit Scholarship (CGPA 3.99/4.0) CERTIFICATIONS\\nTechnology Virtual Experience Program (Deloitte), Tableau 2022 Advanced (Ligency Team), The Complete SQL Course\\n2022 (Udemy), Software Product Management Specialization (University of Alberta), Google Analytics (Simplilearn)\\nKEY SKILLS\\n● Technical Skills: Python, R, SQL, MS Excel, Tableau, Git/GitHub, scikit-learn, TensorFlow, pandas, seaborn, Jira\\n● Data Analytics: Data Cleaning and Mining, Statistical Analysis, Data Visualisation, Database Management, A/B\\nTesting, Google Analytics, Looker Studio, ETL / Workflow Automation\\n● Machine Learning: Applied ML, Model Development, Hyperparameter Tuning, Predictive Modelling\\n● Others: Agile Project Management (Scrum), Leadership and Interdisciplinary Collaboration, Problem Solving,\\nCommunication, Multitasking, Deadline-Driven\\nKEY PROJECTS\\n● Sea Level Rise Visualisation and Path Planning for Pacific Nations (BoM Collaboration)\\nDesigned an impedance-based pathfinding system integrating the A* search algorithm with environmental data to\\noptimise evacuation routes in flood-prone regions, utilising LiDAR for terrain analysis. Praised by the Bureau of\\nMeteorology (BoM) for its novel and realistic approach to disaster preparedness and infrastructure planning.\\n● Automatic Fact Checking System for Climate Science Claims\\nDeveloped a machine learning system using Transformers to verify climate-related claims by retrieving relevant\\nevidence and classifying claim veracity. Applied advanced evaluation methods to optimise model performance,\\nleading to a 20% improvement over baseline accuracy. Focused on ensuring data quality, model interpretability,\\nand actionable insights throughout the process.\\n● Air Quality and Lung Disorder Analysis in Australia\\nBuilt a cloud-based platform and used regression models to assess impact of air pollutants on public health\\nmetrics in Victoria, Australia. Utilised scalable infrastructure to process and analyse large datasets. Conducted\\ninteractive statistical analyses using Jupyter Notebooks, delivering insights to inform public health and policy.\\n● Alcohol-Related Road Crashes in Victoria Between 2015-2019\\nDesigned and developed an interactive Tableau dashboard to explore patterns in alcohol-related road accidents.\\nVisualised crash sites, demographics of at-risk groups, and contributing factors to support data-driven policy\\ndecisions and improve road safety strategies.\\nAWARDS & ACHIEVEMENTS\\n● Best Presentation Award: Recognised at the International Conference on Satellite and Space Communications\\n2020 Paris among 50+ submissions for innovative research presentation on error detection onboard nanosatellites\\n● IEEE Publication 2022: Developed a custom CNN for breast cancer prediction using raw mammograms with\\n94% test accuracy, surpassing performance of pre-trained models\\n● 2nd Runner-Up: National award for Care2U at COVID-19 Combatants Unification Competition 2020 Bangladesh\\n● Top 10: HULT Prize @ BRACU (2019) for an augmented reality app supporting rural schools with no science labs\\n● Class Valedictorian: Scholastica 2017 for best student of the graduating batch\\nVOLUNTEERING & EXTRACURRICULAR ACTIVITIES\\n● Operations Intern (Apollo Hospitals Dhaka): Ensured healthcare quality compliance with JCI standards\\n● Robotics Club Member (BracU): Co-launched Traction 2020, a major inter-university hackathon in Bangladesh\\n● Volunteer (JAAGO Foundation): Organised charity events to help with education of underprivileged children\\nREFERENCES\\nAvailable upon request',\n",
       "  'c69f2087775fc760': 'Yanlin (Nikki) Chen\\nTel: 0490850973 Email: chen_0616@outlook.com LinkedIn: www.linkedin.com/in/nikki-chen-a589ba20b\\nEDUCATION\\nMaster of Business Information Systems 02/2023 – 12/2024\\nMonash University, Melbourne\\n\\uf09f Published first-author research in spatial data analysis (Nov 2024).\\n\\uf09f Awarded International Study Grants (2023); achieved the highest grade in Advanced Database Technology.\\n\\uf09f Leadership & Activities: PASS Leader, Project Translator (English-Mandarin), Monash Open Day\\nAmbassador.\\nPROFESSIONAL EXPERIENCE\\nMonash University — Melbourne, Australia\\nOnline Unit Coordinator 02/2025 – Present\\n• Responsible for the end-to-end coordination of ITO4132, reporting directly to the Chief Examiner and\\nMonash Online management team.\\n• Oversee delivery for 100+ students per semester, managing learning schedules, assessments, grading\\nschemes, and LMS infrastructure.\\n• Lead weekly live sessions to deepen student understanding of relational database concepts (SQL, ER\\nmodeling, Transaction Management) and real-world data handling.\\n• Resolve 100% of escalated academic and administrative student issues, maintaining a smooth and high-\\nsatisfaction learning experience.\\n• Ensure curriculum and assessment alignment with Monash Online’s academic standards and industry best\\npractices, contributing to consistently strong student performance.\\nTeaching Associate 01/2024 – Present\\n• Lead tutorials for FIT9132, FIT5137, FIT3003, and PASS units at Monash University, covering relational\\ndatabases, advanced SQL, dimensional modeling, and business intelligence tools.\\n• Assess technical assignments and final projects, providing constructive feedback to students that improve\\nanalytical thinking and practical database skills.\\n• Achieve a 91% student satisfaction rating in the 2025 SETU, reflecting high-quality instruction and\\ntechnical mentorship.\\nTECHNICAL SKILLS\\n\\uf09f Programming & Query Languages: SQL (PostgreSQL, MySQL, Oracle SQL), Python (Pandas, Matplotlib,\\nNumPy)\\n\\uf09f Data Engineering: DBT (Data Build Tool), ETL Pipelines, Data Warehousing, Star/Snowflake Schema,\\nOLAP\\n\\uf09f Visualization & BI Tools: Tableau, Power BI, QGIS\\n\\uf09f Database Management: PostgreSQL, Oracle SQL Developer, MySQL, MongoDB\\n\\uf09f Big Data & Cloud: Docker, GitLab\\n\\uf09f Others: MS Office suites, Google suites, Miro\\nRESEARCH PROJECT\\nFirst Author 03/2024 – 11/2024\\nSpatial analysis of train catchments in outer Melbourne\\n\\uf09f Designed and engineered a data processing framework, integrating more than seven public datasets with\\nPostgreSQL/PostGIS for large-scale spatial analysis.\\n\\uf09f Developed spatial SQL queries to analyze transit accessibility, revealing that 20.81% of the outer\\nMelbourne population lacks access to transit catchments and informing infrastructure planning.\\n\\uf09f Created interactive maps and dashboards in QGIS and Tableau, transforming raw data into actionable\\ninsights for urban planners.\\n\\uf09f Led research and manuscript writing, securing publication in Computers (MDPI) Journal:\\nhttps://doi.org/10.3390/computers13110299\\nCERTIFICATIONS\\n\\uf09f Professional Scrum Master II, Scrum org.\\n\\uf09f Oracle Certified Foundations Associate Database, Oracle\\n\\uf09f Data Engineer Associate, Datacamp\\n\\uf09f Standard Mental Health First Aider, Mental Health First Aid Australia',\n",
       "  '934d1fdb8a6d1063': 'Declan Barrett BSc\\nMelbourne, Victoria | LinkedIn | 040 335 0334 | declanbarrett123@gmail.com\\nSKILLS\\n● Proficient - R (dplyr, base R) | SQL (SQL Server, PostgreSQL) | Power BI (DAX) | Microsoft Office\\n● Developing - Git | Python (Pandas, NumPy, DASH) | VBA\\n● Some exposure - Typescript | HTML | CSS | WSL | Nano | Powershell | Nodejs | Wagtail\\nWORK EXPERIENCE\\nScarlatti Wellington, NZ\\nData Analyst Nov 2022 - Nov 2024\\n● Developed and implemented queries of the Statistics New Zealand Integrated Data Infrastructure\\n(IDI), in order to find insights about the movements, shape and size of New Zealand workforces,\\nusing SQL\\n● Processed data and carried out statistical modelling using R and Python for the independent review of\\nthe trial of Facial Recognition systems in New Zealand supermarkets - a trial of international\\nsignificance (further reading)\\n● Built a dashboard which illustrates results of a longitudinal survey of stakeholders in the New\\nZealand Beef Industry, using R, Power BI and Wagtail (see dashboard)\\n● Established a database (which underpins the below tool) on the workforce requirements of various\\nland-uses in New Zealand using R, in order for land stewards to scope potential land use change\\nopportunities (see tool)\\n● Created a macro using VBA to streamline the process of exporting outlook calendars for our internal\\ntimesheets, saving the company ~4 person hours each month\\nCigna New Zealand (now Chubb) Wellington, NZ\\nActuarial Intern Nov 2021 - Feb 2022\\n● Assisted in year end NZ IFRS/US GAAP valuation reporting (including inflation\\nassumptions/forecasts etc.)\\n● Developed a claims register tool using Excel and VBA for a new insurance product, which\\nautomatically populated a secure database and generated an email to the head claims consultant\\n● Improved various Excel tools used for the pricing and repricing of insurance products\\nEDUCATION\\nVictoria University of Wellington Wellington, NZ\\nBachelor of Science in Mathematics and Actuarial Science Nov 2022\\n● Victoria University of Wellington Cigna New Zealand Actuarial Scholarship (further reading)\\n● Victoria University of Wellington Tangiwai Scholarship\\n● Dean’s List for Academic Excellence (2020)\\nFrancis Douglas Memorial College New Plymouth, NZ\\nNCEA levels 1-3 endorsed with Excellence Nov 2019\\n● Taranaki Scholarship Trust Board Award\\nINTERESTS\\nIn my spare time I like to keep active by going for a run a few times a week, and occasionally swimming\\nsome lengths here and there. I am also a big live music and sport enthusiast - both things I have loved\\nexploring as a relatively new Melbournian. When I am not being active or social you can often find me\\nreading novels or watching films.',\n",
       "  '1f04921d492de215': 'ERIC ZHANG\\nD a t a P r o f e s s i o n a l\\nABOUT ME\\nData professional with over 6 years total experience in IT working with\\nlarge business datasets. Skilled in database management, data\\n+61-468-839-362\\nvisualisation, programming, and environment management.\\nezhang.98@outlook.com\\nWorked on major projects transitioning legacy systems to new platforms\\nlinkedin.com/in/eric-t-zhang/ and cloud infrastructure.\\nMelbourne, Victoria, Demonstrated proficiency in DevOps, SQL, data visualisation and\\nAustralia Software Engineering workflow.\\nWORK EXPERIENCE\\nJul 2021- May 2025\\nNHP Electrical Engineering Products\\nIT Applications Support Engineer\\nMaintained environment databases via Powershell and SQL scripting; worked on a major project and\\nreduced processing time by 89%, reduced overall tenant size by 65% in accordance with business goals\\nServed as internal support for developers and testers; working with engineers to diagnose issues\\nCreated and debugged CI/CD pipelines within DevOps supporting deployment automation.\\nMaintained and deployed new development/testing environments for the business\\nPlanned and scheduled major update cycles across multiple business environments\\nManaged code releases across environments to ensure continued functionality\\nFeb 2019 - Jul 2021\\nNHP Electrical Engineering Products\\nBusiness Intelligence Analyst\\nWorked with internal stakeholders to gather requirements and translate business needs into technical\\nspecifications\\nGenerated reports and dashboard in Power BI to transform data into presentable information, supporting\\nbusiness processes and performance monitoring\\nMaintained data analytics platforms and issue-spotting to ensure continual improvement of existing BI\\nsystems\\nExposure to the entire BI stack: data lake, data warehouse, SQL scripting, data modelling and analysis\\netc.\\nParticipated in business critical project of transitioning from legacy ERP to D365.\\nDemonstrated proficiency in DAX: created custom functions and expressions in PowerBI, improved\\nreporting speeds by 22%\\nTECHNICAL SKILLS\\nProgramming Languages Python, SQL, R, Dax, Java, C, C++, C#\\nData Tools Microsoft Power BI, Tableau, Analysis Services, SSMS, PowerQuery\\nCloud Platforms Azure DevOps, AWS, Databricks\\nDevOps CI/CD (Git, TFS), Pipeline Management, OOP, Agile\\nTRAINING\\nMicrosoft Azure Data Fundamentals ITIL 4 Foundation - IT Service Management\\nMicrosoft Power BI Data Analyst Associate\\nEDUCATION REFERENCES\\nBachelor of Computer Science Available upon request\\nSwinburne University of Techology\\n2021',\n",
       "  'c5f45dfe145427e1': 'JUSTIN TRAN\\nMelbourne | 0468.365.427 | justintran3103@gmail.com | LinkedIn | GitHub\\nPROFILE\\nWith hands-on experience in business intelligence and data transformation using SQL, Python, and Power BI, I have proven experience in\\noptimizing operations, reducing manual processes by automation, and delivering actionable insights that drive business efficiency.\\nSKILLS\\n• Data Visualisation & BI tools: MS Power BI, Tableau, DAX, MS Excel\\n• Programming Languages: SQL, Python (Pandas, NumPy, Seaborn, DuckDB), R\\n• Data Analytics Techniques: Web Scraping, Basket Analysis, Sentiment Analysis, Statistical Analysis, EDA, Machine Learning.\\n• ERP & CRM Systems: SAP S/4HANA, Zoho CRM\\nPROFESSIONAL EXPERIENCE\\nDATA ANALYST (CONTRACTOR) Jan 2025 – Present\\nLeuleu Accessorize, Hybrid\\n• Conducted operational analysis to identify ordering inefficiencies and inventory gaps, delivering actionable insights to optimise stock\\nand supplier management.\\n• Engineered an automated data pipeline utilizing Python, Selenium, and BeautifulSoup to extract over 4 years of historical order data\\nfrom the online system, decreasing manual data entry efforts by 90% and enhancing inventory accuracy by 30%.\\n• Analyzed seasonal demand fluctuations through interactive visualizations using Power BI, resulting in a strategic adjustment of\\ninventory levels that decreased stockouts by 15% during peak sales periods and enhanced decision-making speed for procurement teams\\nby 30%.\\n• Provided technical mentorship and support to staff during team meetings, enhancing their data analytics capabilities in stock and\\nsupplier management. Ensured reliable data pipeline maintenance to support accurate and efficient decision-making.\\nMore details of my work at: LINK\\nBUSINESS/IT ANALYST (INTERN) Jul 2024 – Nov 2024\\n99aupairs, Bentleigh East VIC 3165, Australia\\n• Analysed and optimised CRM integration processes, identifying inefficiencies and recommending essential data points for\\nintegration, leading to a 15% improvement in operational efficiency and a 20% increase in customer satisfaction, based on post-\\nimplementation feedback.\\n• Reviewed and enhanced candidate recruitment forms, identifying gaps, errors, and process inefficiencies that reduced form completion\\ntime by 25% and increased data quality by 30%, leading to a more efficient and reliable recruitment workflow.\\n• Implemented solutions that reduced recruitment processing time by 30% through refined candidate screening, customized\\nquestioning for diverse backgrounds, and improved data organisation, leading to a faster and more effective recruitment pipeline.\\nMARKET RESEARCH ANALYST Jul 2024 – Nov 2024\\nVitalHubUK, United Kingdom\\n• Conducted in-depth analysis of New Zealand’s digital healthcare market as part of a university-led project for VitalHub. Developed\\na regulatory compliance framework to support market expansion, with a focus on digital innovation and the unique healthcare needs of\\nthe Māori population.\\n• Conducted a comprehensive review of regulatory compliance frameworks to ensure VitalHub’s digital health solutions aligned with\\nNew Zealand’s healthcare data privacy, security, and interoperability standards—contributing to a 100% compliance-ready roadmap\\nand accelerating projected market entry by 3 months.\\n• Developed a comprehensive compliance roadmap addressing data protection, patient information security, and Indigenous\\ndata sovereignty, ensuring VitalHub’s digital health solutions aligned with New Zealand’s regulatory requirements and market\\nexpectations.\\nPROJECTS\\nPower BI Projects with PwC | LINK | CERTIFICARE\\nJob Simulation Data Analysis project Feb 2025\\n• Designed and developed interactive Power BI dashboards to analyze critical business metrics across multiple domains, including\\ntelecom customer retention & churn, HR diversity & inclusion, and call center staff performance, leading to improved data\\naccessibility and faster decision-making. • Engineered advanced DAX calculations to define and visualize dynamic key performance indicators (KPIs) to uncover trends,\\noptimize decision-making, and deliver actionable insights aimed at improving business performance and operational efficiency.\\n• Collaborated with stakeholders with reports and presentations to ensure the data and insights provided were aligned with business\\nobjectives and strategies, enhancing decision-making process.\\nCafe Shop Sales Analysis | LINK\\nPersonal Data Analysis project Jul 2024\\n• Analyzed and visualized sales data for Maven Roasters using SQL and Tableau, delivering actionable insights that influenced key\\nbusiness decisions and drove revenue growth.\\n• Conducted Market Basket Analysis to uncover critical sales trends and customer purchasing patterns, calculating the probability of\\npurchasing Product A with Product B, which led to data-driven product pairing recommendations and increased cross-sell opportunities.\\n• Developed an interactive Tableau dashboard that provided real-time insights and actionable recommendations, resulting in\\nimprovements in sales performance, customer satisfaction, and operational efficiency.\\nU.S. Data Analyst Job Market | LINK\\nPersonal Data Analysis Project Jun 2024\\n• Utilized SQL to analyze and explore the U.S. data-related job market, focusing on data analyst roles, identifying key trends and patterns\\nin job offerings.\\n• Leveraged Power BI to visualize and present insights on top-paying jobs and the most in-demand skills, providing actionable\\nrecommendations for aspiring data analysts entering the job market.\\nEDUCATION & QUALIFICATION\\nSwinburne University of Technology, Hawthorn, VIC\\nBachelor`s Degree of Business Analytics and Analysis\\nCoursework: Database Analysis and Design, Business Analytics and Artificial Intelligence, Machine Learning, Data Visualisation and Business\\nIntelligence, Relational Cloud Approaches for Enterprise Systems, Agile and IT project management.\\nAchievements:\\n• Achieved High Distinction GPA average.\\n• Achieved ‘Best Performance in Unit Database Analysis and Design’ Award, 2022\\n• Recipient of Swinburne International Excellent Scholarship for Undergraduate, 30% tuition, all years applied.\\nCareer Mentor, Australia\\nData Analytics with Python and Power BI Intensive Course | LINK\\nCoursework: SQL Advanced Query formulation, Database & ERD Design, Statistical Analysis, EDA, Data Wrangling and Manipulation,\\nVisual Analytics, Data Scaling/Normalisation, Power Query, Data Modelling, DAX Calculation, Data Visualisation Control and Best Practice.\\nREFERENCE\\n• Ms. Bambi Price, Industry Project Coordinator and Supervisor, Swinburne University, Melbourne, baprice@swin.edu.au\\n• Ms. Irene Becker, Founder & CEO, 99aupairs, Melbourne, irenebecker12@googlemail.com',\n",
       "  'adbef5e743bf5c48': 'LOUIS NANOUX\\n+ 33 6 95 58 53 18 Louisnanoux@outlook.com\\nParis\\nSUMMARY\\nVery complete profile, motivated and graduated in Februrary 2025, I am seeking a challenging\\nopportunity in Finance or as a Data Scientist. I am also able and willing to relocate if necessary.\\nWORK EXPERIENCE\\nFixed Income Trader Assistant, Data Scientist AXA IM, Internship Sep 2024 - Feb 2025\\nImplementing algorithms to automate the trades\\nSecuring Data Quality\\nBuilding report of the trades\\nAssist the Trades\\nEDUCATION\\nMaster of Finance, Data Science and Actuarial Science\\nPSL - Dauphine University, Paris\\nData Science & Analysis : Machine Learning\\n2022-2025\\nQuantitative Trading : Stochastic Finance, Option Pricing\\nStatistics\\nInsurance & Actuarial Science\\nTheorical Mathematics\\nBachelor of Mathematics, Economics, Finance, Actuarial Science\\nPSL - Dauphine University, Paris 2022\\nApplied Mathematics\\nFinance\\nEconomics\\nActuarial Science\\nADDITIONAL INFORMATION\\nTechnical Skills: SQL, Python, Visual Studio, R, C++, Tableau, Bloomberg, Git, Azure\\nLanguages: English, French\\nCertifications: TOEIC : 920/990\\nInterests: Building trading algorithms',\n",
       "  '4a9374ec3b2ca204': 'Kevin (Mengju)  Wu\\n+61 0413976695  |  email: kevinwumj98@gamil.com  |  Linkedin\\n\\n\\t\\t\\nSUMMRAY\\t\\nInnovative Data Analyst with nearly three years of experience, specializing in data-driven decision-making, predictive analytics, and process optimization. Proficient in Python, SQL, Power BI, AWS, and machine learning, with experience delivering insights through Power BI and automated reporting. Highly skilled in auditing and validating data accuracy,cross-functional collaboration, developing predictive models, interactive dashboards, and data-driven strategies. Proven track record of leveraging data to drive innovation and sustainable solutions.\\nSKILLS\\nProgramming Languages: \\tPython, SQL, R, JavaScript\\nData Tools:\\xa0\\t\\t\\tTableau, Power BI, Apache Spark, Apache Kafka, AWS, Rshiny, Excel\\nMachine Learning:\\t\\tModel Training, Computer Vision, Deployment\\nDatabase :               \\t\\tOracle RDBMS, MySQL, DynamoDB\\nWeb Development:\\xa0                     Django, React\\nOther:\\t\\t\\t\\tData Cleaning, Feature Engineering, Predictive Modeling\\n\\nPROFESSIONAL EXPERIENCE\\nClient Services and Operations Analyst\\nAAON Printing & Signage, Melbourne       06/2023 – Present\\nDescription: Leveraged data analytics to optimize advertising operations and support business decision-making for real estate clients.\\nTechnologies:  SQL,\\xa0Tableau, Power BI,\\xa0Python,\\xa0Excel,\\xa0Machine Learning\\nExtract, analyze, and process data using SQL and Python, and visualize data with Tableau and Power BI. Translate complex data and technical terms into meaningful insights to support stakeholder decision-making.\\nAutomated reporting processes using SQL and Python to support internal teams and improve efficiency.\\nCollaborated with cross-functional teams to design data solutions, improving workflow efficiency.\\nAnalyze customer feedback data, including satisfaction surveys and reviews, to identify key trends and insights, enabling data-driven adjustments to internal strategies and operations.\\nProcess Control and Data Analysis Engineer For Energy Sales\\nSinopec Jinan Refinery, Jinan, China     07/2021 – 02/2023\\nDescription: \\xa0Implemented data-driven process control strategies to optimize production efficiency and ensure operational stability in the energy sector.\\nTechnologies:  Excel, DCS (Distributed Control System), Process Control, Statistical Analysis\\nMonitored and adjusted real-time production parameters (e.g., temperature, pressure, flow rates) through DCS to optimize process control and improve workshop stability.\\nResearched industry trends and emerging analytics technologies \\nDesigned dashboards and Excel reports to visualize key performance indicators (KPIs) and support data-driven decision-making.\\nProvided insights on energy production to support strategic planning and sales initiatives. \\nAutomated reporting process using Python. \\n\\nEDUCATION AND CERTIFICATES\\t\\n\\nCompleted in 2024                Master of Science: Data Science                   Monash University\\nCompleted in 2022                Bachelor of Engineer                                     University of Jinan'},\n",
       " 'job_description': 'Graduate Consultant - Data & Analytics\\nAre you a recent graduate with a passion for data and a desire to build a career in analytics and business transformation? Do you enjoy solving complex problems and delivering innovative, data-driven solutions that empower businesses? If so, Synogize is excited to connect with you!\\n \\nWe are seeking a motivated Graduate Consultant to join our growing team of Data & Analytics professionals in Melbourne. This is an exciting opportunity to launch your career in a dynamic field, working alongside experienced consultants on impactful projects. Applicants must have full working rights in Australia.\\n \\nAbout You\\nYou are someone who:\\nIs eager to collaborate with clients to develop tailored data and analytics solutions.\\nEnjoys problem-solving and continuously seeks to optimize the use of data.\\nHas a strong interest in understanding business challenges and translating them into actionable insights.\\nIs seeking a role that supports both personal growth and career development.\\nThrives in a team-oriented environment and is excited to contribute to client success.\\nIs committed to learning and staying current with the latest tools and trends in data analytics.\\nKey Responsibilities\\nData Modeling: Support the design, development, and maintenance of data models to address both internal and client-specific analytics needs.\\nETL Development: Work with data engineers to assist in building efficient ETL/ELT pipelines, transforming raw data into usable datasets.\\nData Transformation: Learn and apply tools like dbt and Matillion for SQL-based transformations to deliver clean, organized datasets for analysis.\\nAnalytics Solutions: Collaborate with senior consultants and business intelligence teams to ensure seamless integration of analytics tools (e.g., Tableau, Power BI, Looker) with data sources.\\nData Quality & Governance: Help maintain data accuracy through testing and governance protocols.\\nClient Engagement: Work with clients under the guidance of senior team members to understand their challenges and contribute to the development of technical solutions.\\nCollaboration: Partner with data scientists, data engineers, and business stakeholders to support data-driven decision-making.\\nContinuous Improvement: Assist in monitoring the performance of data pipelines and models, looking for opportunities to improve efficiency.\\nRequired Skills & Experience\\nA strong academic background in data analytics, computer science, information systems, or a related field.\\nProficiency in SQL and familiarity with data transformation tools (e.g., dbt, Coalesce).\\nExposure to cloud platforms such as Snowflake, BigQuery, Fabric, or Databricks (knowledge from coursework or internships).\\nUnderstanding of ETL/ELT processes.\\nBasic skills in building data visualizations and dashboards using tools like Tableau, Power BI, or Looker.\\nStrong communication skills, with the ability to explain technical concepts to non-technical audiences.\\nEagerness to learn and a proactive approach to tackling new challenges.\\nBonus Skills\\nExperience or coursework in machine learning or advanced data science techniques.\\nAn interest in creating intuitive data visualizations that tell compelling stories.\\nFamiliarity with data governance, ensuring data quality and security.\\nRequired Qualifications\\nA recent graduate with a relevant degree (Data Analytics, Computer Science, Information Systems, or a similar field).\\nMust have valid working rights in Australia.\\nAbout Synogize\\nAt Synogize, we harness synergy and passion to drive success. Founded by Data & Analytics professionals, we pride ourselves on bridging the gap between people, processes, and technology to deliver innovative solutions. Our mission is to create transformative outcomes by aligning data, technology, and talent, helping organizations shape the future of innovation.\\n \\nNext Steps\\nIf this opportunity excites you, we’d love to hear from you! Please apply by submitting your resume.\\n \\nNote to Recruitment Agencies: We have this role covered and do not accept unsolicited CVs. We are not responsible for any fees related to unsolicited resumes. Thank you.',\n",
       " 'job_criteria': ['Data Analytics',\n",
       "  'Business Transformation',\n",
       "  'Information Technology',\n",
       "  'Cloud Computing',\n",
       "  'Data Governance',\n",
       "  'SQL',\n",
       "  'ETL/ELT Processes',\n",
       "  'Data Visualization Tools',\n",
       "  'Data Transformation Tools',\n",
       "  'Data Modeling'],\n",
       " 'batch_size': 5,\n",
       " 'top_candidates_num': 3,\n",
       " 'top_candidates_dict': {'b5142b9b9676a4f9': 'Abhishek Sarda\\n(cid:131) +61455252644 # abhisheksarda0113@gmail.com (cid:239) linkedin.com/in/abhishek-sarda-18a157171\\nProfessional Summary\\nDataprofessionalwithaMaster’sinDataScienceandstrongexpertiseindataanalysis,visualization,and\\ndatabasedevelopment. Skilledindesigning,developing,andoptimizingscalabledatabasesanddatamodelsto\\nsupportlarge-scaledatastorage,analysis,andreporting. ProficientinSQL,Python,AzureServicesandPowerBI,\\nleveragingadvancedanalyticstodriveactionableinsightsandoptimizebusinessstrategies. Experiencedin\\nenhancingcustomerretention,refiningmarketingstrategies,andenablingbusinessgrowththroughpredictive\\nanalytics,automation,andefficientdatamanagement. Passionateaboutcontinuouslearningandapplyingdata\\nsciencesolutionstosolvereal-worldbusinesschallenges.\\nExperience\\nB2CFurniture Nov2022-Aug2024(Part-time),Nov2024-Present(Casual)\\nDataAnalyst Melbourne,Australia\\nAnalyzedcustomerbehaviordatatoidentifytrendsandinsights,resultingina15%increaseincustomerretention\\n•\\nbyoptimizingmarketingstrategies.\\nImplementedmachinelearningmodels(classification&clustering)topredictcustomerpurchasingbehavior,refining\\n•\\nsegmentationandimprovingadconversionratesby10%.\\nDevelopedpredictivemodelsformarkettrendanalysis,providingactionableinsightstothemarketingteamand\\n•\\ncontributingtoimprovedcampaignROIthroughdata-drivendecision-making.\\nBuiltinteractivePowerBIdashboardstovisualizesalesperformance,customersegmentation,andmarketing\\n•\\ncampaignresults,empoweringstakeholderstomakedata-drivendecisions.\\nLeveragedPythontoclean,merge,andtransformdata,automatingreportingprocessestodelivertimelyand\\n•\\naccurateinsights,savingsignificantmanualeffort.\\nPerformedA/Btestingandhypothesistestingtoevaluatemarketingstrategies,providingevidence-based\\n•\\nrecommendationsthatimproveddigitalmarketingeffortsandadperformance.\\nIntegrateddatafrommultipleplatforms(CRM,GoogleAnalytics,marketingtools)intocohesiveSQLdatamodels,\\n•\\nensuringconsistentandaccuratereportingacrossdepartments.\\nCreateddashboardsandreportstocommunicatecomplexdatainsightstostakeholders,enablingbetter\\n•\\ndecision-makingandoptimizationofmarketing,sales,andproductstrategies.\\nConduentBusinessServicesIndiaLLP April2021–Oct2022\\nDataSpecialist Noida,India\\nLedmonthlypaymentprocessingforElectronicChildCare(ECC),analyzingattendancedataandusingadvancedSQL\\n•\\ntoidentifydiscrepancies,ensuringaccuratereconciliationandtimelyreporting.\\nUtilisedSQLtoextract,transform,andintegratedatafromvariousclientdatabases,operationalsystems,and\\n•\\nattendancelogs,ensuringhigh-quality,reliablepaymentreportingforfinanceteams.\\nDevelopedcomplexSQLstoredprocedurestoautomaterecurringfinancialreports,reducingmanualeffortby40%\\n•\\nandimprovingreportingefficiencyandaccuracy,directlysupportingtimelydecision-making.\\nDesignedandimplementedETLpipelinesusingAzureDataFactorytoautomatedataingestion,transformation,and\\n•\\nintegrationfromdiversesourcesintoacentralizeddatawarehouse,enhancingdataaccessibilityandconsistency.\\nLeveragedAzureDatabricksandPySparkforlarge-scaledatatransformations,includingcleansing,deduplication,\\n•\\nandfeatureengineering,ensuringhighdataqualityandconsistentreportingforfinanceandoperationsteams.\\nOptimizedETLworkflowsbyimplementingpartitioning,caching,andjobschedulingtechniques,cuttingdowndata\\n•\\nprocessingtimeby30%andimprovingoverallreportingefficiency.\\nCollaboratedwithfinance,operations,andengineeringteamstointegratediversedatasetsusingPythonandSQL,\\n•\\nstreamliningdatapipelinesandimprovingdecision-makingprocessesacrossdepartments. Projects\\nMetadata-DrivenETLFrameworkforScalableDataProcessing—ADF,Databricks,PySpark,SQL,DataWarehousing\\n* Developedametadata-drivenETLframeworkusingAzureDataFactory,automatingdataingestionand\\ntransformationtoacentralizeddatawarehouse,improvingdataavailabilityandreporting.\\n* CreatedametadatarepositoryinAzureSQLServertostorepipelineconfigurationsanderror-handlinglogic,\\nenablingdynamicETLexecutionwithminimalmanualintervention.\\n* BuiltscalabledataworkflowsinAzureDatabrickswithPySpark,enhancingdataqualitythroughlarge-scale\\ntransformations,cleansing,andenrichment.\\n* ImplementedDeltaLakeonAzureDataLakeStorageformanaginglarge-scaledatawithimprovedquerying\\nefficiency.\\n* OptimizedDatabricksclustersandparallelizedjobs,reducingETLexecutiontime,speedingupdataprocessing.\\n* Designedadatawarehousearchitecturewithdimensionalmodelingtosupportanalyticsandreporting.\\n* DevelopedPowerBIdashboardstovisualizeETLperformanceandkeymetrics,aidinginfasterdecision-making.\\nParkandSwitch|Python,SQL,PySpark,Docker,GitHub,MachineLearning,Azure\\n* DevelopedawebapplicationaspartofmyMaster’scapstoneproject,integratingusageofpublictransportwith\\nprivatevehiclestopromoteeco-friendlytransportationoptionsinMelbourne.\\n* AnalysedhistoricalparkingsensordatausingPySparkonaDockerimagetopredictparkingspotavailability,\\ncreatinginteractivemapstoenhanceuserengagementanddecision-making.\\n* Deployedmachinelearningmodelstorecommendoptimalpublictransportroutes,helpinguserstransition\\nseamlesslyfromprivatevehiclestopublictransit.\\n* UtilisedAzurecloudinfrastructureandApacheKafkaforreal-timedataprocessingandAzureSQLforscalable\\nstorage,enablingefficientdeploymentandmodelscalability.\\n* ImplementedDockerforcontainerizationandusedGitHubforversioncontrol,ensuringsmoothdeploymentand\\nefficientteamcollaboration.\\n* Ledprojectplanning,taskdelegation,andteamcoordination,ensuringtimelydeliverywhilefosteringa\\ncollaborativeteamenvironment.\\n* Deliveredaninnovativesolutionaddressingurbantransportationchallenges,integratingparkingdata,machine\\nlearninginsights,andpublictransportrecommendationsforsustainablecommutingchoices.\\nEducation\\nMonashUniversity June2024\\nMastersofDataScience(WAM:72) Melbourne,Australia\\n* RelevantCoursework:DataWrangling,DataVisualisation,StatisticalDatamodeling(Python),DataWarehouse\\ndesigning,AzureServices,DataEngineering\\nAmityUniversity April2021\\nBachelorsofTechnology:Electronics&Communications(WAM:7.59) Noida,India\\n* RelevantCoursework:Machinelearning,Python,Databases,SQL\\nTechnical Skills\\nLanguages:Python(anditslibraries),R,RShiny,pandas,numpy,scipy,ApachePySpark\\nCloud:AzureDataFactory,ETL,AzureDatabricks\\nDataAnalysis&Visualization:Python,RSHinyTableau,PowerBI,Excel,PredictiveModeling,MachineLearning\\nVersionControl&APITesting:Git,Postman\\nProjectManagement:Agile,Trello,Kanban,LeanKit\\nSoftSkills:Communication,Teamwork,Leadership,CriticalThinking',\n",
       "  '1f04921d492de215': 'ERIC ZHANG\\nD a t a P r o f e s s i o n a l\\nABOUT ME\\nData professional with over 6 years total experience in IT working with\\nlarge business datasets. Skilled in database management, data\\n+61-468-839-362\\nvisualisation, programming, and environment management.\\nezhang.98@outlook.com\\nWorked on major projects transitioning legacy systems to new platforms\\nlinkedin.com/in/eric-t-zhang/ and cloud infrastructure.\\nMelbourne, Victoria, Demonstrated proficiency in DevOps, SQL, data visualisation and\\nAustralia Software Engineering workflow.\\nWORK EXPERIENCE\\nJul 2021- May 2025\\nNHP Electrical Engineering Products\\nIT Applications Support Engineer\\nMaintained environment databases via Powershell and SQL scripting; worked on a major project and\\nreduced processing time by 89%, reduced overall tenant size by 65% in accordance with business goals\\nServed as internal support for developers and testers; working with engineers to diagnose issues\\nCreated and debugged CI/CD pipelines within DevOps supporting deployment automation.\\nMaintained and deployed new development/testing environments for the business\\nPlanned and scheduled major update cycles across multiple business environments\\nManaged code releases across environments to ensure continued functionality\\nFeb 2019 - Jul 2021\\nNHP Electrical Engineering Products\\nBusiness Intelligence Analyst\\nWorked with internal stakeholders to gather requirements and translate business needs into technical\\nspecifications\\nGenerated reports and dashboard in Power BI to transform data into presentable information, supporting\\nbusiness processes and performance monitoring\\nMaintained data analytics platforms and issue-spotting to ensure continual improvement of existing BI\\nsystems\\nExposure to the entire BI stack: data lake, data warehouse, SQL scripting, data modelling and analysis\\netc.\\nParticipated in business critical project of transitioning from legacy ERP to D365.\\nDemonstrated proficiency in DAX: created custom functions and expressions in PowerBI, improved\\nreporting speeds by 22%\\nTECHNICAL SKILLS\\nProgramming Languages Python, SQL, R, Dax, Java, C, C++, C#\\nData Tools Microsoft Power BI, Tableau, Analysis Services, SSMS, PowerQuery\\nCloud Platforms Azure DevOps, AWS, Databricks\\nDevOps CI/CD (Git, TFS), Pipeline Management, OOP, Agile\\nTRAINING\\nMicrosoft Azure Data Fundamentals ITIL 4 Foundation - IT Service Management\\nMicrosoft Power BI Data Analyst Associate\\nEDUCATION REFERENCES\\nBachelor of Computer Science Available upon request\\nSwinburne University of Techology\\n2021'},\n",
       " 'initial_tournament_groups': {0: ['dd417e3f63181d20',\n",
       "   '0176a86af53206ae',\n",
       "   'b5142b9b9676a4f9',\n",
       "   '2b4c705e8b2a0770',\n",
       "   'c69f2087775fc760'],\n",
       "  1: ['ccfed7e7060c9916',\n",
       "   '6700f96c37c87933',\n",
       "   '8ed9c5e84fb32c65',\n",
       "   '77aacb560b1dd217',\n",
       "   'a8ca1108dd06b907'],\n",
       "  2: ['de4e24b044e42f42',\n",
       "   'af62b86c3a326086',\n",
       "   'fae54469e0d024e8',\n",
       "   'c5f45dfe145427e1',\n",
       "   '934d1fdb8a6d1063'],\n",
       "  3: ['493d0d19e0150412',\n",
       "   'f2ca85739fda42e4',\n",
       "   '90e485656ce9236d',\n",
       "   'e5f8984dfa4cae3b',\n",
       "   '8821085d3ae9e0ce'],\n",
       "  4: ['6d743d56d9692fd0',\n",
       "   'c9cddf5fb828eab6',\n",
       "   '1f04921d492de215',\n",
       "   'f476ce0793a219b1',\n",
       "   'adbef5e743bf5c48'],\n",
       "  5: ['4a9374ec3b2ca204']},\n",
       " 'round_summaries': {},\n",
       " 'round_rationales': {}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5dff4d",
   "metadata": {},
   "source": [
    "### creating the format for the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ab3735c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f2ca85739fda42e4': {'experience': [{'job_title': 'Independent Data Analyst',\n",
       "    'company_name': 'Self-Employed',\n",
       "    'years_of_experience': 'Apr 2021 - Present',\n",
       "    'responsibilities': ['Delivered tailored data solutions to diverse clients and through self-initiated projects, translating complex analytics into actionable business recommendations',\n",
       "     'Executed A/B testing on marketing strategies for an e-commerce platform, analyzing revenue, CTR, and conversions; delivered data-backed recommendations that improved campaign ROI',\n",
       "     'Performed sales and customer segmentation analysis for a digital music store; identified high-value genres and user groups, enabling targeted business strategies and operational improvements to enhance revenue by 12%']},\n",
       "   {'job_title': 'Research Data Intern',\n",
       "    'company_name': 'Melbourne Data Analytics Platform (MDAP)',\n",
       "    'years_of_experience': 'Jul 2024 - Oct 2024',\n",
       "    'responsibilities': ['Developed and published SeqBank, a command-line tool for managing large-scale DNA sequences, improving data retrieval and workflow efficiency by 35%; published on PyPI and achieved 100% test coverage',\n",
       "     'Optimized genomic models through hyperparameter tuning, pushing accuracy over 90%',\n",
       "     'Authored comprehensive technical documentation, facilitating cross-disciplinary collaboration and tool adoption',\n",
       "     'Maintained high standards of data integrity and confidentiality, especially when handling sensitive health and genomic datasets']},\n",
       "   {'job_title': 'Student Tutor (ST)',\n",
       "    'company_name': 'BRAC University',\n",
       "    'years_of_experience': 'Jun 2021 - May 2022',\n",
       "    'responsibilities': ['Taught foundational programming and computer science to 100+ students, improving academic performance by 10%',\n",
       "     'Organised coding contests to foster learning, boosting student participation on competitive platforms by 25%']},\n",
       "   {'job_title': 'Project Lead and Manager',\n",
       "    'company_name': 'Care2U',\n",
       "    'years_of_experience': 'May 2020 - Jan 2021',\n",
       "    'responsibilities': ['Led a multidisciplinary team to design and develop Care2U, a PPE donation platform for healthcare workers during COVID-19, and managed overall operations',\n",
       "     'Self-taught basic web development skills and applied Agile practices, reducing project development time by 40%',\n",
       "     'Facilitated over 500 donations within six months, collaborating with the local government to expand impact']}]},\n",
       " '0176a86af53206ae': {'experience': [{'job_title': 'Data Analyst',\n",
       "    'company_name': 'Johnson & Johnson (J&J), Australia',\n",
       "    'years_of_experience': 'Jan 2025 – Present',\n",
       "    'responsibilities': ['Analyzed historical inventory and sales data using SQL to identify trends in device usage and demand, ensuring optimal stock levels and reducing stockouts.',\n",
       "     'Cleaned and merged large datasets with Python (Pandas, NumPy), handling missing values and ensuring data consistency for accurate reporting and forecasting.',\n",
       "     'Developed predictive models to forecast future demand for medical devices, optimizing inventory levels using Python.',\n",
       "     'Visualized real-time inventory and sales data using Tableau, creating dashboards displaying device usage trends, stock levels, and regional demand.',\n",
       "     'Produced comprehensive reports in Excel, including pivot tables and charts to analyze inventory turnover rates and highlight slow-moving devices.']},\n",
       "   {'job_title': 'Data Scientist Intern',\n",
       "    'company_name': 'Australian Red Cross Lifeblood, Australia',\n",
       "    'years_of_experience': 'May 2024 – Dec 2024',\n",
       "    'responsibilities': ['Processed and cleaned large datasets using Python to ensure consistency in donor information, optimizing the accuracy of the analysis.',\n",
       "     'Developed predictive models in Python to forecast blood type demand in different regions, providing actionable insights for supply chain and donation planning.',\n",
       "     'Utilized SQL to extract donor data, generating key metrics such as monthly donations, donor engagement levels, and donation type distributions.',\n",
       "     'Created interactive dashboards in Tableau to visualize donation trends and regional variations, aiding in informed decisions for targeted donation drives.',\n",
       "     'Generated reports in Excel, presenting analysis on donor retention and engagement, supporting marketing strategies to boost donor participation.']},\n",
       "   {'job_title': 'Data Scientist',\n",
       "    'company_name': 'Mouse & Cheese Design Studio, India',\n",
       "    'years_of_experience': 'Jan 2023 – Apr 2024',\n",
       "    'responsibilities': ['Constructed predictive models using Python, R, and XGBoost to identify demographic groups with low survey engagement.',\n",
       "     'Executed multivariate statistical analyses to evaluate trends affecting citizen participation, offering insights for campaign strategy refinement.',\n",
       "     'Monitored user engagement metrics via Google Analytics to enhance platform accessibility and user experience.',\n",
       "     'Integrated real-time data pipelines to track website traffic, supporting marketing and communications in digital engagement campaigns.',\n",
       "     'Collaborated with UI/UX teams and government stakeholders to translate data insights into user-centric platform improvements.']},\n",
       "   {'job_title': 'Data Analyst',\n",
       "    'company_name': 'SixD Engineering Solutions',\n",
       "    'years_of_experience': 'Jul 2021 – Dec 2022',\n",
       "    'responsibilities': ['Designed automated data ingestion pipelines using Python to collect and preprocess sensor data from manufacturing equipment.',\n",
       "     'Implemented complex SQL queries to extract actionable insights from data related to machine uptime and production cycles.',\n",
       "     'Built interactive dashboards in Power BI to visualize plant KPIs including machine utilization and energy usage.',\n",
       "     'Performed root cause analysis on production delays and inefficiencies, aiding operations and finance teams in cost control.',\n",
       "     'Coordinated with production engineers, quality teams, and financial analysts to ensure data models aligned with operational needs.']}]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class JobExperience(BaseModel):\n",
    "    \"\"\"Details about a single job experience.\"\"\"\n",
    "    job_title: str = Field(..., description=\"The candidate's job title.\")\n",
    "    company_name: str = Field(..., description=\"The name of the company.\")\n",
    "    years_of_experience: str = Field(..., description=\"The duration of employment (e.g., '2020 - 2023' or '2 years').\")\n",
    "    responsibilities: List[str] = Field(..., description=\"A list of key responsibilities and accomplishments.\")\n",
    "\n",
    "class CandidateExperience(BaseModel):\n",
    "    \"\"\"The structured extraction of a candidate's professional experience.\"\"\"\n",
    "    experience: List[JobExperience] = Field(..., description=\"A list of all professional experiences listed on the resume.\")\n",
    "\n",
    "candidate_experience = json.load(\n",
    "    fp = open(\"/Users/santiagocardenas/Documents/MDSI/202502/internship/internship_project/data/processed/candidates_experience.json\", mode = 'r'),\n",
    ")\n",
    "def format_experience_for_prompt(candidate_exp: CandidateExperience) -> str:\n",
    "    lines = []\n",
    "    for i, job in enumerate(candidate_exp.experience, start=1):\n",
    "        lines.append(f\"{i}. Job Title: {job.job_title}\")\n",
    "        lines.append(f\"   Company: {job.company_name}\")\n",
    "        lines.append(f\"   Duration: {job.years_of_experience}\")\n",
    "        lines.append(f\"   Responsibilities:\")\n",
    "        for r in job.responsibilities:\n",
    "            lines.append(f\"     - {r}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "candidate_experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13d17c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': CandidateExperience(experience=[JobExperience(job_title='finance expert', company_name='tyba', years_of_experience='from april 2020 to december 2022', responsibilities=['develop and agentic worflow for resumes screening', 'colaborate with other interns to build agents'])]),\n",
       " '2': CandidateExperience(experience=[JobExperience(job_title='data scientist intern', company_name='synogize', years_of_experience='from August 2025 to November 2025', responsibilities=['develop and agentic worflow for resumes screening', 'colaborate with other interns to build agents'])])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_1 = CandidateExperience(\n",
    "    experience = [\n",
    "        JobExperience(\n",
    "            job_title = \"data scientist intern\",\n",
    "            company_name = \"synogize\",\n",
    "            years_of_experience = \"from August 2025 to November 2025\",\n",
    "            responsibilities = ['develop and agentic worflow for resumes screening', \"colaborate with other interns to build agents\"]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "experience_2 = CandidateExperience(\n",
    "    experience = [\n",
    "        JobExperience(\n",
    "            job_title = \"finance expert\",\n",
    "            company_name = \"tyba\",\n",
    "            years_of_experience = \"from april 2020 to december 2022\",\n",
    "            responsibilities = ['develop and agentic worflow for resumes screening', \"colaborate with other interns to build agents\"]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "tmp = {\n",
    "    \"1\" : experience_2,\n",
    "    \"2\" : experience_1\n",
    "}\n",
    "tmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
