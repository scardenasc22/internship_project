{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd76ae9",
   "metadata": {},
   "source": [
    "### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af4f2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['c69f2087775fc760', '1', 'b5142b9b9676a4f9', 'c5f45dfe145427e1'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from json import load\n",
    "root = \"/\".join(os.getcwd().split(\"/\")[:-1])\n",
    "experience_path = os.path.join(root, \"data\", \"processed\", \"candidates_experience.json\")\n",
    "with open(file = experience_path, mode = \"r\") as file:\n",
    "    experience_dict = load(file)\n",
    "experience_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ea0a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experience': [{'job_title': 'Data Analyst Assistant',\n",
       "   'company_name': 'UTS, Sydney, Australia',\n",
       "   'years_of_experience': 'Sep 2025 – Current',\n",
       "   'responsibilities': ['Redesigned and deployed robust pipelines for UTS Alumni, Donors, Student Data from NXT CRM into a data warehouse using on premise SQL server.',\n",
       "    'Used Airflow for pipeline orchestration.']},\n",
       "  {'job_title': 'Data Engineer III',\n",
       "   'company_name': 'Wizeline - Nike, Bogotá, CO',\n",
       "   'years_of_experience': 'Dec 2024 – Jun 2025',\n",
       "   'responsibilities': ['Oversaw a team of three Level 2 data engineers within a 14-member team, providing technical mentorship.',\n",
       "    'Orchestrated the migration of core Airflow DAGs to Databricks Workflows.',\n",
       "    'Engineered and maintained seven core data pipelines ingesting global data for locations, employees, leasing, building access, and IoT sensor-based headcounts.',\n",
       "    'Structured a medallion data model with Delta Lake and enforced governance with Unity Catalog to facilitate self-service analytics for Data Science and HR divisions.',\n",
       "    'Partnered with data analysts and product managers in agile sprints, presenting updates in demo days and translating business needs into technical specifications.',\n",
       "    'Spearheaded a cost-optimization analysis, applying strategies that yielded a 30% reduction in monthly pipeline execution costs on the Databricks platform.']},\n",
       "  {'job_title': 'Ssr. Data Engineer',\n",
       "   'company_name': 'SII Colombia - Amadeus, Bogotá, CO',\n",
       "   'years_of_experience': 'Apr 2024 – Dec 2024',\n",
       "   'responsibilities': ['Led a team of two engineers in a strategic migration of a legacy on-premise solution to a scalable Azure cloud environment.',\n",
       "    'Redesigned and deployed robust pipelines for petabyte-scale data from airline, reservation, and hospitality partners, including Hilton and Marriott.',\n",
       "    'Executed a phased migration, running cloud and on-premise systems in parallel to ensure data consistency and achieve zero downtime during the transition.',\n",
       "    'Accelerated critical data processing and query times by over 400x, reducing execution from three days to under 10 minutes.']},\n",
       "  {'job_title': 'Ssr. Machine Learning Engineer (Part Time)',\n",
       "   'company_name': 'BoostEducation, Fort Lauderdale, FL',\n",
       "   'years_of_experience': 'Oct 2023 – Jun 2025',\n",
       "   'responsibilities': [\"Jointly created a proprietary 'Cognitive Architecture' from the ground up, building a platform for adaptive, AI-driven digital teachers for an early-stage startup.\",\n",
       "    \"Devised a multi-component system with a data ingestion module for user interactions, a processing engine for insight generation with NLP, and a feedback loop to update an agent's knowledge graph.\",\n",
       "    'Propelled business growth by securing partnerships with 10 educational institutions across the US and LATAM and initiated pilot discussions with a major Colombian bank.']},\n",
       "  {'job_title': 'Coordinator, Competitive Intelligence Area',\n",
       "   'company_name': 'Pontificia Universidad Javeriana, Bogotá, CO',\n",
       "   'years_of_experience': 'Dec 2022 – Feb 2024',\n",
       "   'responsibilities': ['Developed and automated ETL pipelines using Python (Pandas) and SQL to produce critical analytics for over 30,000 users and the Colombian Ministry of Education.',\n",
       "    'Generated annual reports on key university metrics, including graduate outcomes, dropout rates, subject success rates, and student enrolment statuses.',\n",
       "    'Spearheaded a data-driven initiative to reform graduation honours by formulating a new algorithm to set fair award thresholds based on faculty grades.',\n",
       "    'Established automated data consumption layers from sources like the Scopus API and Web of Science to ingest details of publications by faculty and students.']},\n",
       "  {'job_title': 'Data Analyst',\n",
       "   'company_name': 'Jesuit Refugee Service, Bogotá, CO',\n",
       "   'years_of_experience': 'Jan 2020 – Jul 2022',\n",
       "   'responsibilities': ['Applied linear programming techniques to formulate optimization algorithms, maximizing allocation of donor funds for camp necessities.',\n",
       "    'Built time-series forecasting models to predict funding shortfalls, informing proactive communication with donors to secure resources.',\n",
       "    'Automated data workflows with Azure Data Factory to analyze resource utilization and monitor real-time environmental conditions in camps.']}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_experience = experience_dict['1']\n",
    "sample_experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947d8cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graduate Consultant - Data & Analytics\n",
      "Are you a recent graduate with a passion for data and a desire to build a career in analytics and business transformation? Do you enjoy solving complex problems and delivering innovative, data-driven solutions that empower businesses? If so, Synogize is excited to connect with you!\n",
      " \n",
      "We are seeking a motivated Graduate Consultant to join our growing team of Data & Analytics professionals in Melbourne. This is an exciting opportunity to launch your career in a dynamic field, working alongside experienced consultants on impactful projects. Applicants must have full working rights in Australia.\n",
      " \n",
      "About You\n",
      "You are someone who:\n",
      "Is eager to collaborate with clients to develop tailored data and analytics solutions.\n",
      "Enjoys problem-solving and continuously seeks to optimize the use of data.\n",
      "Has a strong interest in understanding business challenges and translating them into actionable insights.\n",
      "Is seeking a role that supports both personal growth and career development.\n",
      "Thrives in a team-oriented environment and is excited to contribute to client success.\n",
      "Is committed to learning and staying current with the latest tools and trends in data analytics.\n",
      "Key Responsibilities\n",
      "Data Modeling: Support the design, development, and maintenance of data models to address both internal and client-specific analytics needs.\n",
      "ETL Development: Work with data engineers to assist in building efficient ETL/ELT pipelines, transforming raw data into usable datasets.\n",
      "Data Transformation: Learn and apply tools like dbt and Matillion for SQL-based transformations to deliver clean, organized datasets for analysis.\n",
      "Analytics Solutions: Collaborate with senior consultants and business intelligence teams to ensure seamless integration of analytics tools (e.g., Tableau, Power BI, Looker) with data sources.\n",
      "Data Quality & Governance: Help maintain data accuracy through testing and governance protocols.\n",
      "Client Engagement: Work with clients under the guidance of senior team members to understand their challenges and contribute to the development of technical solutions.\n",
      "Collaboration: Partner with data scientists, data engineers, and business stakeholders to support data-driven decision-making.\n",
      "Continuous Improvement: Assist in monitoring the performance of data pipelines and models, looking for opportunities to improve efficiency.\n",
      "Required Skills & Experience\n",
      "A strong academic background in data analytics, computer science, information systems, or a related field.\n",
      "Proficiency in SQL and familiarity with data transformation tools (e.g., dbt, Coalesce).\n",
      "Exposure to cloud platforms such as Snowflake, BigQuery, Fabric, or Databricks (knowledge from coursework or internships).\n",
      "Understanding of ETL/ELT processes.\n",
      "Basic skills in building data visualizations and dashboards using tools like Tableau, Power BI, or Looker.\n",
      "Strong communication skills, with the ability to explain technical concepts to non-technical audiences.\n",
      "Eagerness to learn and a proactive approach to tackling new challenges.\n",
      "Bonus Skills\n",
      "Experience or coursework in machine learning or advanced data science techniques.\n",
      "An interest in creating intuitive data visualizations that tell compelling stories.\n",
      "Familiarity with data governance, ensuring data quality and security.\n",
      "Required Qualifications\n",
      "A recent graduate with a relevant degree (Data Analytics, Computer Science, Information Systems, or a similar field).\n",
      "Must have valid working rights in Australia.\n",
      "About Synogize\n",
      "At Synogize, we harness synergy and passion to drive success. Founded by Data & Analytics professionals, we pride ourselves on bridging the gap between people, processes, and technology to deliver innovative solutions. Our mission is to create transformative outcomes by aligning data, technology, and talent, helping organizations shape the future of innovation.\n",
      " \n",
      "Next Steps\n",
      "If this opportunity excites you, we’d love to hear from you! Please apply by submitting your resume.\n",
      " \n",
      "Note to Recruitment Agencies: We have this role covered and do not accept unsolicited CVs. We are not responsible for any fees related to unsolicited resumes. Thank you.\n"
     ]
    }
   ],
   "source": [
    "from functions import text_extraction\n",
    "job_description = text_extraction(\n",
    "    os.path.join(root, \"data\", \"raw\", \"job\", \"data_role_des.txt\")\n",
    ")\n",
    "print(job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a37e03b",
   "metadata": {},
   "source": [
    "### what should we look for in the answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "683f24e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InterviewQuestions(experience_questions=['Can you describe the process and outcomes of migrating on-premises solutions to cloud environments, like the strategic migration to Azure you led at SII Colombia?', 'What specific challenges did you encounter while managing data governance with Unity Catalog for Wizeline, and how did you address them?', 'Tell us more about your experience with tools like Airflow for pipeline orchestration. How did you utilize these tools in your roles at UTS or Wizeline?', \"Could you elaborate on your experience with data transformation tools and how you've applied SQL-based transformations in your previous roles?\"], situational_questions=['Imagine you are working on a project where the data sources are inconsistent and lack standardization. How would you tackle this issue to ensure reliable data analytics solutions for the client?', 'A client is not satisfied with the current insights generated from their data analytics setup. How would you approach understanding the root of their dissatisfaction and improving the quality of the insights?', 'If you were tasked with optimizing the performance of a data pipeline but faced resource constraints, how would you prioritize which areas to improve first?', 'Describe a time you had to work collaboratively with a cross-functional team. How did you manage differing priorities and communicated effectively to achieve a common goal?'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from functions import format_experience_for_prompt\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-2024-08-06\"\n",
    ")\n",
    "\n",
    "class InterviewQuestions(BaseModel):\n",
    "    \"\"\"generation of interview questions\"\"\"\n",
    "    experience_questions : List[str] = Field(\n",
    "        description = \"List of questions aimed to clarify or expand upon the candidate's professional experience\"\n",
    "    )\n",
    "    situational_questions : List[str] = Field(\n",
    "        description = \"List of questions aimed to understand the candidate's behavior, problem-solving abilities, and soft skills in relation to the target role\"\n",
    "    )\n",
    "    @field_validator(\"experience_questions\", \"situational_questions\")\n",
    "    @classmethod\n",
    "    def check_questions(cls, v: List[int]):\n",
    "        # This validator is crucial for ensuring data integrity\n",
    "        if len(v) == 0:\n",
    "            raise ValueError(\"The list should contain at least one value\")\n",
    "        return v\n",
    "\n",
    "interview_questions_template = [\n",
    "    (\n",
    "        \"system\",\n",
    "        (\n",
    "            \"You are an expert recruiter. Your task is to analyze a candidate's experience and job description to \"\n",
    "            \"generate a set of interview questions. Your output must be a list of questions.\"\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        (\n",
    "            \"Please provide a JSON object with the following keys.\\n\"\n",
    "            \"- **Experience questions**: <List of 4 questions aimed to clarify or expand upon the candidate's professional experience, \"\n",
    "            \"skills, and achievements. These questions should help assess the candidate's expertise and past job performance.>\\n\"\n",
    "            \"- **Situational questions**: <List of 4 questions aimed to understand the candidate's behavior, problem-solving abilities, \"\n",
    "            \"and soft skills in relation to the target role. These questions should explore how the candidate handles specific situations and challenges.>\\n\"\n",
    "            \"\\n\\n**Job Description:**\\n{job_description}\"\n",
    "            \"\\n\\n**Candidate Experience:**\\n{candidate_experience}\"\n",
    "        )\n",
    "    )\n",
    "]\n",
    "interview_questions_prompt = ChatPromptTemplate.from_messages(messages = interview_questions_template)\n",
    "\n",
    "llm_constrained = llm.with_structured_output(schema = InterviewQuestions)\n",
    "\n",
    "questions_chain = interview_questions_prompt | llm_constrained\n",
    "\n",
    "sample_questions = questions_chain.invoke(\n",
    "    input = {\n",
    "        \"job_description\" : job_description,\n",
    "        \"candidate_experience\" : format_experience_for_prompt(sample_experience)\n",
    "    }\n",
    ")\n",
    "sample_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66ee3fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI  # or your LLM client\n",
    "\n",
    "# 1) Define a clean schema\n",
    "class QAItem(BaseModel):\n",
    "    question: str = Field(description=\"Interview question\")\n",
    "    look_for: str = Field(description=\"What the recruiter should look for in the candidate's answer\")\n",
    "\n",
    "class InterviewQuestionsV2(BaseModel):\n",
    "    experience_questions: List[QAItem] = Field(\n",
    "        description=\"4 questions to expand/clarify candidate experience, with what to look for\"\n",
    "    )\n",
    "    technical_questions: List[QAItem] = Field(\n",
    "        description=\"4 questions to assess job-relevant tools/skills, with what to look for\"\n",
    "    )\n",
    "\n",
    "# 2) Build prompt\n",
    "template = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an expert recruiter. Analyze the candidate's experience and job description to \"\n",
    "        \"produce interview questions and what to look for in responses.\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"Generate 4 experience questions and 4 technical questions. Each must include what to look for.\\n\"\n",
    "        \"**Job Description:**\\n{job_description}\\n\"\n",
    "        \"**Candidate Experience:**\\n{candidate_experience}\"\n",
    "    ),\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(template)\n",
    "\n",
    "# 3) Constrain output to the schema\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")  # pick your model\n",
    "llm_constrained = llm.with_structured_output(schema=InterviewQuestionsV2)\n",
    "\n",
    "chain = prompt | llm_constrained\n",
    "\n",
    "# 4) Invoke\n",
    "sample_questions = chain.invoke(\n",
    "    input = {\n",
    "        \"job_description\" : job_description,\n",
    "        \"candidate_experience\" : format_experience_for_prompt(sample_experience)\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fce9f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InterviewQuestionsV2(experience_questions=[QAItem(question='Can you describe your experience redesigning and deploying robust data pipelines in your current role? What challenges did you face, and how did you overcome them?', look_for='Look for specific examples of problem-solving, innovative approaches, and the ability to adapt to obstacles. The candidate should highlight both technical skills and teamwork.'), QAItem(question='In your role as a Data Engineer III, how did you communicate technical needs to non-technical stakeholders? Can you provide an example of a project where effective communication made a difference?', look_for=\"Assess the ability to simplify complex concepts and gauge the candidate's interpersonal skills. Look for examples that illustrate successful outcomes stemming from clear communication.\"), QAItem(question=\"Tell me about a time you worked in a team-oriented environment. What role did you play, and how did your contributions affect the project's success?\", look_for='Evaluate candidates on their ability to collaborate, support peers, and demonstrate leadership or initiative within a team. Look for understanding of collective goals.'), QAItem(question='Could you discuss your experience with data governance and maintaining data quality? What methods did you use to ensure accuracy and compliance?', look_for='Listen for a solid understanding of data governance principles and specific methodologies or frameworks. Seek examples that demonstrate attention to detail and accountability.')], technical_questions=[QAItem(question='Can you explain the ETL process and your experience with tools like Python, SQL, and Azure Data Factory in carrying out ETL tasks?', look_for='Look for a clear understanding of the ETL process, familiarity with the mentioned tools, and practical experiences that demonstrate competence in executing ETL workflows.'), QAItem(question='Describe your experience with data transformation tools such as dbt or Matillion. Can you provide an example of how you utilized these tools effectively in a project?', look_for='Assess familiarity with specific tools, depth of experience in data transformation processes, and practical challenges navigated during project execution.'), QAItem(question=\"What data visualization tools have you used, and how have you applied them to present data insights to stakeholders? Can you give an example of a dashboard or report you've created?\", look_for='Evaluate experience and comfort level with visualization tools. Look for creativity, clarity, and the impact of the visualizations on decision-making.'), QAItem(question='How do you keep yourself updated with the latest trends and technologies in data analytics? Can you name a recent development that you found particularly interesting or useful?', look_for='Look for eagerness to learn and stay current. Assess how they approach personal and professional development and whether they are proactive in seeking out new knowledge.')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeec962e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Can you explain the ETL process and your experience with tools like Python, SQL, and Azure Data Factory in carrying out ETL tasks?\n",
      "what to look for: Look for a clear understanding of the ETL process, familiarity with the mentioned tools, and practical experiences that demonstrate competence in executing ETL workflows.\n",
      "--------------------\n",
      "question: Describe your experience with data transformation tools such as dbt or Matillion. Can you provide an example of how you utilized these tools effectively in a project?\n",
      "what to look for: Assess familiarity with specific tools, depth of experience in data transformation processes, and practical challenges navigated during project execution.\n",
      "--------------------\n",
      "question: What data visualization tools have you used, and how have you applied them to present data insights to stakeholders? Can you give an example of a dashboard or report you've created?\n",
      "what to look for: Evaluate experience and comfort level with visualization tools. Look for creativity, clarity, and the impact of the visualizations on decision-making.\n",
      "--------------------\n",
      "question: How do you keep yourself updated with the latest trends and technologies in data analytics? Can you name a recent development that you found particularly interesting or useful?\n",
      "what to look for: Look for eagerness to learn and stay current. Assess how they approach personal and professional development and whether they are proactive in seeking out new knowledge.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for q_and_a in sample_questions.technical_questions:\n",
    "    print(f\"question: {q_and_a.question}\")\n",
    "    print(f\"what to look for: {q_and_a.look_for}\")\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff74cd2d",
   "metadata": {},
   "source": [
    "### candidates tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd5afcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round: 1\n",
      "initial candidates: 28\n",
      "********************\n",
      "for the batch: [17, 14, 3, 27, 21]\n",
      "the winners were: [17, 14]\n",
      "--------------------\n",
      "for the batch: [28, 10, 26, 8, 12]\n",
      "the winners were: [12, 28]\n",
      "--------------------\n",
      "for the batch: [4, 1, 18, 24, 22]\n",
      "the winners were: [18, 24]\n",
      "--------------------\n",
      "for the batch: [11, 20, 7, 13, 5]\n",
      "the winners were: [13, 11]\n",
      "--------------------\n",
      "for the batch: [19, 6, 23, 9, 16]\n",
      "the winners were: [23, 6]\n",
      "--------------------\n",
      "for the batch: [25, 2, 15]\n",
      "the winners were: [25, 2]\n",
      "--------------------\n",
      "round: 2\n",
      "initial candidates: 12\n",
      "********************\n",
      "for the batch: [14, 2, 28, 13, 12]\n",
      "the winners were: [14, 12]\n",
      "--------------------\n",
      "for the batch: [24, 23, 18, 25, 17]\n",
      "the winners were: [17, 25]\n",
      "--------------------\n",
      "for the batch: [6, 11]\n",
      "the winners were: [6, 11]\n",
      "--------------------\n",
      "round: 3\n",
      "initial candidates: 6\n",
      "********************\n",
      "for the batch: [11, 17, 25, 14, 6]\n",
      "the winners were: [17, 6]\n",
      "--------------------\n",
      "for the batch: [12]\n",
      "the winners were: [12]\n",
      "--------------------\n",
      "round: 4\n",
      "initial candidates: 3\n",
      "********************\n",
      "for the batch: [6, 12, 17]\n",
      "the winners were: [17, 6]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from math import floor, ceil\n",
    "random.seed(2000)\n",
    "# parameters\n",
    "population = 28\n",
    "batch_size = 5\n",
    "selected_per_batch = 2\n",
    "num_batches = ceil(population / batch_size)\n",
    "# example\n",
    "candidates = list(range(1, population + 1))\n",
    "# controling the rounds\n",
    "candidates_copy = candidates.copy()\n",
    "for round in range(4):\n",
    "    print(f\"round: {round + 1}\")\n",
    "    # random ids\n",
    "    random.shuffle(candidates_copy)\n",
    "    print(f\"initial candidates: {len(candidates_copy)}\")\n",
    "    print(f\"*\"*20)\n",
    "    # batches\n",
    "    batches = [\n",
    "        candidates_copy[i : batch_size + i] for i in range(0, len(candidates_copy) + 1, batch_size)\n",
    "    ]\n",
    "    # selection of the candidates per batch\n",
    "    round_winners = []\n",
    "    for b in batches:\n",
    "        print(f\"for the batch: {b}\")\n",
    "        winners = random.sample(b, k = min(selected_per_batch, len(b)))\n",
    "        print(f\"the winners were: {winners}\")\n",
    "        print(\"-\"*20)\n",
    "        for w in winners:\n",
    "            round_winners.append(w)\n",
    "    candidates_copy = round_winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79881fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round: 1\n",
      "initial candidates: 28\n",
      "********************\n",
      "for the batch: [17, 14, 3, 27, 21]\n",
      "the winners were: [17, 14]\n",
      "--------------------\n",
      "for the batch: [28, 10, 26, 8, 12]\n",
      "the winners were: [12, 28]\n",
      "--------------------\n",
      "for the batch: [4, 1, 18, 24, 22]\n",
      "the winners were: [18, 24]\n",
      "--------------------\n",
      "for the batch: [11, 20, 7, 13, 5]\n",
      "the winners were: [13, 11]\n",
      "--------------------\n",
      "for the batch: [19, 6, 23, 9, 16]\n",
      "the winners were: [23, 6]\n",
      "--------------------\n",
      "for the batch: [25, 2, 15]\n",
      "the winners were: [25, 2]\n",
      "--------------------\n",
      "the remaining group represents: 42.86%\n",
      "round: 2\n",
      "initial candidates: 12\n",
      "********************\n",
      "for the batch: [14, 2, 28, 13, 12]\n",
      "the winners were: [14, 12]\n",
      "--------------------\n",
      "for the batch: [24, 23, 18, 25, 17]\n",
      "the winners were: [17, 25]\n",
      "--------------------\n",
      "for the batch: [6, 11]\n",
      "the winners were: [6, 11]\n",
      "--------------------\n",
      "the remaining group represents: 21.43%\n",
      "round: 3\n",
      "initial candidates: 6\n",
      "********************\n",
      "for the batch: [11, 17, 25, 14, 6]\n",
      "the winners were: [17, 6]\n",
      "--------------------\n",
      "for the batch: [12]\n",
      "the winners were: [12]\n",
      "--------------------\n",
      "the remaining group represents: 10.71%\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from math import floor, ceil\n",
    "random.seed(2000)\n",
    "# parameters\n",
    "population = 28\n",
    "batch_size = 5\n",
    "selected_per_batch = 2\n",
    "num_batches = ceil(population / batch_size)\n",
    "# example\n",
    "candidates = list(range(1, population + 1))\n",
    "# controling the rounds\n",
    "candidates_copy = candidates.copy()\n",
    "round = 0\n",
    "while len(candidates_copy) > selected_per_batch + 1:\n",
    "    print(f\"round: {round + 1}\")\n",
    "    # random ids\n",
    "    random.shuffle(candidates_copy)\n",
    "    print(f\"initial candidates: {len(candidates_copy)}\")\n",
    "    print(f\"*\"*20)\n",
    "    # batches\n",
    "    batches = [\n",
    "        candidates_copy[i : batch_size + i] for i in range(0, len(candidates_copy) + 1, batch_size)\n",
    "    ]\n",
    "    # selection of the candidates per batch\n",
    "    round_winners = []\n",
    "    for b in batches:\n",
    "        print(f\"for the batch: {b}\")\n",
    "        winners = random.sample(b, k = min(selected_per_batch, len(b)))\n",
    "        print(f\"the winners were: {winners}\")\n",
    "        print(\"-\"*20)\n",
    "        for w in winners:\n",
    "            round_winners.append(w)\n",
    "    print(f\"the remaining group represents: {len(round_winners)/population:.2%}\")\n",
    "    candidates_copy = round_winners\n",
    "    round += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64878a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tournament simulation:\n",
      "round 1. Initial candidates: 28 -> Final candidates: 12 (42.86% of population)\n",
      "round 2. Initial candidates: 12 -> Final candidates: 6 (21.43% of population)\n",
      "round 3. Initial candidates: 6 -> Final candidates: 3 (10.71% of population)\n",
      "round 4. Initial candidates: 3 -> Final candidates: 2 (7.14% of population)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from math import floor, ceil\n",
    "random.seed(2000)\n",
    "# parameters\n",
    "population = 28\n",
    "batch_size = 5\n",
    "selected_per_batch = 2\n",
    "num_batches = ceil(population / batch_size)\n",
    "# example\n",
    "candidates = list(range(1, population + 1))\n",
    "# controling the rounds\n",
    "candidates_copy = candidates.copy()\n",
    "round = 0\n",
    "print(f\"tournament simulation:\")\n",
    "while len(candidates_copy) > selected_per_batch:\n",
    "    # random ids\n",
    "    random.shuffle(candidates_copy)\n",
    "    # batches\n",
    "    batches = [\n",
    "        candidates_copy[i : batch_size + i] for i in range(0, len(candidates_copy) + 1, batch_size)\n",
    "    ]\n",
    "    # selection of the candidates per batch\n",
    "    round_winners = []\n",
    "    for b in batches:\n",
    "        winners = random.sample(b, k = min(selected_per_batch, len(b)))\n",
    "        for w in winners:\n",
    "            round_winners.append(w)\n",
    "    print(f\"round {round + 1}. Initial candidates: {len(candidates_copy)} -> Final candidates: {len(round_winners)} ({len(round_winners)/population:.2%} of population)\")\n",
    "    candidates_copy = round_winners\n",
    "    round += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2bb8a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'functions' from '/Users/santiagocardenas/Documents/MDSI/202502/internship/internship_project/notebooks/functions.py'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import functions\n",
    "reload(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "414af7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the candidate parameters are:\n",
      "-batch size: 5\n",
      "-selection per batch: 2\n",
      "--------------------\n",
      "tournament simulation:\n",
      "round 1. Initial candidates: 28 -> Final candidates: 12 (42.86% of population)\n",
      "round 2. Initial candidates: 12 -> Final candidates: 6 (21.43% of population)\n",
      "round 3. Initial candidates: 6 -> Final candidates: 3 (10.71% of population)\n",
      "round 4. Initial candidates: 3 -> Final candidates: 2 (7.14% of population)\n",
      "what parameters do you wish to run?\n",
      "the candidate parameters are:\n",
      "-batch size: 4\n",
      "-selection per batch: 2\n",
      "--------------------\n",
      "tournament simulation:\n",
      "round 1. Initial candidates: 28 -> Final candidates: 14 (50.00% of population)\n",
      "round 2. Initial candidates: 14 -> Final candidates: 8 (28.57% of population)\n",
      "round 3. Initial candidates: 8 -> Final candidates: 4 (14.29% of population)\n",
      "round 4. Initial candidates: 4 -> Final candidates: 2 (7.14% of population)\n"
     ]
    }
   ],
   "source": [
    "from functions import candidate_simulation\n",
    "population = 28\n",
    "batch_size = 5\n",
    "selected_per_batch = 2\n",
    "parameters_approval = False\n",
    "while not parameters_approval:\n",
    "    print(f\"the candidate parameters are:\")\n",
    "    print(f\"-batch size: {batch_size}\")\n",
    "    print(f\"-selection per batch: {selected_per_batch}\")\n",
    "    print(\"-\"*20)\n",
    "    candidate_simulation(\n",
    "        population = population,\n",
    "        batch_size = batch_size,\n",
    "        selected_per_batch = selected_per_batch\n",
    "    )\n",
    "    parameters_feedback = input(\"Do you approve this parameters?\")\n",
    "    if parameters_feedback.lower() in ['yes', 'y']:\n",
    "        suggested_rounds = int(input(\"how many rounds to you wish to run?\"))\n",
    "        parameters_approval = True\n",
    "    else:\n",
    "        print(\"what parameters do you wish to run?\")\n",
    "        batch_size = int(input(\"What batch size do you want to use?\"))\n",
    "        selected_per_batch = int(input(\"How many candidates do you want to select per batch?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
